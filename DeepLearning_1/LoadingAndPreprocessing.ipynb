{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit"
  },
  "interpreter": {
   "hash": "965ba1120b0c101b3f715b6e258a73742ec1cf86f2c8b04492724c87d9f112c3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<h3> Loading and Preprocessing Data with TensorFlow </h3> \n",
    "\n",
    "- Data API\n",
    "- Features API\n",
    "- tf.Transform\n",
    "- TF Datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h3> Data API </h3> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "source": [
    "The from_tensor_slices() function takes a tensor and creates a tf.data.Dataset whose elements are all the slices of X. So this dataset contains 10 items. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\ntf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\ntf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\ntf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\ntf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#repeat the dataset instance 3 times and get 7 items out of it \n",
    "dataset1 = dataset.repeat(3).batch(7)\n",
    "for item in dataset1:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\ntf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\ntf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\ntf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset2 = dataset.repeat(3).batch(7, drop_remainder=True)\n",
    "for item in dataset2:\n",
    "    print(item)"
   ]
  },
  {
   "source": [
    "dataset methods do not modify datasets. They only create new ones. Hence reference to the dataset is required"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#applying transformations or functions to the data\n",
    "dataset3 = dataset.map(lambda x: x*2)\n",
    "for item in dataset3:\n",
    "    print(item)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(4, shape=(), dtype=int32)\ntf.Tensor(6, shape=(), dtype=int32)\ntf.Tensor(8, shape=(), dtype=int32)\ntf.Tensor(10, shape=(), dtype=int32)\ntf.Tensor(12, shape=(), dtype=int32)\ntf.Tensor(14, shape=(), dtype=int32)\ntf.Tensor(16, shape=(), dtype=int32)\ntf.Tensor(18, shape=(), dtype=int32)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(4, shape=(), dtype=int32)\ntf.Tensor(6, shape=(), dtype=int32)\ntf.Tensor(8, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset4 = dataset.filter(lambda x: x%2 == 0)\n",
    "for item in dataset4:\n",
    "    print(item)"
   ]
  },
  {
   "source": [
    "<h3> Shuffling the Data </h3> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "For effective shuffing, we can split a data source to multiple files, and then pick files randomly and simultaneously read them, interleaving their lines. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "X_train_full,X_test,y_train_full,y_test = train_test_split(housing.data,housing.target.reshape(-1,1), random_state = 42)\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(X_train_full,y_train_full,random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_"
   ]
  },
  {
   "source": [
    "<h4> Splitting the data into many csv files </h4> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe of train,valid,test data\n",
    "import pandas as pd\n",
    "column_names = housing.feature_names + housing.target_names\n",
    "\n",
    "housing_train_df = pd.DataFrame(data = X_train)\n",
    "housing_train_df[\"Price\"] = y_train\n",
    "housing_train_df.columns = column_names\n",
    "\n",
    "housing_valid_df = pd.DataFrame(data = X_valid)\n",
    "housing_valid_df[\"Price\"] = y_valid\n",
    "housing_valid_df.columns = column_names\n",
    "\n",
    "housing_test_df = pd.DataFrame(data = X_test)\n",
    "housing_test_df[\"Price\"] = y_test\n",
    "housing_test_df.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_dict = {'train':housing_train_df, 'valid':housing_valid_df, 'test':housing_test_df}\n",
    "n_parts = {'train':20, 'valid':10,'test':10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "file_path_dict = dict()\n",
    "for key in dataframe_dict.keys():\n",
    "    file_path_dict[key] = list()\n",
    "    df = dataframe_dict[key]\n",
    "    no_files = n_parts[key]\n",
    "    dir_path = os.path.join(os.getcwd(),\"housing\",key)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    for file_idx,idx_array in enumerate(np.array_split(np.arange(df.shape[0]),no_files)):\n",
    "        temp_df = df.iloc[idx_array,:]\n",
    "        saving_path = os.path.join(dir_path,\"{}_{}.csv\".format(key,file_idx))\n",
    "        file_path_dict[key].append(saving_path)\n",
    "        temp_df.to_csv(saving_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of filepaths for train, validate, test\n",
    "train_filepaths = file_path_dict[\"train\"]\n",
    "valid_filepaths = file_path_dict[\"valid\"]\n",
    "test_filepaths = file_path_dict[\"test\"]"
   ]
  },
  {
   "source": [
    "By default the tf.data.Dataset.list_files() returns a dataset that shuffles the file paths"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataset containing only the train filepaths\n",
    "train_filepath_dataset = tf.data.Dataset.list_files(train_filepaths,seed = 42)\n",
    "#next we can use the interleave() methods, with a number of files to read specified\n",
    "n_readers = 5\n",
    "dataset = train_filepath_dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length = n_readers)\n",
    "#the interleave method will create a dataset that will pull 5 file paths from the filepath_dataset and for each one, it calls the function we gave to create a new dataset. \n",
    "#After it runs through the first 5 filepaths, it will continue to run on the other filepaths"
   ]
  },
  {
   "source": [
    "By default interleave() does not use parallelism. It reads one line at a time from each file, sequentially. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "b'5.9522,26.0,6.196521739130435,1.0069565217391305,1479.0,2.5721739130434784,34.5,-119.75,4.384'\nb'3.8456,35.0,5.461346633416459,0.9576059850374065,1154.0,2.8778054862842892,37.96,-122.05,1.598'\nb'2.1856,41.0,3.7189873417721517,1.0658227848101265,803.0,2.0329113924050635,32.76,-117.12,1.205'\nb'1.6571,34.0,4.454976303317536,1.0876777251184835,1358.0,3.2180094786729856,37.94,-122.35,1.052'\nb'3.9688,41.0,5.259786476868327,0.9715302491103203,916.0,3.2597864768683276,33.98,-118.07,1.698'\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "source": [
    "<h3> Preprocessing the data </h3> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 3.89175860e+00,  2.86245478e+01,  5.45593655e+00,  1.09963474e+00,\n",
       "        1.42428122e+03,  2.95886657e+00,  3.56464315e+01, -1.19584363e+02])"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "X_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1.90927329e+00, 1.26409177e+01, 2.55038070e+00, 4.65460128e-01,\n",
       "       1.09576000e+03, 2.36138048e+00, 2.13456672e+00, 2.00093304e+00])"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(line):\n",
    "    defs = [0.]*n_inputs + [tf.constant([],dtype = tf.float32)]\n",
    "    #first argument is the line to pass\n",
    "    #second is the default value in the column\n",
    "    #by passing an empty array in tf.constant([]), we can accept any value into this, however it will raise exception if no value is available\n",
    "    fields = tf.io.decode_csv(line,record_defaults=defs)\n",
    "    #decode_csv returns a list of scalar tensors, which needs to be stacked to give a single 1D tensor\n",
    "    x = tf.stack(fields[:-1])\n",
    "    #stack allows us to stack the list of scalar tensors into one single 1D tensor\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return (x - X_mean)/X_std, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths,n_readers = 5, n_read_threads = None, shuffle_buffer_size = 10000, n_parse_threads = 5, batch_size = 32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths)\n",
    "    dataset = dataset.interleave(lambda filepath:tf.data.TextLineDataset(filepath), cycle_length = n_readers, num_parallel_calls = n_read_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls = n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_processed = csv_reader_dataset(train_filepaths, batch_size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X = tf.Tensor(\n[[-0.6995115   1.0581076  -0.50768846  0.00485739  0.6933259   0.9386411\n   1.0416952  -1.3321956 ]\n [ 0.18763229  0.8998913  -0.06012543 -0.1459513  -0.18825404  0.23343122\n  -0.68230814  0.71184903]\n [-0.38934112  1.8491895  -0.03967553 -0.1600461  -0.6409079  -0.39896438\n   1.3696309  -0.93737936]], shape=(3, 8), dtype=float32)\ny = tf.Tensor(\n[[3.714]\n [1.779]\n [1.589]], shape=(3, 1), dtype=float32)\nX = tf.Tensor(\n[[ 0.09262241 -0.8404887  -0.79305923 -0.3465419  -0.25578707 -0.44558677\n   0.83556503 -1.277221  ]\n [ 0.29337934 -0.20762321  0.10447887 -0.33604828 -0.42096922  0.03345006\n   1.3883705  -0.8524199 ]\n [ 0.89135545 -0.60316414  0.19229767 -0.18285045  1.4480531   0.17167047\n   0.6622282  -1.0123452 ]], shape=(3, 8), dtype=float32)\ny = tf.Tensor(\n[[3.786]\n [1.105]\n [3.868]], shape=(3, 1), dtype=float32)\nX = tf.Tensor(\n[[-0.6398553   0.9789995  -0.22239974 -0.08394154  0.23337112 -0.2509843\n   1.5195441  -0.7474693 ]\n [-0.04654054  0.5834586  -0.01093122 -0.21019256  0.01890811 -0.15243022\n   1.1494452  -1.3122045 ]\n [-1.300316    0.42524222  0.12930529  1.6274391  -1.251443    0.3501544\n  -0.4761762   0.8317958 ]], shape=(3, 8), dtype=float32)\ny = tf.Tensor(\n[[1.39 ]\n [1.299]\n [1.375]], shape=(3, 1), dtype=float32)\nX = tf.Tensor(\n[[ 0.17993306  0.1879177  -0.49215898 -0.15814985  0.09921767  1.1603459\n  -0.644829    0.57691365]\n [ 1.9693574  -1.3942459   0.57391685 -0.18768094  0.78641194 -0.17258006\n   1.0089018  -1.2172476 ]\n [ 3.0522826  -1.5524622   1.1176071  -0.26494953  0.5080663   0.16646163\n   0.8636744  -1.1522793 ]], shape=(3, 8), dtype=float32)\ny = tf.Tensor(\n[[1.568  ]\n [3.593  ]\n [5.00001]], shape=(3, 1), dtype=float32)\nX = tf.Tensor(\n[[-0.27065727 -1.6315705   0.08441731  0.10967682 -0.92290395 -0.05509469\n   0.680966   -0.13275439]\n [ 1.0190481   0.5834586  -0.00695349 -0.2904981   0.03533506 -0.09281277\n  -0.81348175  0.60190356]\n [-0.9173431   0.10880951 -1.0022461   0.16813074  0.01890811 -0.19614981\n  -0.72915536  0.6268896 ]], shape=(3, 8), dtype=float32)\ny = tf.Tensor(\n[[0.807]\n [2.896]\n [2.   ]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#this takes 5 batches, with each batch having 3 training examples in it\n",
    "for X_batch,y_batch in Train_processed.take(5):\n",
    "    print(\"X =\", X_batch)\n",
    "    print(\"y =\", y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Valid_processed = csv_reader_dataset(valid_filepaths)\n",
    "Test_processed = csv_reader_dataset(test_filepaths)"
   ]
  },
  {
   "source": [
    "Look into \n",
    "- concatenate()\n",
    "- zip()\n",
    "- window()\n",
    "- reduce()\n",
    "- cache()\n",
    "- shard()\n",
    "- flat_map()\n",
    "- padded_batch()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h3> TFRecord Format </h3> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- TensorFlow's preferred format for storing large amounts of data\n",
    "- Contains a sequence of binary records of varying sizes\n",
    "- Each record has a length, a Cycle Redundancy Check checksum to check that the length was not corrupted. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h3> Protocol buffers </h3>\n",
    "\n",
    "A portable, extensible and efficient binary format. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = b'\\n\\x05Alice\\x10{\\x1a\\x07a@b.com\\x1a\\x07c@d.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "b'\\n\\x05Alice\\x10{\\x1a\\x07a@b.com\\x1a\\x07c@d.com'"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n\\x05Alice\\x10{\\x1a\\x07a@b.com\\x1a\\x07c@d.com'"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "a.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "\n",
    "test_json = {'one':{'a':'A','b':[1,2,3]},'two':{'c':'C','d':[1,2,3]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "json_string = json.dumps(test_json)\n",
    "type(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'one': {'a': 'A', 'b': [1, 2, 3]}, 'two': {'c': 'C', 'd': [1, 2, 3]}}"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "json.loads(json_string)"
   ]
  },
  {
   "source": [
    "<h3> The Features API </h3> \n",
    "\n",
    "It lets you define how each feature or group of features should be preprocessed. \n",
    "\n",
    "tf.feature_column\n",
    "\n",
    "- Using bucketized_column to bin data\n",
    "\n",
    "Sometimes you have multimodal data where there are separate peaks in it's distribution. In these cases, you define a bucket for each mode, where the boundaries are between the peaks."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "https://www.tensorflow.org/tutorials/structured_data/feature_columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  Type  Age                Breed1  Gender Color1    Color2 MaturitySize  \\\n",
       "0  Cat    3                 Tabby    Male  Black     White        Small   \n",
       "1  Cat    1  Domestic Medium Hair    Male  Black     Brown       Medium   \n",
       "2  Dog    1           Mixed Breed    Male  Brown     White       Medium   \n",
       "3  Dog    4           Mixed Breed  Female  Black     Brown       Medium   \n",
       "4  Dog    1           Mixed Breed    Male  Black  No Color       Medium   \n",
       "\n",
       "  FurLength Vaccinated Sterilized   Health  Fee  \\\n",
       "0     Short         No         No  Healthy  100   \n",
       "1    Medium   Not Sure   Not Sure  Healthy    0   \n",
       "2    Medium        Yes         No  Healthy    0   \n",
       "3     Short        Yes         No  Healthy  150   \n",
       "4     Short         No         No  Healthy    0   \n",
       "\n",
       "                                         Description  PhotoAmt  AdoptionSpeed  \n",
       "0  Nibble is a 3+ month old ball of cuteness. He ...         1              2  \n",
       "1  I just found it alone yesterday near my apartm...         2              0  \n",
       "2  Their pregnant mother was dumped by her irresp...         7              3  \n",
       "3  Good guard dog, very alert, active, obedience ...         8              2  \n",
       "4  This handsome yet cute boy is up for adoption....         3              2  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Type</th>\n      <th>Age</th>\n      <th>Breed1</th>\n      <th>Gender</th>\n      <th>Color1</th>\n      <th>Color2</th>\n      <th>MaturitySize</th>\n      <th>FurLength</th>\n      <th>Vaccinated</th>\n      <th>Sterilized</th>\n      <th>Health</th>\n      <th>Fee</th>\n      <th>Description</th>\n      <th>PhotoAmt</th>\n      <th>AdoptionSpeed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Cat</td>\n      <td>3</td>\n      <td>Tabby</td>\n      <td>Male</td>\n      <td>Black</td>\n      <td>White</td>\n      <td>Small</td>\n      <td>Short</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Healthy</td>\n      <td>100</td>\n      <td>Nibble is a 3+ month old ball of cuteness. He ...</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Cat</td>\n      <td>1</td>\n      <td>Domestic Medium Hair</td>\n      <td>Male</td>\n      <td>Black</td>\n      <td>Brown</td>\n      <td>Medium</td>\n      <td>Medium</td>\n      <td>Not Sure</td>\n      <td>Not Sure</td>\n      <td>Healthy</td>\n      <td>0</td>\n      <td>I just found it alone yesterday near my apartm...</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dog</td>\n      <td>1</td>\n      <td>Mixed Breed</td>\n      <td>Male</td>\n      <td>Brown</td>\n      <td>White</td>\n      <td>Medium</td>\n      <td>Medium</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Healthy</td>\n      <td>0</td>\n      <td>Their pregnant mother was dumped by her irresp...</td>\n      <td>7</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Dog</td>\n      <td>4</td>\n      <td>Mixed Breed</td>\n      <td>Female</td>\n      <td>Black</td>\n      <td>Brown</td>\n      <td>Medium</td>\n      <td>Short</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Healthy</td>\n      <td>150</td>\n      <td>Good guard dog, very alert, active, obedience ...</td>\n      <td>8</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Dog</td>\n      <td>1</td>\n      <td>Mixed Breed</td>\n      <td>Male</td>\n      <td>Black</td>\n      <td>No Color</td>\n      <td>Medium</td>\n      <td>Short</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Healthy</td>\n      <td>0</td>\n      <td>This handsome yet cute boy is up for adoption....</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "petsdf = pd.read_csv(r'C:\\Users\\ASUS\\Desktop\\Hands on ML\\Hands-on-Machine-Learning-Textbook-Exercises\\DeepLearning_1\\petfinder-mini\\petfinder-mini.csv')\n",
    "petsdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  Type  Age                Breed1  Gender Color1    Color2 MaturitySize  \\\n",
       "0  Cat    3                 Tabby    Male  Black     White        Small   \n",
       "1  Cat    1  Domestic Medium Hair    Male  Black     Brown       Medium   \n",
       "2  Dog    1           Mixed Breed    Male  Brown     White       Medium   \n",
       "3  Dog    4           Mixed Breed  Female  Black     Brown       Medium   \n",
       "4  Dog    1           Mixed Breed    Male  Black  No Color       Medium   \n",
       "\n",
       "  FurLength Vaccinated Sterilized   Health  Fee  PhotoAmt  target  \n",
       "0     Short         No         No  Healthy  100         1       1  \n",
       "1    Medium   Not Sure   Not Sure  Healthy    0         2       1  \n",
       "2    Medium        Yes         No  Healthy    0         7       1  \n",
       "3     Short        Yes         No  Healthy  150         8       1  \n",
       "4     Short         No         No  Healthy    0         3       1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Type</th>\n      <th>Age</th>\n      <th>Breed1</th>\n      <th>Gender</th>\n      <th>Color1</th>\n      <th>Color2</th>\n      <th>MaturitySize</th>\n      <th>FurLength</th>\n      <th>Vaccinated</th>\n      <th>Sterilized</th>\n      <th>Health</th>\n      <th>Fee</th>\n      <th>PhotoAmt</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Cat</td>\n      <td>3</td>\n      <td>Tabby</td>\n      <td>Male</td>\n      <td>Black</td>\n      <td>White</td>\n      <td>Small</td>\n      <td>Short</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Healthy</td>\n      <td>100</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Cat</td>\n      <td>1</td>\n      <td>Domestic Medium Hair</td>\n      <td>Male</td>\n      <td>Black</td>\n      <td>Brown</td>\n      <td>Medium</td>\n      <td>Medium</td>\n      <td>Not Sure</td>\n      <td>Not Sure</td>\n      <td>Healthy</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dog</td>\n      <td>1</td>\n      <td>Mixed Breed</td>\n      <td>Male</td>\n      <td>Brown</td>\n      <td>White</td>\n      <td>Medium</td>\n      <td>Medium</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Healthy</td>\n      <td>0</td>\n      <td>7</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Dog</td>\n      <td>4</td>\n      <td>Mixed Breed</td>\n      <td>Female</td>\n      <td>Black</td>\n      <td>Brown</td>\n      <td>Medium</td>\n      <td>Short</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Healthy</td>\n      <td>150</td>\n      <td>8</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Dog</td>\n      <td>1</td>\n      <td>Mixed Breed</td>\n      <td>Male</td>\n      <td>Black</td>\n      <td>No Color</td>\n      <td>Medium</td>\n      <td>Short</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Healthy</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "petsdf['target'] = np.where(petsdf[\"AdoptionSpeed\"] == 4,0,1)\n",
    "petsdf.drop(columns=['AdoptionSpeed', 'Description'], inplace = True)\n",
    "petsdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(petsdf, test_size=0.2, random_state=42)\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "source": [
    "Wrap the dataset under tf.data as it allows us to use feature columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(df,shuffle = True, batch_size = 32):\n",
    "    df = df.copy()\n",
    "    #obtain the targets\n",
    "    labels = df.pop('target')\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(df),labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(df))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the item thats required from a tensorflow dataset\n",
    "#labels = train.pop('target')\n",
    "#list(tf.data.Dataset.from_tensor_slices((dict(train),labels)).take(1).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = df_to_dataset(train,batch_size=5)\n",
    "val_ds = df_to_dataset(val,shuffle = False, batch_size=5)\n",
    "test_ds = df_to_dataset(test,shuffle = False, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Type', 'Age', 'Breed1', 'Gender', 'Color1', 'Color2', 'MaturitySize', 'FurLength', 'Vaccinated', 'Sterilized', 'Health', 'Fee', 'PhotoAmt']\ntf.Tensor([1 4 3 4 2], shape=(5,), dtype=int64)\ntf.Tensor([1 0 0 1 1], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in train_ds.take(1):\n",
    "    print(list(feature_batch.keys()))\n",
    "    print(feature_batch[\"Age\"])\n",
    "    print(label_batch)"
   ]
  },
  {
   "source": [
    "Using feature columns \n",
    "\n",
    "Now we create several types of feature columns and demonstrate how they transform a column from a dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iter creates a iterable of the input object\n",
    "#then we can use next(iter(obj)) to obtain the element starting with the first one\n",
    "example_batch = next(iter(train_ds))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Type': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'Dog', b'Dog', b'Cat', b'Dog', b'Dog'], dtype=object)>,\n",
       " 'Age': <tf.Tensor: shape=(5,), dtype=int64, numpy=array([ 2,  2,  2, 36,  2], dtype=int64)>,\n",
       " 'Breed1': <tf.Tensor: shape=(5,), dtype=string, numpy=\n",
       " array([b'Mixed Breed', b'Mixed Breed', b'Domestic Short Hair',\n",
       "        b'Golden Retriever', b'Mixed Breed'], dtype=object)>,\n",
       " 'Gender': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'Female', b'Female', b'Male', b'Female', b'Male'], dtype=object)>,\n",
       " 'Color1': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'Yellow', b'Black', b'Black', b'Golden', b'Brown'], dtype=object)>,\n",
       " 'Color2': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'Cream', b'Brown', b'Gray', b'No Color', b'White'], dtype=object)>,\n",
       " 'MaturitySize': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'Medium', b'Medium', b'Small', b'Large', b'Medium'], dtype=object)>,\n",
       " 'FurLength': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'Short', b'Short', b'Medium', b'Long', b'Short'], dtype=object)>,\n",
       " 'Vaccinated': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'No', b'Not Sure', b'No', b'No', b'Yes'], dtype=object)>,\n",
       " 'Sterilized': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'No', b'No', b'Not Sure', b'No', b'No'], dtype=object)>,\n",
       " 'Health': <tf.Tensor: shape=(5,), dtype=string, numpy=\n",
       " array([b'Healthy', b'Healthy', b'Healthy', b'Minor Injury', b'Healthy'],\n",
       "       dtype=object)>,\n",
       " 'Fee': <tf.Tensor: shape=(5,), dtype=int64, numpy=array([0, 0, 0, 0, 0], dtype=int64)>,\n",
       " 'PhotoAmt': <tf.Tensor: shape=(5,), dtype=int64, numpy=array([1, 1, 2, 5, 4], dtype=int64)>}"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "example_batch"
   ]
  },
  {
   "source": [
    "Understanding of tf.keras.layers.DenseFeatures\n",
    "\n",
    "By creating tf.feature_columns.xxx_column that does specific transformations, then adding these columns to a list, we create a list of transformations to be done on the data\n",
    "\n",
    "feature_columns.append(transformation_name)\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "\n",
    "\n",
    "Then we create a dictionary to associate column names with column values\n",
    "\n",
    "inputs = {}\n",
    "\n",
    "inputs(\"temp_num\") = tf.keras.Input(shape = (1,), name = \"feature_name\")\n",
    "\n",
    "#apply the transformation\n",
    "\n",
    "x = feature_layer(inputs)\n",
    "\n",
    "Continue normally \n",
    "\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "https://stackoverflow.com/questions/54375298/how-to-use-tensorflow-feature-columns-as-input-to-a-keras-model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(feature_column):\n",
    "    feature_layer = tf.keras.layers.DenseFeatures(feature_column)\n",
    "    print(feature_layer(example_batch).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo_count= tf.feature_column.numeric_column('PhotoAmt')"
   ]
  },
  {
   "source": [
    "<h3> Bucketized columns </h3> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = tf.feature_column.numeric_column('Age')\n",
    "age_bucketized = tf.feature_column.bucketized_column(age,boundaries = [1,3,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0. 1. 0. 0.]\n [0. 1. 0. 0.]\n [0. 1. 0. 0.]\n [0. 0. 0. 1.]\n [0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "demo(age_bucketized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 2.]\n [ 2.]\n [ 2.]\n [36.]\n [ 2.]]\n"
     ]
    }
   ],
   "source": [
    "demo(age)"
   ]
  },
  {
   "source": [
    "<h3> Categorical columns </h3> \n",
    "\n",
    "We cannot feed strings directly into a model, hence we must first map them to numeric values. \n",
    "\n",
    "To pass in strings we can use:\n",
    "\n",
    "- categorical_column_with_vocabulary_list\n",
    "- categorical_column_with_vocabulary_file (if loading the categories from a list)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0. 1.]\n [0. 1.]\n [1. 0.]\n [0. 1.]\n [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#converts into numerical values\n",
    "animal_type = tf.feature_column.categorical_column_with_vocabulary_list('Type', ['Cat','Dog'])\n",
    "#converts into one-hot vector\n",
    "animal_type_one_hot = tf.feature_column.indicator_column(animal_type)\n",
    "demo(animal_type_one_hot)"
   ]
  },
  {
   "source": [
    "<h3> Embedding Columns </h3> \n",
    "\n",
    "If we have thousands or more values per category, as the number of categories grows large, it becomes infeasible to train a neural network using one-hot encodings. Embedding represents the data as a lower dimensional dense vector. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[-0.6872541   0.23651108  0.19703852  0.87541044  0.33767933]\n [-0.6872541   0.23651108  0.19703852  0.87541044  0.33767933]\n [-0.20582496 -0.01826351  0.7618332   0.5168024  -0.52089995]\n [ 0.15614133 -0.06637327 -0.41340065 -0.10094053  0.06796455]\n [-0.6872541   0.23651108  0.19703852  0.87541044  0.33767933]]\n"
     ]
    }
   ],
   "source": [
    "breed1 = tf.feature_column.categorical_column_with_vocabulary_list('Breed1', train[\"Breed1\"].unique().tolist())\n",
    "breed1_embedding = tf.feature_column.embedding_column(breed1, dimension=5)\n",
    "demo(breed1_embedding)"
   ]
  },
  {
   "source": [
    "<h3> Hashed Feature Columns </h3>\n",
    "\n",
    "Calculates the hash value of a input, then selects the one of hash_bucket_size to encode the string. We can make the hash_bucket_size much smaller than the actual number of categories.\n",
    "\n",
    "Key Point: An important downside of this technique is that there may be collisions in which different strings are mapped to the same bucket. In practice, this can work well for some datasets regardless."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "breed1_hashed = tf.feature_column.categorical_column_with_hash_bucket(\"Breed1\", hash_bucket_size = 10)\n",
    "demo(tf.feature_column.indicator_column(breed1_hashed))"
   ]
  },
  {
   "source": [
    "<h3> Crossed feature columns </h3> \n",
    "\n",
    "Some features are better when combined, hence we may use this feature. \n",
    "\n",
    "It first creates the combinations for the 2 categories. Then makes hashes of the combinations. Then bucketize the hashes. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "OverflowError",
     "evalue": "Python int too large to convert to C long",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\feature_column\\dense_features.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, features, cols_to_output_tensors, training)\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m           tensor = column.get_dense_tensor(\n\u001b[0m\u001b[0;32m    166\u001b[0m               transformation_cache, self._state_manager, training=training)\n",
      "\u001b[1;31mTypeError\u001b[0m: get_dense_tensor() got an unexpected keyword argument 'training'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, key, state_manager, training)\u001b[0m\n\u001b[0;32m   2351\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2352\u001b[1;33m       transformed = column.transform_feature(\n\u001b[0m\u001b[0;32m   2353\u001b[0m           self, state_manager, training=training)\n",
      "\u001b[1;31mTypeError\u001b[0m: transform_feature() got an unexpected keyword argument 'training'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, key, state_manager, training)\u001b[0m\n\u001b[0;32m   2351\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2352\u001b[1;33m       transformed = column.transform_feature(\n\u001b[0m\u001b[0;32m   2353\u001b[0m           self, state_manager, training=training)\n",
      "\u001b[1;31mTypeError\u001b[0m: transform_feature() got an unexpected keyword argument 'training'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-0843b8588349>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcrossed_feature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrossed_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mage_bucketized\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0manimal_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhash_bucket_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdemo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindicator_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcrossed_feature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-74-c1c854ffe856>\u001b[0m in \u001b[0;36mdemo\u001b[1;34m(feature_column)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdemo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mfeature_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDenseFeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1012\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1013\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\feature_column\\dense_features.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, features, cols_to_output_tensors, training)\u001b[0m\n\u001b[0;32m    166\u001b[0m               transformation_cache, self._state_manager, training=training)\n\u001b[0;32m    167\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m           tensor = column.get_dense_tensor(transformation_cache,\n\u001b[0m\u001b[0;32m    169\u001b[0m                                            self._state_manager)\n\u001b[0;32m    170\u001b[0m         \u001b[0mprocessed_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_dense_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\u001b[0m in \u001b[0;36mget_dense_tensor\u001b[1;34m(self, transformation_cache, state_manager)\u001b[0m\n\u001b[0;32m   4156\u001b[0m     \u001b[1;31m# Feature has been already transformed. Return the intermediate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4157\u001b[0m     \u001b[1;31m# representation created by transform_feature.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4158\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtransformation_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4160\u001b[0m   @deprecation.deprecated(_FEATURE_COLUMN_DEPRECATION_DATE,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, key, state_manager, training)\u001b[0m\n\u001b[0;32m   2353\u001b[0m           self, state_manager, training=training)\n\u001b[0;32m   2354\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2355\u001b[1;33m       \u001b[0mtransformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2356\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtransformed\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2357\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Column {} is not supported.'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\u001b[0m in \u001b[0;36mtransform_feature\u001b[1;34m(self, transformation_cache, state_manager)\u001b[0m\n\u001b[0;32m   4092\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m \u001b[0mrank\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mknown\u001b[0m \u001b[0mat\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mbuilding\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4093\u001b[0m     \"\"\"\n\u001b[1;32m-> 4094\u001b[1;33m     id_weight_pair = self.categorical_column.get_sparse_tensors(\n\u001b[0m\u001b[0;32m   4095\u001b[0m         transformation_cache, state_manager)\n\u001b[0;32m   4096\u001b[0m     return self._transform_id_weight_pair(id_weight_pair,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\u001b[0m in \u001b[0;36mget_sparse_tensors\u001b[1;34m(self, transformation_cache, state_manager)\u001b[0m\n\u001b[0;32m   3950\u001b[0m     \u001b[1;34m\"\"\"See `CategoricalColumn` base class.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3951\u001b[0m     return CategoricalColumn.IdWeightPair(\n\u001b[1;32m-> 3952\u001b[1;33m         transformation_cache.get(self, state_manager), None)\n\u001b[0m\u001b[0;32m   3953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3954\u001b[0m   @deprecation.deprecated(_FEATURE_COLUMN_DEPRECATION_DATE,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, key, state_manager, training)\u001b[0m\n\u001b[0;32m   2353\u001b[0m           self, state_manager, training=training)\n\u001b[0;32m   2354\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2355\u001b[1;33m       \u001b[0mtransformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2356\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtransformed\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2357\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Column {} is not supported.'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py\u001b[0m in \u001b[0;36mtransform_feature\u001b[1;34m(self, transformation_cache, state_manager)\u001b[0m\n\u001b[0;32m   3908\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3909\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unsupported column type. Given: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3910\u001b[1;33m     return sparse_ops.sparse_cross_hashed(\n\u001b[0m\u001b[0;32m   3911\u001b[0m         \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3912\u001b[0m         \u001b[0mnum_buckets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhash_bucket_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\sparse_ops.py\u001b[0m in \u001b[0;36msparse_cross_hashed\u001b[1;34m(inputs, num_buckets, hash_key, name)\u001b[0m\n\u001b[0;32m    671\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m   \"\"\"\n\u001b[1;32m--> 673\u001b[1;33m   return _sparse_cross_internal(\n\u001b[0m\u001b[0;32m    674\u001b[0m       \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m       \u001b[0mhashed_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\sparse_ops.py\u001b[0m in \u001b[0;36m_sparse_cross_internal\u001b[1;34m(inputs, hashed_output, num_buckets, hash_key, name)\u001b[0m\n\u001b[0;32m    745\u001b[0m       \u001b[0minternal_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 747\u001b[1;33m   indices_out, values_out, shape_out = gen_sparse_ops.sparse_cross(\n\u001b[0m\u001b[0;32m    748\u001b[0m       \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m       \u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\gen_sparse_ops.py\u001b[0m in \u001b[0;36msparse_cross\u001b[1;34m(indices, values, shapes, dense_inputs, hashed_output, num_buckets, hash_key, out_type, internal_type, name)\u001b[0m\n\u001b[0;32m   1056\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1058\u001b[1;33m       return sparse_cross_eager_fallback(\n\u001b[0m\u001b[0;32m   1059\u001b[0m           \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhashed_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhashed_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m           \u001b[0mnum_buckets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_buckets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhash_key\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhash_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\gen_sparse_ops.py\u001b[0m in \u001b[0;36msparse_cross_eager_fallback\u001b[1;34m(indices, values, shapes, dense_inputs, hashed_output, num_buckets, hash_key, out_type, internal_type, name, ctx)\u001b[0m\n\u001b[0;32m   1136\u001b[0m   \u001b[1;34m\"dense_types\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_attr_dense_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"out_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"internal_type\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m   internal_type)\n\u001b[1;32m-> 1138\u001b[1;33m   _result = _execute.execute(b\"SparseCross\", 3, inputs=_inputs_flat,\n\u001b[0m\u001b[0;32m   1139\u001b[0m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0;32m   1140\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: Python int too large to convert to C long"
     ]
    }
   ],
   "source": [
    "crossed_feature = tf.feature_column.crossed_column([age_bucketized,animal_type], hash_bucket_size = 100)\n",
    "demo(tf.feature_column.indicator_column(crossed_feature))"
   ]
  },
  {
   "source": [
    "How to implement feature columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "#remove target from features\n",
    "attributes = train.columns.tolist()[:-1]\n",
    "\n",
    "numeric_columns = [value for value in attributes if re.match(r'int[0-9]*',str(train.dtypes.to_dict()[value])) != None]\n",
    "\n",
    "#numeric columns\n",
    "for header in numeric_columns:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bucketized_cols\n",
    "age = tf.feature_column.numeric_column('Age')\n",
    "age_buckets = tf.feature_column.bucketized_column(age, boundaries = [1,2,3,4,5])\n",
    "feature_columns.append(age_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator_columns = ['Type', 'Color1', 'Color2', 'Gender', 'MaturitySize','FurLength', 'Vaccinated', 'Sterilized', 'Health']\n",
    "\n",
    "for header in indicator_columns:\n",
    "    categorical_column = tf.feature_column.categorical_column_with_vocabulary_list(header, petsdf[header].unique().tolist())\n",
    "    feature_columns.append(tf.feature_column.indicator_column(categorical_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding columns\n",
    "breed1 = tf.feature_column.categorical_column_with_vocabulary_list('Breed1',petsdf[\"Breed1\"].unique().tolist())\n",
    "breed1_embedding = tf.feature_column.embedding_column(breed1, dimension = 8)\n",
    "feature_columns.append(breed1_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_type_feature = tf.feature_column.crossed_column([age_buckets,animal_type], hash_bucket_size = 100)\n",
    "feature_columns.append(tf.feature_column.indicator_column(age_type_feature))"
   ]
  },
  {
   "source": [
    "Now that we have defined all out column transformations using the Feature API, we can create a DenseFeatures layer to input to Keras model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#earlier we only used 5 instances per batch. Now we use 32\n",
    "train_ds = df_to_dataset(train,batch_size=32)\n",
    "val_ds = df_to_dataset(val,shuffle = False, batch_size=32)\n",
    "test_ds = df_to_dataset(test,shuffle = False, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(feature_layer)\n",
    "model.add(tf.keras.layers.Dense(128,activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(128,activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.1))\n",
    "model.add(tf.keras.layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}