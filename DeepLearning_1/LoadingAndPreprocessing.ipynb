{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit"
  },
  "interpreter": {
   "hash": "965ba1120b0c101b3f715b6e258a73742ec1cf86f2c8b04492724c87d9f112c3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<h3> Loading and Preprocessing Data with TensorFlow </h3> \n",
    "\n",
    "- Data API\n",
    "- Features API\n",
    "- tf.Transform\n",
    "- TF Datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h3> Data API </h3> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "source": [
    "The from_tensor_slices() function takes a tensor and creates a tf.data.Dataset whose elements are all the slices of X. So this dataset contains 10 items. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\ntf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\ntf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\ntf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\ntf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#repeat the dataset instance 3 times and get 7 items out of it \n",
    "dataset1 = dataset.repeat(3).batch(7)\n",
    "for item in dataset1:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\ntf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\ntf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\ntf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset2 = dataset.repeat(3).batch(7, drop_remainder=True)\n",
    "for item in dataset2:\n",
    "    print(item)"
   ]
  },
  {
   "source": [
    "dataset methods do not modify datasets. They only create new ones. Hence reference to the dataset is required"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#applying transformations or functions to the data\n",
    "dataset3 = dataset.map(lambda x: x*2)\n",
    "for item in dataset3:\n",
    "    print(item)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(4, shape=(), dtype=int32)\ntf.Tensor(6, shape=(), dtype=int32)\ntf.Tensor(8, shape=(), dtype=int32)\ntf.Tensor(10, shape=(), dtype=int32)\ntf.Tensor(12, shape=(), dtype=int32)\ntf.Tensor(14, shape=(), dtype=int32)\ntf.Tensor(16, shape=(), dtype=int32)\ntf.Tensor(18, shape=(), dtype=int32)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(4, shape=(), dtype=int32)\ntf.Tensor(6, shape=(), dtype=int32)\ntf.Tensor(8, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset4 = dataset.filter(lambda x: x%2 == 0)\n",
    "for item in dataset4:\n",
    "    print(item)"
   ]
  },
  {
   "source": [
    "<h3> Shuffling the Data </h3> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "For effective shuffing, we can split a data source to multiple files, and then pick files randomly and simultaneously read them, interleaving their lines. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "X_train_full,X_test,y_train_full,y_test = train_test_split(housing.data,housing.target.reshape(-1,1), random_state = 42)\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(X_train_full,y_train_full,random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_"
   ]
  },
  {
   "source": [
    "<h4> Splitting the data into many csv files </h4> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe of train,valid,test data\n",
    "import pandas as pd\n",
    "column_names = housing.feature_names + housing.target_names\n",
    "\n",
    "housing_train_df = pd.DataFrame(data = X_train)\n",
    "housing_train_df[\"Price\"] = y_train\n",
    "housing_train_df.columns = column_names\n",
    "\n",
    "housing_valid_df = pd.DataFrame(data = X_valid)\n",
    "housing_valid_df[\"Price\"] = y_valid\n",
    "housing_valid_df.columns = column_names\n",
    "\n",
    "housing_test_df = pd.DataFrame(data = X_test)\n",
    "housing_test_df[\"Price\"] = y_test\n",
    "housing_test_df.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_dict = {'train':housing_train_df, 'valid':housing_valid_df, 'test':housing_test_df}\n",
    "n_parts = {'train':20, 'valid':10,'test':10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "file_path_dict = dict()\n",
    "for key in dataframe_dict.keys():\n",
    "    file_path_dict[key] = list()\n",
    "    df = dataframe_dict[key]\n",
    "    no_files = n_parts[key]\n",
    "    dir_path = os.path.join(os.getcwd(),\"housing\",key)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    for file_idx,idx_array in enumerate(np.array_split(np.arange(df.shape[0]),no_files)):\n",
    "        temp_df = df.iloc[idx_array,:]\n",
    "        saving_path = os.path.join(dir_path,\"{}_{}.csv\".format(key,file_idx))\n",
    "        file_path_dict[key].append(saving_path)\n",
    "        temp_df.to_csv(saving_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of filepaths for train, validate, test\n",
    "train_filepaths = file_path_dict[\"train\"]\n",
    "valid_filepaths = file_path_dict[\"valid\"]\n",
    "test_filepaths = file_path_dict[\"test\"]"
   ]
  },
  {
   "source": [
    "By default the tf.data.Dataset.list_files() returns a dataset that shuffles the file paths"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataset containing only the train filepaths\n",
    "train_filepath_dataset = tf.data.Dataset.list_files(train_filepaths,seed = 42)\n",
    "#next we can use the interleave() methods, with a number of files to read specified\n",
    "n_readers = 5\n",
    "dataset = train_filepath_dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length = n_readers)\n",
    "#the interleave method will create a dataset that will pull 5 file paths from the filepath_dataset and for each one, it calls the function we gave to create a new dataset. \n",
    "#After it runs through the first 5 filepaths, it will continue to run on the other filepaths"
   ]
  },
  {
   "source": [
    "By default interleave() does not use parallelism. It reads one line at a time from each file, sequentially. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "b'3.3456,37.0,4.514084507042254,0.9084507042253521,458.0,3.2253521126760565,36.67,-121.7,2.526'\nb'2.3,25.0,5.828178694158075,0.9587628865979382,909.0,3.1237113402061856,36.25,-119.4,1.328'\nb'4.2083,44.0,5.323204419889502,0.9171270718232044,846.0,2.3370165745856353,37.47,-122.2,2.782'\nb'4.6477,38.0,5.03728813559322,0.911864406779661,745.0,2.5254237288135593,32.64,-117.07,1.504'\nb'5.9522,26.0,6.196521739130435,1.0069565217391305,1479.0,2.5721739130434784,34.5,-119.75,4.384'\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "source": [
    "<h3> Preprocessing the data </h3> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 3.89175860e+00,  2.86245478e+01,  5.45593655e+00,  1.09963474e+00,\n",
       "        1.42428122e+03,  2.95886657e+00,  3.56464315e+01, -1.19584363e+02])"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "X_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1.90927329e+00, 1.26409177e+01, 2.55038070e+00, 4.65460128e-01,\n",
       "       1.09576000e+03, 2.36138048e+00, 2.13456672e+00, 2.00093304e+00])"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(line):\n",
    "    defs = [0.]*n_inputs + [tf.constant([],dtype = tf.float32)]\n",
    "    #first argument is the line to pass\n",
    "    #second is the default value in the column\n",
    "    #by passing an empty array in tf.constant([]), we can accept any value into this, however it will raise exception if no value is available\n",
    "    fields = tf.io.decode_csv(line,record_defaults=defs)\n",
    "    #decode_csv returns a list of scalar tensors, which needs to be stacked to give a single 1D tensor\n",
    "    x = tf.stack(fields[:-1])\n",
    "    #stack allows us to stack the list of scalar tensors into one single 1D tensor\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return (x - X_mean)/X_std, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths,n_readers = 5, n_read_threads = None, shuffle_buffer_size = 10000, n_parse_threads = 5, batch_size = 32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths)\n",
    "    dataset = dataset.interleave(lambda filepath:tf.data.TextLineDataset(filepath), cycle_length = n_readers, num_parallel_calls = n_read_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls = n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_processed = csv_reader_dataset(train_filepaths, batch_size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X = tf.Tensor(\n[[ 0.8097538   1.8491895  -0.326431   -0.01406329 -0.13258491 -0.48908564\n   1.0089018  -1.4121602 ]\n [ 0.41850558 -0.12851503  0.05890939 -0.2804167  -0.45291054  0.2643136\n  -0.64014494  0.4169884 ]\n [-0.07880411 -0.36583957  0.03906041 -0.13970348  0.29086548  0.09147053\n   1.1166518  -0.8624136 ]], shape=(3, 8), dtype=float32)\ny = tf.Tensor(\n[[5.00001]\n [1.866  ]\n [1.053  ]], shape=(3, 1), dtype=float32)\nX = tf.Tensor(\n[[-0.6853176   0.6625668  -0.50870836 -0.25073668  0.3784759   0.64644355\n  -0.8087977   0.686863  ]\n [ 0.10278316 -0.12851503  0.24828458 -0.08981109 -0.30598056 -0.14176884\n   1.3368375  -0.9273857 ]\n [ 3.5853646  -0.8404887   1.395939   -0.03880845  1.4480531   0.03106615\n   1.0370111  -1.2072539 ]], shape=(3, 8), dtype=float32)\ny = tf.Tensor(\n[[1.327  ]\n [0.968  ]\n [5.00001]], shape=(3, 1), dtype=float32)\nX = tf.Tensor(\n[[ 1.1396699   0.26702586  0.37872288 -0.24554788  0.09100419 -0.2019081\n   1.0651188  -1.3571855 ]\n [-0.9231044  -1.5524622  -0.2825846   0.1992569   0.46882415  0.31022617\n  -1.1414175   1.1716374 ]\n [ 5.818099   -0.91959685  1.2015617  -0.16691267 -0.377164    0.08910571\n  -1.2444817   1.1766324 ]], shape=(3, 8), dtype=float32)\ny = tf.Tensor(\n[[3.216  ]\n [1.514  ]\n [5.00001]], shape=(3, 1), dtype=float32)\nX = tf.Tensor(\n[[ 0.2214147   0.1879177   0.39578792 -0.10769939 -0.36803794  0.1739535\n  -0.7104158   0.8667794 ]\n [-0.519443   -0.7613805   0.28528067  0.09449202 -0.4173188  -0.16391174\n   1.4539573  -0.5575555 ]\n [-0.18104206  0.26702586 -0.5837259  -0.21992643  0.42319372  0.9314905\n  -0.8181658   0.70185536]], shape=(3, 8), dtype=float32)\ny = tf.Tensor(\n[[2.33 ]\n [1.587]\n [1.464]], shape=(3, 1), dtype=float32)\nX = tf.Tensor(\n[[ 0.69829774 -0.998705    0.018417   -0.25152883  0.03350985  0.5455396\n   0.9058376  -1.2422374 ]\n [ 1.2998356  -1.6315705  -0.04579945 -0.15801105 -1.0461061  -0.22930454\n   1.1447612  -1.3022108 ]\n [-0.45889646  0.6625668  -0.24984351 -0.10185059  0.56921107  0.07921753\n   0.7887178  -1.1422856 ]], shape=(3, 8), dtype=float32)\ny = tf.Tensor(\n[[2.137]\n [1.663]\n [2.703]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#this takes 5 batches, with each batch having 3 training examples in it\n",
    "for X_batch,y_batch in Train_processed.take(5):\n",
    "    print(\"X =\", X_batch)\n",
    "    print(\"y =\", y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "Valid_processed = csv_reader_dataset(valid_filepaths)\n",
    "Test_processed = csv_reader_dataset(test_filepaths)"
   ]
  },
  {
   "source": [
    "Look into \n",
    "- concatenate()\n",
    "- zip()\n",
    "- window()\n",
    "- reduce()\n",
    "- cache()\n",
    "- shard()\n",
    "- flat_map()\n",
    "- padded_batch()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}