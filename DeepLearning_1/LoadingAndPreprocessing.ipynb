{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit"
  },
  "interpreter": {
   "hash": "965ba1120b0c101b3f715b6e258a73742ec1cf86f2c8b04492724c87d9f112c3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<h3> Loading and Preprocessing Data with TensorFlow </h3> \n",
    "\n",
    "- Data API\n",
    "- Features API\n",
    "- tf.Transform\n",
    "- TF Datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h3> Data API </h3> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "source": [
    "The from_tensor_slices() function takes a tensor and creates a tf.data.Dataset whose elements are all the slices of X. So this dataset contains 10 items. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\ntf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\ntf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\ntf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\ntf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#repeat the dataset instance 3 times and get 7 items out of it \n",
    "dataset1 = dataset.repeat(3).batch(7)\n",
    "for item in dataset1:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\ntf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\ntf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\ntf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset2 = dataset.repeat(3).batch(7, drop_remainder=True)\n",
    "for item in dataset2:\n",
    "    print(item)"
   ]
  },
  {
   "source": [
    "dataset methods do not modify datasets. They only create new ones. Hence reference to the dataset is required"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#applying transformations or functions to the data\n",
    "dataset3 = dataset.map(lambda x: x*2)\n",
    "for item in dataset3:\n",
    "    print(item)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(4, shape=(), dtype=int32)\ntf.Tensor(6, shape=(), dtype=int32)\ntf.Tensor(8, shape=(), dtype=int32)\ntf.Tensor(10, shape=(), dtype=int32)\ntf.Tensor(12, shape=(), dtype=int32)\ntf.Tensor(14, shape=(), dtype=int32)\ntf.Tensor(16, shape=(), dtype=int32)\ntf.Tensor(18, shape=(), dtype=int32)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(4, shape=(), dtype=int32)\ntf.Tensor(6, shape=(), dtype=int32)\ntf.Tensor(8, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset4 = dataset.filter(lambda x: x%2 == 0)\n",
    "for item in dataset4:\n",
    "    print(item)"
   ]
  },
  {
   "source": [
    "<h3> Shuffling the Data </h3> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "For effective shuffing, we can split a data source to multiple files, and then pick files randomly and simultaneously read them, interleaving their lines. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "X_train_full,X_test,y_train_full,y_test = train_test_split(housing.data,housing.target.reshape(-1,1), random_state = 42)\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(X_train_full,y_train_full,random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_"
   ]
  },
  {
   "source": [
    "<h4> Splitting the data into many csv files </h4> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe of train,valid,test data\n",
    "import pandas as pd\n",
    "column_names = housing.feature_names + housing.target_names\n",
    "\n",
    "housing_train_df = pd.DataFrame(data = X_train)\n",
    "housing_train_df[\"Price\"] = y_train\n",
    "housing_train_df.columns = column_names\n",
    "\n",
    "housing_valid_df = pd.DataFrame(data = X_valid)\n",
    "housing_valid_df[\"Price\"] = y_valid\n",
    "housing_valid_df.columns = column_names\n",
    "\n",
    "housing_test_df = pd.DataFrame(data = X_test)\n",
    "housing_test_df[\"Price\"] = y_test\n",
    "housing_test_df.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_dict = {'train':housing_train_df, 'valid':housing_valid_df, 'test':housing_test_df}\n",
    "n_parts = {'train':20, 'valid':10,'test':10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "file_path_dict = dict()\n",
    "for key in dataframe_dict.keys():\n",
    "    file_path_dict[key] = list()\n",
    "    df = dataframe_dict[key]\n",
    "    no_files = n_parts[key]\n",
    "    dir_path = os.path.join(os.getcwd(),\"housing\",key)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    for file_idx,idx_array in enumerate(np.array_split(np.arange(df.shape[0]),no_files)):\n",
    "        temp_df = df.iloc[idx_array,:]\n",
    "        saving_path = os.path.join(dir_path,\"{}_{}.csv\".format(key,file_idx))\n",
    "        file_path_dict[key].append(saving_path)\n",
    "        temp_df.to_csv(saving_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of filepaths for train, validate, test\n",
    "train_filepaths = file_path_dict[\"train\"]\n",
    "valid_filepaths = file_path_dict[\"valid\"]\n",
    "test_filepaths = file_path_dict[\"test\"]"
   ]
  },
  {
   "source": [
    "By default the tf.data.Dataset.list_files() returns a dataset that shuffles the file paths"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataset containing only the train filepaths\n",
    "train_filepath_dataset = tf.data.Dataset.list_files(train_filepaths,seed = 42)\n",
    "#next we can use the interleave() methods, with a number of files to read specified\n",
    "n_readers = 5\n",
    "dataset = train_filepath_dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length = n_readers)\n",
    "#the interleave method will create a dataset that will pull 5 file paths from the filepath_dataset and for each one, it calls the function we gave to create a new dataset. \n",
    "#After it runs through the first 5 filepaths, it will continue to run on the other filepaths"
   ]
  },
  {
   "source": [
    "By default interleave() does not use parallelism. It reads one line at a time from each file, sequentially. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "b'3.3456,37.0,4.514084507042254,0.9084507042253521,458.0,3.2253521126760565,36.67,-121.7,2.526'\nb'2.3,25.0,5.828178694158075,0.9587628865979382,909.0,3.1237113402061856,36.25,-119.4,1.328'\nb'4.2083,44.0,5.323204419889502,0.9171270718232044,846.0,2.3370165745856353,37.47,-122.2,2.782'\nb'4.6477,38.0,5.03728813559322,0.911864406779661,745.0,2.5254237288135593,32.64,-117.07,1.504'\nb'5.9522,26.0,6.196521739130435,1.0069565217391305,1479.0,2.5721739130434784,34.5,-119.75,4.384'\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "source": [
    "<h3> Preprocessing the data </h3> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 3.89175860e+00,  2.86245478e+01,  5.45593655e+00,  1.09963474e+00,\n",
       "        1.42428122e+03,  2.95886657e+00,  3.56464315e+01, -1.19584363e+02])"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "X_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1.90927329e+00, 1.26409177e+01, 2.55038070e+00, 4.65460128e-01,\n",
       "       1.09576000e+03, 2.36138048e+00, 2.13456672e+00, 2.00093304e+00])"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(line):\n",
    "    defs = [0.]*n_inputs + [tf.constant([],dtype = tf.float32)]\n",
    "    #first argument is the line to pass\n",
    "    #second is the default value in the column\n",
    "    #by passing an empty array in tf.constant([]), we can accept any value into this, however it will raise exception if no value is available\n",
    "    fields = tf.io.decode_csv(line,record_defaults=defs)\n",
    "    #decode_csv returns a list of scalar tensors, which needs to be stacked to give a single 1D tensor\n",
    "    x = tf.stack(fields[:-1])\n",
    "    #stack allows us to stack the list of scalar tensors into one single 1D tensor\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return (x - X_mean)/X_std, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths,n_readers = 5, n_read_threads = None, shuffle_buffer_size = 10000, n_parse_threads = 5, batch_size = 32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths)\n",
    "    dataset = dataset.interleave(lambda filepath:tf.data.TextLineDataset(filepath), cycle_length = n_readers, num_parallel_calls = n_read_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls = n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_processed = csv_reader_dataset(train_filepaths, batch_size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X = tf.Tensor(\n[[ 0.8097538   1.8491895  -0.326431   -0.01406329 -0.13258491 -0.48908564\n   1.0089018  -1.4121602 ]\n [ 0.41850558 -0.12851503  0.05890939 -0.2804167  -0.45291054  0.2643136\n  -0.64014494  0.4169884 ]\n [-0.07880411 -0.36583957  0.03906041 -0.13970348  0.29086548  0.09147053\n   1.1166518  -0.8624136 ]], shape=(3, 8), dtype=float32)\ny = tf.Tensor(\n[[5.00001]\n [1.866  ]\n [1.053  ]], shape=(3, 1), dtype=float32)\nX = tf.Tensor(\n[[-0.6853176   0.6625668  -0.50870836 -0.25073668  0.3784759   0.64644355\n  -0.8087977   0.686863  ]\n [ 0.10278316 -0.12851503  0.24828458 -0.08981109 -0.30598056 -0.14176884\n   1.3368375  -0.9273857 ]\n [ 3.5853646  -0.8404887   1.395939   -0.03880845  1.4480531   0.03106615\n   1.0370111  -1.2072539 ]], shape=(3, 8), dtype=float32)\ny = tf.Tensor(\n[[1.327  ]\n [0.968  ]\n [5.00001]], shape=(3, 1), dtype=float32)\nX = tf.Tensor(\n[[ 1.1396699   0.26702586  0.37872288 -0.24554788  0.09100419 -0.2019081\n   1.0651188  -1.3571855 ]\n [-0.9231044  -1.5524622  -0.2825846   0.1992569   0.46882415  0.31022617\n  -1.1414175   1.1716374 ]\n [ 5.818099   -0.91959685  1.2015617  -0.16691267 -0.377164    0.08910571\n  -1.2444817   1.1766324 ]], shape=(3, 8), dtype=float32)\ny = tf.Tensor(\n[[3.216  ]\n [1.514  ]\n [5.00001]], shape=(3, 1), dtype=float32)\nX = tf.Tensor(\n[[ 0.2214147   0.1879177   0.39578792 -0.10769939 -0.36803794  0.1739535\n  -0.7104158   0.8667794 ]\n [-0.519443   -0.7613805   0.28528067  0.09449202 -0.4173188  -0.16391174\n   1.4539573  -0.5575555 ]\n [-0.18104206  0.26702586 -0.5837259  -0.21992643  0.42319372  0.9314905\n  -0.8181658   0.70185536]], shape=(3, 8), dtype=float32)\ny = tf.Tensor(\n[[2.33 ]\n [1.587]\n [1.464]], shape=(3, 1), dtype=float32)\nX = tf.Tensor(\n[[ 0.69829774 -0.998705    0.018417   -0.25152883  0.03350985  0.5455396\n   0.9058376  -1.2422374 ]\n [ 1.2998356  -1.6315705  -0.04579945 -0.15801105 -1.0461061  -0.22930454\n   1.1447612  -1.3022108 ]\n [-0.45889646  0.6625668  -0.24984351 -0.10185059  0.56921107  0.07921753\n   0.7887178  -1.1422856 ]], shape=(3, 8), dtype=float32)\ny = tf.Tensor(\n[[2.137]\n [1.663]\n [2.703]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#this takes 5 batches, with each batch having 3 training examples in it\n",
    "for X_batch,y_batch in Train_processed.take(5):\n",
    "    print(\"X =\", X_batch)\n",
    "    print(\"y =\", y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "Valid_processed = csv_reader_dataset(valid_filepaths)\n",
    "Test_processed = csv_reader_dataset(test_filepaths)"
   ]
  },
  {
   "source": [
    "Look into \n",
    "- concatenate()\n",
    "- zip()\n",
    "- window()\n",
    "- reduce()\n",
    "- cache()\n",
    "- shard()\n",
    "- flat_map()\n",
    "- padded_batch()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h3> TFRecord Format </h3> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- TensorFlow's preferred format for storing large amounts of data\n",
    "- Contains a sequence of binary records of varying sizes\n",
    "- Each record has a length, a Cycle Redundancy Check checksum to check that the length was not corrupted. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h3> Protocol buffers </h3>\n",
    "\n",
    "A portable, extensible and efficient binary format. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = b'\\n\\x05Alice\\x10{\\x1a\\x07a@b.com\\x1a\\x07c@d.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "b'\\n\\x05Alice\\x10{\\x1a\\x07a@b.com\\x1a\\x07c@d.com'"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n\\x05Alice\\x10{\\x1a\\x07a@b.com\\x1a\\x07c@d.com'"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "a.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "\n",
    "test_json = {'one':{'a':'A','b':[1,2,3]},'two':{'c':'C','d':[1,2,3]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "json_string = json.dumps(test_json)\n",
    "type(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'one': {'a': 'A', 'b': [1, 2, 3]}, 'two': {'c': 'C', 'd': [1, 2, 3]}}"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "json.loads(json_string)"
   ]
  },
  {
   "source": [
    "<h3> The Features API </h3> \n",
    "\n",
    "It lets you define how each feature or group of features should be preprocessed. \n",
    "\n",
    "tf.feature_column\n",
    "\n",
    "- Using bucketized_column to bin data\n",
    "\n",
    "Sometimes you have multimodal data where there are separate peaks in it's distribution. In these cases, you define a bucket for each mode, where the boundaries are between the peaks."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  Type  Age                Breed1  Gender Color1    Color2 MaturitySize  \\\n",
       "0  Cat    3                 Tabby    Male  Black     White        Small   \n",
       "1  Cat    1  Domestic Medium Hair    Male  Black     Brown       Medium   \n",
       "2  Dog    1           Mixed Breed    Male  Brown     White       Medium   \n",
       "3  Dog    4           Mixed Breed  Female  Black     Brown       Medium   \n",
       "4  Dog    1           Mixed Breed    Male  Black  No Color       Medium   \n",
       "\n",
       "  FurLength Vaccinated Sterilized   Health  Fee  \\\n",
       "0     Short         No         No  Healthy  100   \n",
       "1    Medium   Not Sure   Not Sure  Healthy    0   \n",
       "2    Medium        Yes         No  Healthy    0   \n",
       "3     Short        Yes         No  Healthy  150   \n",
       "4     Short         No         No  Healthy    0   \n",
       "\n",
       "                                         Description  PhotoAmt  AdoptionSpeed  \n",
       "0  Nibble is a 3+ month old ball of cuteness. He ...         1              2  \n",
       "1  I just found it alone yesterday near my apartm...         2              0  \n",
       "2  Their pregnant mother was dumped by her irresp...         7              3  \n",
       "3  Good guard dog, very alert, active, obedience ...         8              2  \n",
       "4  This handsome yet cute boy is up for adoption....         3              2  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Type</th>\n      <th>Age</th>\n      <th>Breed1</th>\n      <th>Gender</th>\n      <th>Color1</th>\n      <th>Color2</th>\n      <th>MaturitySize</th>\n      <th>FurLength</th>\n      <th>Vaccinated</th>\n      <th>Sterilized</th>\n      <th>Health</th>\n      <th>Fee</th>\n      <th>Description</th>\n      <th>PhotoAmt</th>\n      <th>AdoptionSpeed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Cat</td>\n      <td>3</td>\n      <td>Tabby</td>\n      <td>Male</td>\n      <td>Black</td>\n      <td>White</td>\n      <td>Small</td>\n      <td>Short</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Healthy</td>\n      <td>100</td>\n      <td>Nibble is a 3+ month old ball of cuteness. He ...</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Cat</td>\n      <td>1</td>\n      <td>Domestic Medium Hair</td>\n      <td>Male</td>\n      <td>Black</td>\n      <td>Brown</td>\n      <td>Medium</td>\n      <td>Medium</td>\n      <td>Not Sure</td>\n      <td>Not Sure</td>\n      <td>Healthy</td>\n      <td>0</td>\n      <td>I just found it alone yesterday near my apartm...</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dog</td>\n      <td>1</td>\n      <td>Mixed Breed</td>\n      <td>Male</td>\n      <td>Brown</td>\n      <td>White</td>\n      <td>Medium</td>\n      <td>Medium</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Healthy</td>\n      <td>0</td>\n      <td>Their pregnant mother was dumped by her irresp...</td>\n      <td>7</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Dog</td>\n      <td>4</td>\n      <td>Mixed Breed</td>\n      <td>Female</td>\n      <td>Black</td>\n      <td>Brown</td>\n      <td>Medium</td>\n      <td>Short</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Healthy</td>\n      <td>150</td>\n      <td>Good guard dog, very alert, active, obedience ...</td>\n      <td>8</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Dog</td>\n      <td>1</td>\n      <td>Mixed Breed</td>\n      <td>Male</td>\n      <td>Black</td>\n      <td>No Color</td>\n      <td>Medium</td>\n      <td>Short</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Healthy</td>\n      <td>0</td>\n      <td>This handsome yet cute boy is up for adoption....</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 84
    }
   ],
   "source": [
    "petsdf = pd.read_csv(r'C:\\Users\\ASUS\\Desktop\\Hands on ML\\Hands-on-Machine-Learning-Textbook-Exercises\\DeepLearning_1\\petfinder-mini\\petfinder-mini.csv')\n",
    "petsdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  Type  Age                Breed1  Gender Color1    Color2 MaturitySize  \\\n",
       "0  Cat    3                 Tabby    Male  Black     White        Small   \n",
       "1  Cat    1  Domestic Medium Hair    Male  Black     Brown       Medium   \n",
       "2  Dog    1           Mixed Breed    Male  Brown     White       Medium   \n",
       "3  Dog    4           Mixed Breed  Female  Black     Brown       Medium   \n",
       "4  Dog    1           Mixed Breed    Male  Black  No Color       Medium   \n",
       "\n",
       "  FurLength Vaccinated Sterilized   Health  Fee  PhotoAmt  target  \n",
       "0     Short         No         No  Healthy  100         1       1  \n",
       "1    Medium   Not Sure   Not Sure  Healthy    0         2       1  \n",
       "2    Medium        Yes         No  Healthy    0         7       1  \n",
       "3     Short        Yes         No  Healthy  150         8       1  \n",
       "4     Short         No         No  Healthy    0         3       1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Type</th>\n      <th>Age</th>\n      <th>Breed1</th>\n      <th>Gender</th>\n      <th>Color1</th>\n      <th>Color2</th>\n      <th>MaturitySize</th>\n      <th>FurLength</th>\n      <th>Vaccinated</th>\n      <th>Sterilized</th>\n      <th>Health</th>\n      <th>Fee</th>\n      <th>PhotoAmt</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Cat</td>\n      <td>3</td>\n      <td>Tabby</td>\n      <td>Male</td>\n      <td>Black</td>\n      <td>White</td>\n      <td>Small</td>\n      <td>Short</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Healthy</td>\n      <td>100</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Cat</td>\n      <td>1</td>\n      <td>Domestic Medium Hair</td>\n      <td>Male</td>\n      <td>Black</td>\n      <td>Brown</td>\n      <td>Medium</td>\n      <td>Medium</td>\n      <td>Not Sure</td>\n      <td>Not Sure</td>\n      <td>Healthy</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dog</td>\n      <td>1</td>\n      <td>Mixed Breed</td>\n      <td>Male</td>\n      <td>Brown</td>\n      <td>White</td>\n      <td>Medium</td>\n      <td>Medium</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Healthy</td>\n      <td>0</td>\n      <td>7</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Dog</td>\n      <td>4</td>\n      <td>Mixed Breed</td>\n      <td>Female</td>\n      <td>Black</td>\n      <td>Brown</td>\n      <td>Medium</td>\n      <td>Short</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>Healthy</td>\n      <td>150</td>\n      <td>8</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Dog</td>\n      <td>1</td>\n      <td>Mixed Breed</td>\n      <td>Male</td>\n      <td>Black</td>\n      <td>No Color</td>\n      <td>Medium</td>\n      <td>Short</td>\n      <td>No</td>\n      <td>No</td>\n      <td>Healthy</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "petsdf['target'] = np.where(petsdf[\"AdoptionSpeed\"] == 4,0,1)\n",
    "petsdf.drop(columns=['AdoptionSpeed', 'Description'], inplace = True)\n",
    "petsdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(petsdf, test_size=0.2, random_state=42)\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "source": [
    "Wrap the dataset under tf.data as it allows us to use feature columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dataset(df,shuffle = True, batch_size = 32):\n",
    "    df = df.copy()\n",
    "    #obtain the targets\n",
    "    labels = df.pop('target')\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(df),labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(df))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the item thats required from a tensorflow dataset\n",
    "#labels = train.pop('target')\n",
    "#list(tf.data.Dataset.from_tensor_slices((dict(train),labels)).take(1).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = df_to_dataset(train,batch_size=5)\n",
    "val_ds = df_to_dataset(val,shuffle = False, batch_size=5)\n",
    "test_ds = df_to_dataset(test,shuffle = False, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Type', 'Age', 'Breed1', 'Gender', 'Color1', 'Color2', 'MaturitySize', 'FurLength', 'Vaccinated', 'Sterilized', 'Health', 'Fee', 'PhotoAmt']\ntf.Tensor([36 24  8  3 24], shape=(5,), dtype=int64)\ntf.Tensor([1 1 0 1 1], shape=(5,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in train_ds.take(1):\n",
    "    print(list(feature_batch.keys()))\n",
    "    print(feature_batch[\"Age\"])\n",
    "    print(label_batch)"
   ]
  },
  {
   "source": [
    "Using feature columns \n",
    "\n",
    "Now we create several types of feature columns and demonstrate how they transform a column from a dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iter creates a iterable of the input object\n",
    "#then we can use next(iter(obj)) to obtain the element starting with the first one\n",
    "example_batch = next(iter(train_ds))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Type': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'Cat', b'Cat', b'Cat', b'Cat', b'Dog'], dtype=object)>,\n",
       " 'Age': <tf.Tensor: shape=(5,), dtype=int64, numpy=array([ 2, 24,  3, 60,  3], dtype=int64)>,\n",
       " 'Breed1': <tf.Tensor: shape=(5,), dtype=string, numpy=\n",
       " array([b'Domestic Long Hair', b'Persian', b'Persian', b'Burmese',\n",
       "        b'Mixed Breed'], dtype=object)>,\n",
       " 'Gender': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'Female', b'Female', b'Female', b'Male', b'Female'], dtype=object)>,\n",
       " 'Color1': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'Black', b'Golden', b'Black', b'Brown', b'Brown'], dtype=object)>,\n",
       " 'Color2': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'Brown', b'Yellow', b'Brown', b'No Color', b'White'], dtype=object)>,\n",
       " 'MaturitySize': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'Small', b'Medium', b'Large', b'Medium', b'Medium'], dtype=object)>,\n",
       " 'FurLength': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'Long', b'Medium', b'Medium', b'Long', b'Medium'], dtype=object)>,\n",
       " 'Vaccinated': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'No', b'Not Sure', b'Not Sure', b'Yes', b'Yes'], dtype=object)>,\n",
       " 'Sterilized': <tf.Tensor: shape=(5,), dtype=string, numpy=array([b'No', b'No', b'Not Sure', b'Yes', b'No'], dtype=object)>,\n",
       " 'Health': <tf.Tensor: shape=(5,), dtype=string, numpy=\n",
       " array([b'Healthy', b'Healthy', b'Healthy', b'Healthy', b'Healthy'],\n",
       "       dtype=object)>,\n",
       " 'Fee': <tf.Tensor: shape=(5,), dtype=int64, numpy=array([0, 0, 0, 0, 0], dtype=int64)>,\n",
       " 'PhotoAmt': <tf.Tensor: shape=(5,), dtype=int64, numpy=array([5, 1, 2, 1, 2], dtype=int64)>}"
      ]
     },
     "metadata": {},
     "execution_count": 138
    }
   ],
   "source": [
    "example_batch"
   ]
  },
  {
   "source": [
    "Understanding of tf.keras.layers.DenseFeatures\n",
    "\n",
    "By creating tf.feature_columns.xxx_column that does specific transformations, then adding these columns to a list, we create a list of transformations to be done on the data\n",
    "\n",
    "feature_columns.append(transformation_name)\n",
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "\n",
    "\n",
    "Then we create a dictionary to associate column names with column values\n",
    "\n",
    "inputs = {}\n",
    "\n",
    "inputs(\"temp_num\") = tf.keras.Input(shape = (1,), name = \"feature_name\")\n",
    "\n",
    "#apply the transformation\n",
    "\n",
    "x = feature_layer(inputs)\n",
    "\n",
    "Continue normally \n",
    "\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "https://stackoverflow.com/questions/54375298/how-to-use-tensorflow-feature-columns-as-input-to-a-keras-model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(feature_column):\n",
    "    feature_layer = tf.keras.layers.DenseFeatures(feature_column)\n",
    "    print(feature_layer(example_batch).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo_count= tf.feature_column.numeric_column('PhotoAmt')"
   ]
  },
  {
   "source": [
    "<h3> Bucketized columns </h3> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = tf.feature_column.numeric_column('Age')\n",
    "age_bucketized = tf.feature_column.bucketized_column(age,boundaries = [1,3,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0. 1. 0. 0.]\n [0. 0. 0. 1.]\n [0. 0. 1. 0.]\n [0. 0. 0. 1.]\n [0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "demo(age_bucketized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 2.]\n [24.]\n [ 3.]\n [60.]\n [ 3.]]\n"
     ]
    }
   ],
   "source": [
    "demo(age)"
   ]
  },
  {
   "source": [
    "<h3> Categorical columns </h3> \n",
    "\n",
    "We cannot feed strings directly into a model, hence we must first map them to numeric values. \n",
    "\n",
    "To pass in strings we can use:\n",
    "\n",
    "- categorical_column_with_vocabulary_list\n",
    "- categorical_column_with_vocabulary_file (if loading the categories from a list)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1. 0.]\n [1. 0.]\n [1. 0.]\n [1. 0.]\n [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#converts into numerical values\n",
    "animal_type = tf.feature_column.categorical_column_with_vocabulary_list('Type', ['Cat','Dog'])\n",
    "#converts into one-hot vector\n",
    "animal_type_one_hot = tf.feature_column.indicator_column(animal_type)\n",
    "demo(animal_type_one_hot)"
   ]
  },
  {
   "source": [
    "<h3> Embedding Columns </h3> \n",
    "\n",
    "If we have thousands or more values per category, as the number of categories grows large, it becomes infeasible to train a neural network using one-hot encodings. Embedding represents the data as a lower dimensional dense vector. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 0.01577405  0.07334787  0.15563081  0.06314064  0.61830914]\n [ 0.41535613 -0.14529578  0.27263784  0.6519023  -0.30284292]\n [ 0.41535613 -0.14529578  0.27263784  0.6519023  -0.30284292]\n [-0.16950928  0.52377564  0.3231648  -0.18466276 -0.3466285 ]\n [ 0.09416489 -0.02737571  0.5676732   0.08034084  0.40011677]]\n"
     ]
    }
   ],
   "source": [
    "breed1 = tf.feature_column.categorical_column_with_vocabulary_list('Breed1', train[\"Breed1\"].unique().tolist())\n",
    "breed1_embedding = tf.feature_column.embedding_column(breed1, dimension=5)\n",
    "demo(breed1_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Beagle',\n",
       " 'Domestic Medium Hair',\n",
       " 'Dachshund',\n",
       " 'Domestic Short Hair',\n",
       " 'Siamese',\n",
       " 'Mixed Breed',\n",
       " 'Labrador Retriever',\n",
       " 'Tabby',\n",
       " 'Persian',\n",
       " 'Chihuahua',\n",
       " 'Abyssinian',\n",
       " 'Siberian Husky',\n",
       " 'Shih Tzu',\n",
       " 'German Shepherd Dog',\n",
       " 'Oriental Short Hair',\n",
       " 'Yorkshire Terrier Yorkie',\n",
       " 'Bengal',\n",
       " 'Domestic Long Hair',\n",
       " 'Golden Retriever',\n",
       " 'Corgi',\n",
       " 'Doberman Pinscher',\n",
       " 'Calico',\n",
       " 'Terrier',\n",
       " 'Poodle',\n",
       " 'American Shorthair',\n",
       " 'Great Dane',\n",
       " 'Miniature Pinscher',\n",
       " 'Schnauzer',\n",
       " 'Irish Setter',\n",
       " 'Rottweiler',\n",
       " 'Dalmatian',\n",
       " 'Black Labrador Retriever',\n",
       " 'Spitz',\n",
       " 'Tiger',\n",
       " 'Bobtail',\n",
       " 'Jack Russell Terrier',\n",
       " 'Cymric',\n",
       " 'Tortoiseshell',\n",
       " 'Rhodesian Ridgeback',\n",
       " 'Husky',\n",
       " 'Silky Terrier',\n",
       " 'Boston Terrier',\n",
       " 'British Shorthair',\n",
       " 'Collie',\n",
       " 'Basenji',\n",
       " 'Tuxedo',\n",
       " 'English Pointer',\n",
       " 'Cocker Spaniel',\n",
       " 'Ragdoll',\n",
       " 'Japanese Bobtail',\n",
       " 'Bombay',\n",
       " 'Belgian Shepherd Laekenois',\n",
       " 'Lhasa Apso',\n",
       " 'Saint Bernard',\n",
       " 'Border Collie',\n",
       " 'Australian Terrier',\n",
       " 'Burmese',\n",
       " 'Manx',\n",
       " 'West Highland White Terrier Westie',\n",
       " 'Rat Terrier',\n",
       " 'Australian Kelpie',\n",
       " 'German Spitz',\n",
       " 'Pit Bull Terrier',\n",
       " 'Shepherd',\n",
       " 'Maltese',\n",
       " 'Shar Pei',\n",
       " 'American Curl',\n",
       " 'Bull Terrier',\n",
       " 'Siberian',\n",
       " 'Boxer',\n",
       " '0',\n",
       " 'Pug',\n",
       " 'Basset Hound',\n",
       " 'Toy Fox Terrier',\n",
       " 'Chow Chow',\n",
       " 'Belgian Shepherd Malinois',\n",
       " 'Mastiff',\n",
       " 'Irish Wolfhound',\n",
       " 'Munsterlander',\n",
       " 'Jack Russell Terrier (Parson Russell Terrier)',\n",
       " 'Pekingese',\n",
       " 'Oriental Long Hair',\n",
       " 'Maine Coon',\n",
       " 'Welsh Corgi',\n",
       " 'American Bulldog',\n",
       " 'Tonkinese',\n",
       " 'Russian Blue',\n",
       " 'Snowshoe',\n",
       " 'English Cocker Spaniel',\n",
       " 'Greyhound',\n",
       " 'Retriever',\n",
       " 'Yellow Labrador Retriever',\n",
       " 'Weimaraner',\n",
       " 'Silver',\n",
       " 'Dilute Calico',\n",
       " 'Somali',\n",
       " 'Whippet',\n",
       " 'Pomeranian',\n",
       " 'Burmilla',\n",
       " 'German Pinscher',\n",
       " 'Cavalier King Charles Spaniel',\n",
       " 'Oriental Tabby',\n",
       " 'Birman',\n",
       " 'Himalayan',\n",
       " 'Applehead Siamese',\n",
       " 'Shiba Inu',\n",
       " 'Javanese',\n",
       " 'Bullmastiff',\n",
       " 'Hound',\n",
       " 'English Springer Spaniel',\n",
       " 'Mountain Dog',\n",
       " 'Turkish Van',\n",
       " 'Pointer',\n",
       " 'Coonhound',\n",
       " 'French Bulldog',\n",
       " 'Singapura',\n",
       " 'Akita',\n",
       " 'Flat-coated Retriever',\n",
       " 'Samoyed',\n",
       " 'Old English Sheepdog',\n",
       " 'Havana',\n",
       " 'Standard Poodle',\n",
       " 'American Water Spaniel',\n",
       " 'English Bulldog',\n",
       " 'Foxhound',\n",
       " 'Kai Dog',\n",
       " 'Shetland Sheepdog Sheltie',\n",
       " 'Balinese',\n",
       " 'Fox Terrier',\n",
       " 'American Wirehair',\n",
       " 'Dilute Tortoiseshell',\n",
       " 'Turkish Angora',\n",
       " 'Korat',\n",
       " 'Wirehaired Terrier',\n",
       " 'Norwegian Forest Cat',\n",
       " 'American Staffordshire Terrier',\n",
       " 'Black Mouth Cur',\n",
       " 'Staffordshire Bull Terrier',\n",
       " 'Ocicat',\n",
       " 'Chausie',\n",
       " 'Torbie',\n",
       " 'Sphynx (hairless cat)',\n",
       " 'Swedish Vallhund',\n",
       " 'Egyptian Mau',\n",
       " 'Manchester Terrier',\n",
       " 'Exotic Shorthair',\n",
       " 'Ragamuffin',\n",
       " 'Glen of Imaal Terrier',\n",
       " 'Chinese Crested Dog']"
      ]
     },
     "metadata": {},
     "execution_count": 171
    }
   ],
   "source": [
    "train[\"Breed1\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}