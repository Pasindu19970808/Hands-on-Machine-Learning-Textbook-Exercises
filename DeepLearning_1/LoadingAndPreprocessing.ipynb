{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit"
  },
  "interpreter": {
   "hash": "965ba1120b0c101b3f715b6e258a73742ec1cf86f2c8b04492724c87d9f112c3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<h3> Loading and Preprocessing Data with TensorFlow </h3> \n",
    "\n",
    "- Data API\n",
    "- Features API\n",
    "- tf.Transform\n",
    "- TF Datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h3> Data API </h3> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "source": [
    "The from_tensor_slices() function takes a tensor and creates a tf.data.Dataset whose elements are all the slices of X. So this dataset contains 10 items. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\ntf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\ntf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\ntf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\ntf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#repeat the dataset instance 3 times and get 7 items out of it \n",
    "dataset1 = dataset.repeat(3).batch(7)\n",
    "for item in dataset1:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\ntf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\ntf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\ntf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset2 = dataset.repeat(3).batch(7, drop_remainder=True)\n",
    "for item in dataset2:\n",
    "    print(item)"
   ]
  },
  {
   "source": [
    "dataset methods do not modify datasets. They only create new ones. Hence reference to the dataset is required"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#applying transformations or functions to the data\n",
    "dataset3 = dataset.map(lambda x: x*2)\n",
    "for item in dataset3:\n",
    "    print(item)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(4, shape=(), dtype=int32)\ntf.Tensor(6, shape=(), dtype=int32)\ntf.Tensor(8, shape=(), dtype=int32)\ntf.Tensor(10, shape=(), dtype=int32)\ntf.Tensor(12, shape=(), dtype=int32)\ntf.Tensor(14, shape=(), dtype=int32)\ntf.Tensor(16, shape=(), dtype=int32)\ntf.Tensor(18, shape=(), dtype=int32)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(4, shape=(), dtype=int32)\ntf.Tensor(6, shape=(), dtype=int32)\ntf.Tensor(8, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset4 = dataset.filter(lambda x: x%2 == 0)\n",
    "for item in dataset4:\n",
    "    print(item)"
   ]
  },
  {
   "source": [
    "<h3> Shuffling the Data </h3> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "For effective shuffing, we can split a data source to multiple files, and then pick files randomly and simultaneously read them, interleaving their lines. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "X_train_full,X_test,y_train_full,y_test = train_test_split(housing.data,housing.target.reshape(-1,1), random_state = 42)\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(X_train_full,y_train_full,random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_"
   ]
  },
  {
   "source": [
    "<h4> Splitting the data into many csv files </h4> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe of train,valid,test data\n",
    "import pandas as pd\n",
    "column_names = housing.feature_names + housing.target_names\n",
    "\n",
    "housing_train_df = pd.DataFrame(data = X_train)\n",
    "housing_train_df[\"Price\"] = y_train\n",
    "housing_train_df.columns = column_names\n",
    "\n",
    "housing_valid_df = pd.DataFrame(data = X_valid)\n",
    "housing_valid_df[\"Price\"] = y_valid\n",
    "housing_valid_df.columns = column_names\n",
    "\n",
    "housing_test_df = pd.DataFrame(data = X_test)\n",
    "housing_test_df[\"Price\"] = y_test\n",
    "housing_test_df.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_dict = {'train':housing_train_df, 'valid':housing_valid_df, 'test':housing_test_df}\n",
    "n_parts = {'train':20, 'valid':10,'test':10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "file_path_dict = dict()\n",
    "for key in dataframe_dict.keys():\n",
    "    file_path_dict[key] = list()\n",
    "    df = dataframe_dict[key]\n",
    "    no_files = n_parts[key]\n",
    "    dir_path = os.path.join(os.getcwd(),\"housing\",key)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    for file_idx,idx_array in enumerate(np.array_split(np.arange(df.shape[0]),no_files)):\n",
    "        temp_df = df.iloc[idx_array,:]\n",
    "        saving_path = os.path.join(dir_path,\"{}_{}.csv\".format(key,file_idx))\n",
    "        file_path_dict[key].append(saving_path)\n",
    "        temp_df.to_csv(saving_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of filepaths for train, validate, test\n",
    "train_filepaths = file_path_dict[\"train\"]\n",
    "valid_filepaths = file_path_dict[\"valid\"]\n",
    "test_filepaths = file_path_dict[\"test\"]"
   ]
  },
  {
   "source": [
    "By default the tf.data.Dataset.list_files() returns a dataset that shuffles the file paths"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataset containing only the train filepaths\n",
    "train_filepath_dataset = tf.data.Dataset.list_files(train_filepaths,seed = 42)\n",
    "#next we can use the interleave() methods, with a number of files to read specified\n",
    "n_readers = 5\n",
    "dataset = train_filepath_dataset.interleave(lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length = n_readers)\n",
    "#the interleave method will create a dataset that will pull 5 file paths from the filepath_dataset and for each one, it calls the function we gave to create a new dataset. \n",
    "#After it runs through the first 5 filepaths, it will continue to run on the other filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3.9688,41.0,5.259786476868327,0.9715302491103203,916.0,3.2597864768683276,33.98,-118.07,1.698\n3.6641,17.0,5.577142857142857,1.1542857142857144,511.0,2.92,40.85,-121.07,0.808\n2.1856,41.0,3.7189873417721517,1.0658227848101265,803.0,2.0329113924050635,32.76,-117.12,1.205\n3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442\n4.7361,7.0,7.464968152866242,1.1178343949044587,846.0,2.694267515923567,34.49,-117.27,1.745\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line.numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}