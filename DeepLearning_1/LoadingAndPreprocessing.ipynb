{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit"
  },
  "interpreter": {
   "hash": "965ba1120b0c101b3f715b6e258a73742ec1cf86f2c8b04492724c87d9f112c3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<h3> Loading and Preprocessing Data with TensorFlow </h3> \n",
    "\n",
    "- Data API\n",
    "- Features API\n",
    "- tf.Transform\n",
    "- TF Datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h3> Data API </h3> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "source": [
    "The from_tensor_slices() function takes a tensor and creates a tf.data.Dataset whose elements are all the slices of X. So this dataset contains 10 items. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\ntf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\ntf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\ntf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\ntf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#repeat the dataset instance 3 times and get 7 items out of it \n",
    "dataset1 = dataset.repeat(3).batch(7)\n",
    "for item in dataset1:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\ntf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\ntf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\ntf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset2 = dataset.repeat(3).batch(7, drop_remainder=True)\n",
    "for item in dataset2:\n",
    "    print(item)"
   ]
  },
  {
   "source": [
    "dataset methods do not modify datasets. They only create new ones. Hence reference to the dataset is required"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#applying transformations or functions to the data\n",
    "dataset3 = dataset.map(lambda x: x*2)\n",
    "for item in dataset3:\n",
    "    print(item)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(4, shape=(), dtype=int32)\ntf.Tensor(6, shape=(), dtype=int32)\ntf.Tensor(8, shape=(), dtype=int32)\ntf.Tensor(10, shape=(), dtype=int32)\ntf.Tensor(12, shape=(), dtype=int32)\ntf.Tensor(14, shape=(), dtype=int32)\ntf.Tensor(16, shape=(), dtype=int32)\ntf.Tensor(18, shape=(), dtype=int32)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(4, shape=(), dtype=int32)\ntf.Tensor(6, shape=(), dtype=int32)\ntf.Tensor(8, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset4 = dataset.filter(lambda x: x%2 == 0)\n",
    "for item in dataset4:\n",
    "    print(item)"
   ]
  },
  {
   "source": [
    "<h3> Shuffling the Data </h3> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "For effective shuffing, we can split a data source to multiple files, and then pick files randomly and simultaneously read them, interleaving their lines. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "X_train_full,X_test,y_train_full,y_test = train_test_split(housing.data,housing.target.reshape(-1,1), random_state = 42)\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(X_train_full,y_train_full,random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_"
   ]
  },
  {
   "source": [
    "<h4> Splitting the data into many csv files </h4> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe of train,valid,test data\n",
    "import pandas as pd\n",
    "column_names = housing.feature_names + housing.target_names\n",
    "\n",
    "housing_train_df = pd.DataFrame(data = X_train)\n",
    "housing_train_df[\"Price\"] = y_train\n",
    "housing_train_df.columns = column_names\n",
    "\n",
    "housing_valid_df = pd.DataFrame(data = X_valid)\n",
    "housing_valid_df[\"Price\"] = y_valid\n",
    "housing_valid_df.columns = column_names\n",
    "\n",
    "housing_test_df = pd.DataFrame(data = X_test)\n",
    "housing_test_df[\"Price\"] = y_test\n",
    "housing_test_df.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_dict = {'train':housing_train_df, 'valid':housing_valid_df, 'test':housing_test_df}\n",
    "n_parts = {'train':20, 'valid':10,'test':10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'c:\\\\Users\\\\ASUS\\\\Desktop\\\\Hands on ML\\\\Hands-on-Machine-Learning-Textbook-Exercises\\\\DeepLearning_1\\\\housing\\\\train'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-b407954cc956>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfile_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0midx_array\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mno_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mtemp_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_array\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mtemp_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaving_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"{}_{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3385\u001b[0m         )\n\u001b[0;32m   3386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3387\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3388\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3389\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1081\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m         )\n\u001b[1;32m-> 1083\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1084\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m         \"\"\"\n\u001b[0;32m    227\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    229\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'c:\\\\Users\\\\ASUS\\\\Desktop\\\\Hands on ML\\\\Hands-on-Machine-Learning-Textbook-Exercises\\\\DeepLearning_1\\\\housing\\\\train'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "for key in dataframe_dict.keys():\n",
    "    df = dataframe_dict[key]\n",
    "    no_files = n_parts[key]\n",
    "    saving_path = os.path.join(os.getcwd(),\"housing\",key)\n",
    "    if not os.path.exists(saving_path):\n",
    "        os.makedirs(saving_path)\n",
    "    for file_idx,idx_array in enumerate(np.array_split(np.arange(df.shape[0]),no_files)):\n",
    "        temp_df = df.iloc[idx_array,:]\n",
    "        temp_df.to_csv(saving_path,\"{}_{}\".format(key,file_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "580"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "housing_train_df.shape[0]//20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(19,\n",
       " array([11030, 11031, 11032, 11033, 11034, 11035, 11036, 11037, 11038,\n",
       "        11039, 11040, 11041, 11042, 11043, 11044, 11045, 11046, 11047,\n",
       "        11048, 11049, 11050, 11051, 11052, 11053, 11054, 11055, 11056,\n",
       "        11057, 11058, 11059, 11060, 11061, 11062, 11063, 11064, 11065,\n",
       "        11066, 11067, 11068, 11069, 11070, 11071, 11072, 11073, 11074,\n",
       "        11075, 11076, 11077, 11078, 11079, 11080, 11081, 11082, 11083,\n",
       "        11084, 11085, 11086, 11087, 11088, 11089, 11090, 11091, 11092,\n",
       "        11093, 11094, 11095, 11096, 11097, 11098, 11099, 11100, 11101,\n",
       "        11102, 11103, 11104, 11105, 11106, 11107, 11108, 11109, 11110,\n",
       "        11111, 11112, 11113, 11114, 11115, 11116, 11117, 11118, 11119,\n",
       "        11120, 11121, 11122, 11123, 11124, 11125, 11126, 11127, 11128,\n",
       "        11129, 11130, 11131, 11132, 11133, 11134, 11135, 11136, 11137,\n",
       "        11138, 11139, 11140, 11141, 11142, 11143, 11144, 11145, 11146,\n",
       "        11147, 11148, 11149, 11150, 11151, 11152, 11153, 11154, 11155,\n",
       "        11156, 11157, 11158, 11159, 11160, 11161, 11162, 11163, 11164,\n",
       "        11165, 11166, 11167, 11168, 11169, 11170, 11171, 11172, 11173,\n",
       "        11174, 11175, 11176, 11177, 11178, 11179, 11180, 11181, 11182,\n",
       "        11183, 11184, 11185, 11186, 11187, 11188, 11189, 11190, 11191,\n",
       "        11192, 11193, 11194, 11195, 11196, 11197, 11198, 11199, 11200,\n",
       "        11201, 11202, 11203, 11204, 11205, 11206, 11207, 11208, 11209,\n",
       "        11210, 11211, 11212, 11213, 11214, 11215, 11216, 11217, 11218,\n",
       "        11219, 11220, 11221, 11222, 11223, 11224, 11225, 11226, 11227,\n",
       "        11228, 11229, 11230, 11231, 11232, 11233, 11234, 11235, 11236,\n",
       "        11237, 11238, 11239, 11240, 11241, 11242, 11243, 11244, 11245,\n",
       "        11246, 11247, 11248, 11249, 11250, 11251, 11252, 11253, 11254,\n",
       "        11255, 11256, 11257, 11258, 11259, 11260, 11261, 11262, 11263,\n",
       "        11264, 11265, 11266, 11267, 11268, 11269, 11270, 11271, 11272,\n",
       "        11273, 11274, 11275, 11276, 11277, 11278, 11279, 11280, 11281,\n",
       "        11282, 11283, 11284, 11285, 11286, 11287, 11288, 11289, 11290,\n",
       "        11291, 11292, 11293, 11294, 11295, 11296, 11297, 11298, 11299,\n",
       "        11300, 11301, 11302, 11303, 11304, 11305, 11306, 11307, 11308,\n",
       "        11309, 11310, 11311, 11312, 11313, 11314, 11315, 11316, 11317,\n",
       "        11318, 11319, 11320, 11321, 11322, 11323, 11324, 11325, 11326,\n",
       "        11327, 11328, 11329, 11330, 11331, 11332, 11333, 11334, 11335,\n",
       "        11336, 11337, 11338, 11339, 11340, 11341, 11342, 11343, 11344,\n",
       "        11345, 11346, 11347, 11348, 11349, 11350, 11351, 11352, 11353,\n",
       "        11354, 11355, 11356, 11357, 11358, 11359, 11360, 11361, 11362,\n",
       "        11363, 11364, 11365, 11366, 11367, 11368, 11369, 11370, 11371,\n",
       "        11372, 11373, 11374, 11375, 11376, 11377, 11378, 11379, 11380,\n",
       "        11381, 11382, 11383, 11384, 11385, 11386, 11387, 11388, 11389,\n",
       "        11390, 11391, 11392, 11393, 11394, 11395, 11396, 11397, 11398,\n",
       "        11399, 11400, 11401, 11402, 11403, 11404, 11405, 11406, 11407,\n",
       "        11408, 11409, 11410, 11411, 11412, 11413, 11414, 11415, 11416,\n",
       "        11417, 11418, 11419, 11420, 11421, 11422, 11423, 11424, 11425,\n",
       "        11426, 11427, 11428, 11429, 11430, 11431, 11432, 11433, 11434,\n",
       "        11435, 11436, 11437, 11438, 11439, 11440, 11441, 11442, 11443,\n",
       "        11444, 11445, 11446, 11447, 11448, 11449, 11450, 11451, 11452,\n",
       "        11453, 11454, 11455, 11456, 11457, 11458, 11459, 11460, 11461,\n",
       "        11462, 11463, 11464, 11465, 11466, 11467, 11468, 11469, 11470,\n",
       "        11471, 11472, 11473, 11474, 11475, 11476, 11477, 11478, 11479,\n",
       "        11480, 11481, 11482, 11483, 11484, 11485, 11486, 11487, 11488,\n",
       "        11489, 11490, 11491, 11492, 11493, 11494, 11495, 11496, 11497,\n",
       "        11498, 11499, 11500, 11501, 11502, 11503, 11504, 11505, 11506,\n",
       "        11507, 11508, 11509, 11510, 11511, 11512, 11513, 11514, 11515,\n",
       "        11516, 11517, 11518, 11519, 11520, 11521, 11522, 11523, 11524,\n",
       "        11525, 11526, 11527, 11528, 11529, 11530, 11531, 11532, 11533,\n",
       "        11534, 11535, 11536, 11537, 11538, 11539, 11540, 11541, 11542,\n",
       "        11543, 11544, 11545, 11546, 11547, 11548, 11549, 11550, 11551,\n",
       "        11552, 11553, 11554, 11555, 11556, 11557, 11558, 11559, 11560,\n",
       "        11561, 11562, 11563, 11564, 11565, 11566, 11567, 11568, 11569,\n",
       "        11570, 11571, 11572, 11573, 11574, 11575, 11576, 11577, 11578,\n",
       "        11579, 11580, 11581, 11582, 11583, 11584, 11585, 11586, 11587,\n",
       "        11588, 11589, 11590, 11591, 11592, 11593, 11594, 11595, 11596,\n",
       "        11597, 11598, 11599, 11600, 11601, 11602, 11603, 11604, 11605,\n",
       "        11606, 11607, 11608, 11609]))"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "import numpy as np\n",
    "tuple(enumerate(np.array_split(np.arange(housing_train_df.shape[0]),20)))[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}