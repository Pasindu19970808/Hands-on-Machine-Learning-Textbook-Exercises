{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0965ba1120b0c101b3f715b6e258a73742ec1cf86f2c8b04492724c87d9f112c3",
   "display_name": "Python 3.8.8 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "965ba1120b0c101b3f715b6e258a73742ec1cf86f2c8b04492724c87d9f112c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<h2> Training Deep Neural Networks </h2> \n",
    "\n",
    "To tackle complex problems, we need to train much deeper DNN with 10 layers or much more, each containing hundreds of neurons. \n",
    "\n",
    "<h3> Issues faced when training DNN </h3> \n",
    "\n",
    "- Vanishing gradient/ exploding gradients\n",
    "- Not enough training data for a large network\n",
    "- Training will be extemely slow\n",
    "- A model with millions of parameters would risk overfitting the training set if there is not enough instances. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h3> Vanishing/Exploding Gradients Problems </h3> \n",
    "\n",
    "- The backpropagation algorithm works by going from the output layer to the input layer, propagating the error gradient on the way. \n",
    "- Once the gradient for each parameter is calculated, the gradients are used to update each parameter. \n",
    "- **However** gradients get smaller and smaller as the algorithm progresses to the lower layers. \n",
    "\n",
    "- Due to this vanishing gradient, the weights and biases of the lower layers stay unchanged. \n",
    "\n",
    "\n",
    "**The opposite can happen too**\n",
    "\n",
    "- In some cases, the gradient gets bigger when moving to lower layers, hence the lower layers end up getting large weight updates and the algorithm diverges. \n",
    "\n",
    "\n",
    "In a general sense, deep neural networks suffer from unstable gradients throughout the network resulting in different layers learning at different speeds. \n",
    "\n",
    "Issues which contribute to this:\n",
    "\n",
    "- Combination of logistic sigmoid activation function and the weight initialization technique (Random initialization using a normal distribution with a mean of 0 and a standard dev. of 1)\n",
    "\n",
    "- Looking at the sigmoid activation function, it saturates at large positive and negative inputs and the function saturates at 0 and 1. At these saturated positions, the gradient is close to zero and hence cannot back propagate through a network. Hence there is not enough to be backpropagated to lower layers.\n",
    "\n",
    "\n",
    "<h3> Vanishing gradient can occur in multiple ways </h3> \n",
    "\n",
    "Either you can have your gradient decreasing as you reach lower layers during backpropagation as you are running out of gradient, or your outputs from the neuron are always zero causing no gradient to exist (Usually happens when your weights are giving out zeros as the output for neuron)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h3> Glorot and He Initialization </h3> \n",
    "\n",
    "For signals to flow properly during forward pass and back propagation, it is required that the variance of the input be the same as the variance of the output. \n",
    "\n",
    "It is not possible to guarantee this condition, unless the layer has an equal number of inputs and neurons. Instead a valid compromise is to **randomly initialize the connection weights of each layer as follows**:\n",
    "\n",
    "fan(avg) = (fan(in) + fan(out))/2\n",
    "\n",
    "Different strategies exist, however they differ only by the scale of variance and whether they use fan(avg) or fan(in)\n",
    "\n",
    "For all the different initialization strategies, we need to plug in the appropriate variance equation which is dependent on **fan(avg) or fan(in)**. Then we can get the boundary of initialization and find the initialized weights and biases using uniform distribution. \n",
    "\n",
    "By default Keras uses Glorot initialization where we use 1/fan(avg) as the variance to plug into find the boundaries for uniform distribution. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h3> Nonsaturating Activation Functions </h3> \n",
    "\n",
    "<h4> Dying ReLU </h4>\n",
    "\n",
    "Dying ReLU are neurons which are outputing 0s continuously. This happens when all the weights of your network(or weights for most neurons) are tweaked such that every output is a negative, causing the ReLU output to be a zero always. \n",
    "\n",
    "A variant of the ReLU function is the Leaky ReLU function. \n",
    "\n",
    "In Leaky ReLU, instead of max(0,z), we have max(a * z, z) in which **a** is the hyperparameter which defines how much the function leaks. It is the slope of the function below z = 0 and usually set to 0.01. \n",
    "\n",
    "\n",
    "<h4> Exponential Linear Unit </h4> \n",
    "\n",
    "In this activation function, if z > 0, then the output is z. However if z < 0, output is a(exp(z) - 1). For all values below 0, it takes on a negative value, has a nonzero gradient which avoids dead neuron problem, and closer to zero, the function is smooth, hence it doesnt bounce around during convergence and fuaster in training than the ReLU. But it is slower in prediction as the function is more complicated.\n",
    "\n",
    "<h4> Standardized Exponential Linear Unit </h4> \n",
    "\n",
    "This activation function allows the network to self normalize where the weights are with standard dev of 1 and mean 0. But the following conditions are required for that to happen:\n",
    "\n",
    "- Input features must be Standardized (mean 0 and standard dev of 1).\n",
    "- Every hidden layers weight must be initialized under the **LeCunn Normal Initialization**\n",
    "- Sequential API needs to be used or else normalization is not guaranteed. \n",
    "\n",
    "\n",
    "Effectiveness of activation functions:\n",
    "\n",
    "SELU>ELU>LeakReLU(and variants)>ReLU>tanh>sigmoid"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h2> Batch Normalization(BN) </h2> \n",
    "\n",
    "Using the ELU or LeakyReLU and its variants, we reduce the problem of vanishing/exploding gradients as we ensure that neurons wont die and that weights and biases are in a range which prevents explosion. But it doesnt guarantee that it wont come back during training. \n",
    "\n",
    "Involves adding an operation in the model before or after the activation function of each layer, to zero center and normalize each input. Then scaling and shifting the result using 2 new parameter vectors per layer. \n",
    "\n",
    "Essentially for each batch, you normalize each feature to have mean 0 and sd 1. Then you do element-wise multiplication with gamma vector in which gamma is a scaling factor applied for each feature of each input. Then you shift it with a vector beta, which also is available for the entire vector. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h2> Implementing Batch Normalization </h2> \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full),(X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid,X_train = X_train_full[:5000]/255.0,X_train_full[5000:]/255.0\n",
    "y_valid,y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bn = keras.models.Sequential()\n",
    "model_bn.add(keras.layers.Flatten(input_shape = [28,28]))\n",
    "#add batch normalization layer\n",
    "model_bn.add(keras.layers.BatchNormalization())\n",
    "model_bn.add(keras.layers.Dense(300,activation = \"elu\",kernel_initializer = \"he_normal\"))\n",
    "#above initializer can also be made as follows:\n",
    "#he_normal_init = keras.initializers.VarianceScaling(scale = 2, mode = \"fan_in\", distribution = \"normal\")\n",
    "model_bn.add(keras.layers.BatchNormalization())\n",
    "model_bn.add(keras.layers.Dense(100,activation = \"elu\",kernel_initializer = \"he_normal\"))\n",
    "model_bn.add(keras.layers.BatchNormalization())\n",
    "model_bn.add(keras.layers.Dense(10,activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten (Flatten)            (None, 784)               0         \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 784)               3136      \n_________________________________________________________________\ndense (Dense)                (None, 300)               235500    \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 300)               1200      \n_________________________________________________________________\ndense_1 (Dense)              (None, 100)               30100     \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 100)               400       \n_________________________________________________________________\ndense_2 (Dense)              (None, 10)                1010      \n=================================================================\nTotal params: 271,346\nTrainable params: 268,978\nNon-trainable params: 2,368\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "#variables of the batch normalization layer\n",
    "[(var.name, var.trainable) for var in model_bn.layers[1].variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding batch normalization before the activation function\n",
    "#notice that we put the activation function always after the batch normalization function\n",
    "model_bn_2 = keras.models.Sequential()\n",
    "model_bn_2.add(keras.layers.Flatten(input_shape = [28,28]))\n",
    "model_bn_2.add(keras.layers.BatchNormalization())\n",
    "#as adding a batch norm results in the bias being redundant as it gets cancelled out later, we say use_bias = False.\n",
    "model_bn_2.add(keras.layers.Dense(300,kernel_initializer = \"he_normal\", use_bias = False))\n",
    "model_bn_2.add(keras.layers.BatchNormalization())\n",
    "model_bn_2.add(keras.layers.Activation(\"elu\"))\n",
    "model_bn_2.add(keras.layers.Dense(100,kernel_initializer = \"he_normal\",use_bias = False))\n",
    "model_bn_2.add(keras.layers.BatchNormalization())\n",
    "model_bn_2.add(keras.layers.Activation(\"elu\"))\n",
    "model_bn_2.add(keras.layers.Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten_2 (Flatten)          (None, 784)               0         \n_________________________________________________________________\nbatch_normalization_6 (Batch (None, 784)               3136      \n_________________________________________________________________\ndense_6 (Dense)              (None, 300)               235200    \n_________________________________________________________________\nbatch_normalization_7 (Batch (None, 300)               1200      \n_________________________________________________________________\nactivation_2 (Activation)    (None, 300)               0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 100)               30000     \n_________________________________________________________________\nbatch_normalization_8 (Batch (None, 100)               400       \n_________________________________________________________________\nactivation_3 (Activation)    (None, 100)               0         \n_________________________________________________________________\ndense_8 (Dense)              (None, 10)                1010      \n=================================================================\nTotal params: 270,946\nTrainable params: 268,578\nNon-trainable params: 2,368\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bn_2.summary()"
   ]
  },
  {
   "source": [
    "<h3> Gradient Clipping </h3> \n",
    "\n",
    "To prevent the gradients from exploding, we can clip the gradients during backpropagation so that they never exceed a threshold. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h2> Reusing Pretrained Layers </h2> \n",
    "\n",
    "It is not advised to train very large Deep Neural Networks from scratch. Instead it is better to find an existing neural network that accomplishes a similar task to what is required from your project. Then use the lower layers of the old network.\n",
    "\n",
    "Generally, if your input to the already trained layers is of a different size of input to the data you now have, it needs to be reshaped. \n",
    "\n",
    "Identifying the useful layers:\n",
    "- Freeze all the reused layers(make weights non trainable and pass in the input to see how it performs)\n",
    "- Train your model and see how it performs \n",
    "- Unfreeze one or two of the top hidden layers to let back propoagation change these weights of the top layers to see if there is performance improvement. Use lower learning rates to see to avoid drastic changes of existing weights. \n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the MNIST algo with a few classes only. then use it to train the other classes using transfer learning\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full_init, y_train_full_init),(X_test_init, y_test_init) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([9, 0, 0, 3, 0, 2, 7, 2, 5, 5], dtype=uint8)"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "#select all classes except for sandals and shirts\n",
    "y_train_full_init[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "\"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x27385c2c550>"
      ]
     },
     "metadata": {},
     "execution_count": 51
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-04-22T07:37:14.355298</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 251.565 248.518125 \r\nL 251.565 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 26.925 224.64 \r\nL 244.365 224.64 \r\nL 244.365 7.2 \r\nL 26.925 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g clip-path=\"url(#pba1a42772c)\">\r\n    <image height=\"218\" id=\"image4943dce3b1\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\r\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\" y=\"-6.64\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"ma7b9149603\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#ma7b9149603\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#ma7b9149603\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#ma7b9149603\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#ma7b9149603\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#ma7b9149603\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#ma7b9149603\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m5e6c1e7a27\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5e6c1e7a27\" y=\"11.082857\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5e6c1e7a27\" y=\"49.911429\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5e6c1e7a27\" y=\"88.74\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5e6c1e7a27\" y=\"127.568571\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5e6c1e7a27\" y=\"166.397143\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5e6c1e7a27\" y=\"205.225714\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 26.925 224.64 \r\nL 26.925 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 244.365 224.64 \r\nL 244.365 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 26.925 224.64 \r\nL 244.365 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 26.925 7.2 \r\nL 244.365 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pba1a42772c\">\r\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.imshow(X_train_full_init[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all indices which are not Class 8 and 9\n",
    "import numpy as np\n",
    "ind_not8or9 = [i for i in range(0,len(y_train_full_init)) if ((y_train_full_init[i] != 8) & (y_train_full_init[i] != 9))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full,y_train_full = X_train_full_init[ind_not8or9], y_train_full_init[ind_not8or9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(48000, 28, 28)"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation data and train data\n",
    "#normalize data\n",
    "X_train,y_train = X_train_full[5000:]/255,y_train_full[5000:]\n",
    "X_valid,y_valid = X_train_full[:5000]/255,y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train a 5 layer functional API so we have enough to use for transfer learning with Batch Normalization and Exponenial Linear Unit\n",
    "#He initializer for Exponential Linear Unit\n",
    "he_init = keras.initializers.VarianceScaling(scale = 2, mode = 'fan_in',distribution = 'normal')\n",
    "\n",
    "model_transfer_mnist = keras.models.Sequential()\n",
    "\n",
    "model_transfer_mnist.add(keras.layers.Flatten(input_shape = [28,28]))\n",
    "\n",
    "model_transfer_mnist.add(keras.layers.Dense(500, kernel_initializer = he_init, use_bias = False))\n",
    "model_transfer_mnist.add(keras.layers.BatchNormalization())\n",
    "model_transfer_mnist.add(keras.layers.ELU())\n",
    "\n",
    "model_transfer_mnist.add(keras.layers.Dense(400, kernel_initializer = he_init, use_bias = False))\n",
    "model_transfer_mnist.add(keras.layers.BatchNormalization())\n",
    "model_transfer_mnist.add(keras.layers.ELU())\n",
    "\n",
    "model_transfer_mnist.add(keras.layers.Dense(300, kernel_initializer = he_init, use_bias = False))\n",
    "model_transfer_mnist.add(keras.layers.BatchNormalization())\n",
    "model_transfer_mnist.add(keras.layers.ELU())\n",
    "\n",
    "model_transfer_mnist.add(keras.layers.Dense(200, kernel_initializer = he_init, use_bias = False))\n",
    "model_transfer_mnist.add(keras.layers.BatchNormalization())\n",
    "model_transfer_mnist.add(keras.layers.ELU())\n",
    "\n",
    "model_transfer_mnist.add(keras.layers.Dense(100, kernel_initializer = he_init, use_bias = False))\n",
    "model_transfer_mnist.add(keras.layers.BatchNormalization())\n",
    "model_transfer_mnist.add(keras.layers.ELU())\n",
    "\n",
    "\n",
    "model_transfer_mnist.add(keras.layers.Dense(8, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten_9 (Flatten)          (None, 784)               0         \n_________________________________________________________________\ndense_17 (Dense)             (None, 500)               392000    \n_________________________________________________________________\nbatch_normalization_17 (Batc (None, 500)               2000      \n_________________________________________________________________\nelu (ELU)                    (None, 500)               0         \n_________________________________________________________________\ndense_18 (Dense)             (None, 400)               200000    \n_________________________________________________________________\nbatch_normalization_18 (Batc (None, 400)               1600      \n_________________________________________________________________\nelu_1 (ELU)                  (None, 400)               0         \n_________________________________________________________________\ndense_19 (Dense)             (None, 300)               120000    \n_________________________________________________________________\nbatch_normalization_19 (Batc (None, 300)               1200      \n_________________________________________________________________\nelu_2 (ELU)                  (None, 300)               0         \n_________________________________________________________________\ndense_20 (Dense)             (None, 200)               60000     \n_________________________________________________________________\nbatch_normalization_20 (Batc (None, 200)               800       \n_________________________________________________________________\nelu_3 (ELU)                  (None, 200)               0         \n_________________________________________________________________\ndense_21 (Dense)             (None, 100)               20000     \n_________________________________________________________________\nbatch_normalization_21 (Batc (None, 100)               400       \n_________________________________________________________________\nelu_4 (ELU)                  (None, 100)               0         \n_________________________________________________________________\ndense_22 (Dense)             (None, 8)                 808       \n=================================================================\nTotal params: 798,808\nTrainable params: 795,808\nNon-trainable params: 3,000\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_transfer_mnist.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}