{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0965ba1120b0c101b3f715b6e258a73742ec1cf86f2c8b04492724c87d9f112c3",
   "display_name": "Python 3.8.8 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "965ba1120b0c101b3f715b6e258a73742ec1cf86f2c8b04492724c87d9f112c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<h2> Unsupervised Pretraining </h2> \n",
    "\n",
    "When we do not have much labeled data, but we cannot find a model trained on a similar task. \n",
    "\n",
    "If the data you have is unlabelled, you may still be able to perform unsupervised pretraining on it. \n",
    "\n",
    "If you can gather plenty of unlabeled training data, you can try to train the layers one by one starting with the lowest layer and going up. \n",
    "\n",
    "<h4> Boltzmaan Machine </h4> \n",
    "\n",
    "Instead of using a deterministic step function to decide which value to output, these neurons output 1 with some probability, and 0 otherwise.\n",
    "\n",
    "If we keep a Boltzmaan machine running long enough, the probbility of observing a perticular outcome will only be a function of the connection weights and bias terms, not of the original configuration.\n",
    "\n",
    "Training a Boltzmann machine means finding the parameters which make the network approximate the training sets probability distribution.\n",
    "\n",
    "The point where your model no longer depends on its intital state, it has reached thermal equilibrium where there is no thermal gradient to cause any change. This is a generative model.\n",
    "\n",
    "<h4> Restricted Boltzmaan Machines </h4> \n",
    "\n",
    "RBMs are shallow neural networks with only 2 layers and is an unsupervised training method used to find patterns in data by reconstructing the input. It is called restricted because neurons in the same layer are not connected. \n",
    "\n",
    "RBMs can be used for both:\n",
    "- reconstruction of input\n",
    "- classify instances lacking classification by using the concept that similar neurons will fire in similar inputs. \n",
    "\n",
    "Given that you have more unlabelled data, you start off by building your network using only unlabelled data such that your input in the forward pass with be equal to the backward output. Then after training sufficient hidden layers, you feed in labelled data to train the output layer. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "Currently available methods for us to speed up training are:\n",
    "- Initialization strategy\n",
    "- Good activation function\n",
    "- Batch Normalization\n",
    "- Reusing Pretrained network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h2> Faster Optimizers </h2> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h2> Momentum Optimization </h2> \n",
    "\n",
    "It starts out slow and starts increasing speed as it picks up momentum, as it goes down the gradient slope. \n",
    "\n",
    "In gradient descent, it simply subtracts the existing theta values by the current gradient. It has no idea of the earlier gradient of the previous step. \n",
    "\n",
    "Momentum optimization subtracts the local gradient from the momentum vector(multiplied by m) and it updates the weights by simply adding this momentum vector. \n",
    "\n",
    "The gradient vector thats multiplied with the learning rate is used to control the accelerating here. not the speed. Hence a larger negative gradient results in a higher accelaration. \n",
    "\n",
    "\n",
    "Momentum optimization can update weights 10 times faster for a typical beta value of 0.9. This means it will not be stuck in plateaus when reaching global minima, hence it can work with non normalized data faster too. \n",
    "\n",
    "momentum <= friction * momentum - lr * gradient\n",
    "weights <= weights + momentum \n",
    "\n",
    "Notice we have added a  new parameter to tune in momentum\n",
    "\n",
    "It is better to think of Momentum optimization with movng averages. \n",
    "\n",
    "moving average of gradient <= momentum * moving average of gradient + (1- momentum) * gradient of this batch\n",
    "\n",
    "Then the moving average of gradient it used to update the weights:\n",
    "\n",
    "weights <= weights - (alpha * moving average of gradient)\n",
    "\n",
    "This ensures that we smoothen out any large movements from single step gradient descents. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h3> Nesterov Accelerated Gradient </h3> \n",
    "\n",
    "Involves measuring the gradient at a few steps ahead of the actual *theta* value we are currently at. Then we multiply this with the learning rate and subtract from momentum vector * friction. \n",
    "\n",
    "This works because in general the momentum vector will be pointing in the right direction. hence to accelerate it a bit, it makes sense to measure the gradient a bit further along the momentum vector. \n",
    "\n",
    "Given that we are moving along the momentum vector in finding the gradient, there is less oscillations and it will converge faster. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h3> AdaGrad </h3> \n",
    "\n",
    "When gradient descent occurs, its quickly starts going down the steepest slope and then slows down at the bottom of the valley. If we can initially make it travel towards the global optimum, it would be better.\n",
    "\n",
    "In AdaGrad we square the gradients into a vector s by doing element wise multiplication of the gradient vector.\n",
    "\n",
    "Given that the gradient vector consists of gradients with respect to multiple theta dimensions, if the gradient is steep across one particular theta dimension, the squaring of it will make it more steeper allowing it to travel down the slope faster. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h3> RMS Prop (Root Mean Square Prop) </h3> \n",
    "\n",
    "RMS Prop is very similar to AdaGrad, however it uses the moving average for each minibatch to update its *s* value. \n",
    "\n",
    "In both AdaGrad and RMS Prop, we look to smoothen out the gradient descent process. For dimensions with large oscillations, the elementwise squaring in the first step of the algorithm causes the *s* to become large for dimensions with large oscillations, and smaller for dimensions with small oscillations. \n",
    "\n",
    "Hence in the 2nd step of the algorithm, when we divide the gradient by the **s** value for a particular dimension, large oscillations are damped down, and the smaller more important oscillations are magnified, leading to faster optimizations. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h3> Adam and Nadam Optimization </h3> \n",
    "\n",
    "Adam: Adaptive Moment Estimation combines momentum optimization and RMS Prop. \n",
    "\n",
    "Just like momentum, it keeps track of exponentially decaying average of past gradients, and like RMSProp, it keeps track of exponentially decaying average of past squared gradients. \n",
    "\n",
    "\n",
    "The momentum parameter for decay of gradient is initialized to 0.9, while the hyperparameter for decay of square of gradient is initialized to 0.999. \n",
    "\n",
    "Nadam : This is Adam optimizer with the Nesterov trick of finding the gradient a few steps further along the line of momentum.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h3> Learning rate scheduling </h3> \n",
    "\n",
    "Instead of starting with a large learning rate and dividing it by 3, we can start with a high learning rate, and reduce it once it stops making fast progress. These strategies are called learning schedules. \n",
    "\n",
    "Power Scheduling: \n",
    "Here we set the learning rate to reduce at each iteration. The function of learning rate will be equal to the iteration number. \n",
    "\n",
    "Exponent scheduling:\n",
    "Set the learning rate to exponential decay of the initial learning rate, where the nate gets decreased by a factor of 10 every *s* steps. \n",
    "(Power Scheduling decreases the learning rate much more slowly as compared to Exponent scheduling)\n",
    "\n",
    "Piecewise constant scheduling:\n",
    "Here we pre set learning rates for ranges of epochs. This is difficulat as we need to find the appropriate number of epochs to spread a learning rate through. \n",
    "\n",
    "Performance scheduling:\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}