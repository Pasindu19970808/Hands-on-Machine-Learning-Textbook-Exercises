{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0965ba1120b0c101b3f715b6e258a73742ec1cf86f2c8b04492724c87d9f112c3",
   "display_name": "Python 3.8.8 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "965ba1120b0c101b3f715b6e258a73742ec1cf86f2c8b04492724c87d9f112c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<h2> Unsupervised Pretraining </h2> \n",
    "\n",
    "When we do not have much labeled data, but we cannot find a model trained on a similar task. \n",
    "\n",
    "If the data you have is unlabelled, you may still be able to perform unsupervised pretraining on it. \n",
    "\n",
    "If you can gather plenty of unlabeled training data, you can try to train the layers one by one starting with the lowest layer and going up. \n",
    "\n",
    "<h4> Boltzmaan Machine </h4> \n",
    "\n",
    "Instead of using a deterministic step function to decide which value to output, these neurons output 1 with some probability, and 0 otherwise.\n",
    "\n",
    "If we keep a Boltzmaan machine running long enough, the probbility of observing a perticular outcome will only be a function of the connection weights and bias terms, not of the original configuration.\n",
    "\n",
    "Training a Boltzmann machine means finding the parameters which make the network approximate the training sets probability distribution.\n",
    "\n",
    "The point where your model no longer depends on its intital state, it has reached thermal equilibrium where there is no thermal gradient to cause any change. This is a generative model.\n",
    "\n",
    "<h4> Restricted Boltzmaan Machines </h4> \n",
    "\n",
    "RBMs are shallow neural networks with only 2 layers and is an unsupervised training method used to find patterns in data by reconstructing the input. It is called restricted because neurons in the same layer are not connected. \n",
    "\n",
    "RBMs can be used for both:\n",
    "- reconstruction of input\n",
    "- classify instances lacking classification by using the concept that similar neurons will fire in similar inputs. \n",
    "\n",
    "Given that you have more unlabelled data, you start off by building your network using only unlabelled data such that your input in the forward pass with be equal to the backward output. Then after training sufficient hidden layers, you feed in labelled data to train the output layer. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "Currently available methods for us to speed up training are:\n",
    "- Initialization strategy\n",
    "- Good activation function\n",
    "- Batch Normalization\n",
    "- Reusing Pretrained network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h2> Faster Optimizers </h2> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h2> Momentum Optimization </h2> \n",
    "\n",
    "It starts out slow and starts increasing speed as it picks up momentum, as it goes down the gradient slope. \n",
    "\n",
    "In gradient descent, it simply subtracts the existing theta values by the current gradient. It has no idea of the earlier gradient of the previous step. \n",
    "\n",
    "Momentum optimization subsctracts the local gradient from the momentum vector(multiplied by m) and it updates the weights by simply adding this momentum vector. \n",
    "\n",
    "The gradient vector thats multiplied with the learning rate is used to control the accelerating here. not the speed. Hence a larger negative gradient results in a higher accelaration. \n",
    "\n",
    "\n",
    "Momentum optimization can update weights 10 times faster for a typical beta value of 0.9. This means it will not be stuck in plateaus when reaching global minima, hence it can work with non normalized data faster too. \n",
    "\n",
    "momentum <= friction * momentum - lr * gradient\n",
    "weights <= weights + momentum \n",
    "\n",
    "Notice we have added a  new parameter to tune in momentum"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}