{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h2> More on Tensorflow </h2> \n",
    "\n",
    "This contains custom functions with tensorflow and custom models."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "GPUS dramatically speed up computations by splitting computations into many smaller chunks and running them in parallel across many GPU threads. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "a = np.array([[1,2,3],[4,5,6]])\n",
    "a"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "a.dtype"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "b = tf.Variable(a)\n",
    "b"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=int32, numpy=\n",
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "b.scatter_nd_update(indices = [[1,1]],updates = [100])\n",
    "b"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=int32, numpy=\n",
       "array([[  1,   2,   3],\n",
       "       [  4, 100,   6]])>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#creating a simple tensor\n",
    "t = tf.constant([[1,2,3],[4,5,6]])\n",
    "t.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TensorShape([2, 3])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#indexing\n",
    "t[:,1:]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[2, 3],\n",
       "       [5, 6]])>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "tf.transpose(t)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
       "array([[1, 4],\n",
       "       [2, 5],\n",
       "       [3, 6]])>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "tf.matmul(t,tf.transpose(t))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[14, 32],\n",
       "       [32, 77]])>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> Tensors and Numpy </h3> "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import numpy as np\n",
    "a = np.array([2,4,5])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "tf.constant(a)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([2, 4, 5])>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "#casting from 32bit float to a 64 bit float to sum in TF\n",
    "t2 = tf.constant(40.,dtype = tf.float64)\n",
    "t2"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=40.0>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "tf.constant(2.0) + tf.cast(t2,tf.float32)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=42.0>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> Variables in Tensorflow </h3> \n",
    "\n",
    "We cannot modify constant tensors, however we can modify tf.Variable. These are required as we need to tweak weights in a neural network. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "v = tf.Variable([[1.,2.,3.],[4.,5.,6.]])\n",
    "v"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A tf.Variable acts like a constant tensor and allows to perform the same operations. But it also allows you to modify th variable using the assign() method."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "a = tf.constant([[1,2,3],[4,5,6]])\n",
    "a"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "v.assign(2*v)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2.,  4.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "v[0,1].assign(42)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[ 2., 42.,  6.],\n",
       "       [ 8., 10., 12.]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "v.scatter_nd_update(indices=[[0,0],[1,2]],updates = [100.,200.])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[100.,  42.,   6.],\n",
       "       [  8.,  10., 200.]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "v.scatter_nd_add(indices=[[0,0],[1,2]],updates = [100.,200.])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\n",
       "array([[200.,  42.,   6.],\n",
       "       [  8.,  10., 400.]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "scatter methods allows you to specify the index position and add to it"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> Sparse Tensors </h3> \n",
    "\n",
    "In a Sparse Tensor, you always need to give the positions first, then the values, then the dense shape"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "s = tf.SparseTensor(indices = [[0,1],[1,0],[2,3]],\n",
    "                    values = [1.,2.,3.], \n",
    "                    dense_shape=[3,4])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "tf.sparse.to_dense(s)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[0., 1., 0., 0.],\n",
       "       [2., 0., 0., 0.],\n",
       "       [0., 0., 0., 3.]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sparse tensors need to always be given in order of indices"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "s5 = tf.SparseTensor(indices=[[0, 1], [0, 2]],\r\n",
    "                     values=[1., 2.],\r\n",
    "                     dense_shape=[3, 4])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "tf.sparse.to_dense(s5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n",
       "array([[0., 1., 2., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> Tensor Arrays </h3>\n",
    "\n",
    "We first crrate the tensor with a fixed size. we cannot add past this"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "array = tf.TensorArray(dtype = tf.float32,size = 3)\r\n",
    "array = array.write(0,tf.constant([1.,2.]))\r\n",
    "array = array.write(1,tf.constant([3.,10.]))\r\n",
    "array = array.write(2,tf.constant([5.,7.]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> Customizing models and training algorithms </h3> "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> Custom Loss Functions </h3> "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "#where y_true - y_pred absolute is less than 1, we want the squared loss and linear loss\r\n",
    "def huber_fn(y_true,y_pred):\r\n",
    "    error = y_true - y_pred\r\n",
    "    is_small_error = tf.abs(error) < 1\r\n",
    "    squared_loss = tf.square(error)/2\r\n",
    "    linear_loss = tf.abs(error) - 0.5\r\n",
    "    return tf.where(is_small_error,squared_loss,linear_loss)\r\n",
    "\r\n",
    "#we need to always use tensorflow functions to be used as custom loss functions. "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the next step, we just apply this to our keras model\n",
    "\n",
    "model.compile(loss = huber_fn, optimizer = \"adam\")\n",
    "\n",
    "model.fit(.....)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "#current implementation allows us to have one threshold of 1 in the Huber Function\r\n",
    "#How to change the threshold?\r\n",
    "def create_huber(threshold = 1.0):\r\n",
    "    def huber_fn(y_true,y_pred):\r\n",
    "        error = y_true - y_pred\r\n",
    "        is_small_errors = tf.abs(error) <  threshold\r\n",
    "        #mae loss\r\n",
    "        squared_loss = tf.square(error)/2\r\n",
    "        #mse loss\r\n",
    "        linear_loss = threshold*tf.abs(error) - threshold**2/2\r\n",
    "        #if your error is less than threshold, return squared loss, \r\n",
    "        #else return linear_loss\r\n",
    "        return tf.where(is_small_errors, squared_loss,linear_loss)\r\n",
    "    return huber_fn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "model.compile(loss = create_huber(2.0), optimizer = \"adam\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above causes an issue when saving the model using \n",
    "keras.callbacks.ModelCheckpoint(\"modelname.h5\", save_best_only = True)\n",
    "\n",
    "When we load the model again we will have to specify which function is being called for the loss function as the loss function is a custom one here\n",
    "\n",
    "model = keras.models.load_model(\"modelname.h5\", custom_objects = {\"huber_fn\":create_huber(2.0)})"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "from tensorflow import keras"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "#to avoid the above we can inherit the Keras.losses.loss method\r\n",
    "\r\n",
    "class HuberLoss(keras.losses.Loss):\r\n",
    "    def __init__(self, threshold = 1.0, **kwargs):\r\n",
    "        self.threshold = threshold\r\n",
    "        #instantiate the superclass\r\n",
    "        super().__init__(**kwargs)\r\n",
    "    def call(self, y_true, y_pred):\r\n",
    "        error = y_true - y_pred\r\n",
    "        is_small_error = tf.abs(error) < self.threshold\r\n",
    "        squared_loss = tf.square(error)/2\r\n",
    "        linear_loss = self.threshold*tf.abs(error) - self.threshold**2/2\r\n",
    "        return tf.where(is_small_error,squared_loss,linear_loss)\r\n",
    "    def get_config(self):\r\n",
    "        #you get the base configuration of superclass\r\n",
    "        base_config = super().get_config()\r\n",
    "        #add in the threshold and creates a new dictionary to be returned\r\n",
    "        return {**base_config, \"threshold\":self.threshold}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "get_config method returns a dictionary mapping each hyperparameter name to its value. It first calls the parent class's get_config() method, then adds the new hyperparameters to this dictionary.\n",
    "\n",
    "from the above, when loading a model, we do this:\n",
    "\n",
    "**model = keras.models.load_model(\"my_model_with_a_custom_loss_class.h5\",\n",
    "custom_objects={\"HuberLoss\": HuberLoss})**\n",
    "\n",
    "Now we dont have to provide the threshold value too.\n",
    "\n",
    "When we save the mode, Keras calls the loss instance (HuberLoss) get_config() method and the returned dictionary is stored as a JSON in the h5 file. \n",
    "\n",
    "When loaded, it calls **from_config()** and creates the instance of the class, passing the return from **from_config()** to **kwargs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Custom Activation Functions, Initializers, Regularizers, and Constraints</h3>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "#defining custom activation functions \r\n",
    "def my_softplus(z):\r\n",
    "    return tf.math.log(tf.exp(z) + 1.0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "def my_glorot_initializer(shape,dtype = tf.float32):\r\n",
    "    stddev = tf.sqrt(2/(shape[0] + shape[1]))\r\n",
    "    return tf.random.normal(shape, stddev = stddev, dtype = dtype)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "def my_li_regularizer(weights):\r\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "#function to return only positive weights\r\n",
    "def my_positive_weights(weights):\r\n",
    "    return tf.where(weights<0, tf.zeros_like(weights),weights)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "#custom regularizer using subclassing\r\n",
    "\r\n",
    "class MyL1Regularizer(keras.regularizers.Regularizer):\r\n",
    "    def __init__(self,regfactor):\r\n",
    "        self.factor = regfactor\r\n",
    "    def __call__(self,weights):\r\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\r\n",
    "    def get_config(self):\r\n",
    "        return {\"regfactor\":self.factor}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For losses, layers and models we implement the call() method, and for regularizers, initializers and constraints we use the _____call_____() method. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Custom Metrics </h3> \n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "a = tf.Variable([[2,2,2],[2,2,2]])\r\n",
    "a = tf.cast(a,tf.float32)\r\n",
    "a"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[2., 2., 2.],\n",
       "       [2., 2., 2.]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "tf.reduce_sum(tf.abs(0.01*a))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.11999999>"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "At each training step the weights will be passed to the regularization function to compute the regularization loss. The return is then added to the main loss to get the final loss used for training. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> Custom Metrics </h3> \n",
    "\n",
    "Usually metrics keep track of the mean of a metric from each epoch. \n",
    "\n",
    "Suppose we have 5 true predictions, but 4 true positives. Thats 0.8 precision. Next epoch, we have 3 true predictions with 0 true positives. Thats 0 precision, but if we use mean, it becomes 0.4 precision overall\n",
    "\n",
    "But the actual one is 8 true predictions with 4 true positives in total. Thats 0.5 precision. Hence we need an object to track the true positives and an object to track the predictions. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "from tensorflow import keras\r\n",
    "precision = keras.metrics.Precision()\r\n",
    "precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.8>"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note above that the same precision object, under 2 runs prduces the overal precision of the data fed into it. Not a mean, or not a new precision for the the new run.\n",
    "\n",
    "This is called a streaming metric. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "precision.variables"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>,\n",
       " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>]"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "def create_huber(threshold = 1.0):\r\n",
    "    def huber_fn(y_true,y_pred):\r\n",
    "        error = y_true - y_pred\r\n",
    "        is_small_errors = tf.abs(error) <  threshold\r\n",
    "        #mae loss\r\n",
    "        squared_loss = tf.square(error)/2\r\n",
    "        #mse loss\r\n",
    "        linear_loss = threshold*tf.abs(error) - threshold**2/2\r\n",
    "        #if your error is less than threshold, return squared loss, \r\n",
    "        #else return linear_loss\r\n",
    "        return tf.where(is_small_errors, squared_loss,linear_loss)\r\n",
    "    return huber_fn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "#creating streaming metrics\r\n",
    "class HuberMetric(keras.metrics.Metric):\r\n",
    "    def __init__(self,threshold = 1.0, **kwargs):\r\n",
    "        super().__init__(**kwargs)\r\n",
    "        self.threshold = threshold\r\n",
    "        #returns a huber function to be used in this class\r\n",
    "        self.huber_fn = create_huber(threshold)\r\n",
    "        #add weight creates variables to keep track of attributes\r\n",
    "        self.total = self.add_weight(\"total\",initializer= \"zeros\")\r\n",
    "        self.count = self.add_weight(\"count\",initializer= \"zeros\")\r\n",
    "    def update_state(self,y_true,y_pred,sample_weight = None):\r\n",
    "        result = self.huber_fn(y_true, y_pred)\r\n",
    "        self.total.assign_add(tf.reduce_sum(result))\r\n",
    "        self.count.assign_add(tf.cast(tf.size(y_pred), tf.float32))\r\n",
    "    def result(self):\r\n",
    "        return self.total/self.count\r\n",
    "    def get_config(self):\r\n",
    "        base_config = self.get_config()\r\n",
    "        return {**base_config, \"threshold\":self.threshold}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Both customPrecision and HuberMetric are inherriting from keras.metrics.Metric which is an abstract class. Abstract classes contain methods which need to be surely built in the child class."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "class customPrecision(keras.metrics.Metric):\r\n",
    "    def __init__(self, **kwargs):\r\n",
    "        super().__init__(**kwargs)\r\n",
    "    def update_state(self,ytrue, ypred,sample_weight = None):\r\n",
    "        try: \r\n",
    "            for _class in self.unique_classes:\r\n",
    "                self.total_true[_class] = self.total_true[_class] + tf.where(ytrue == _class).shape[0]\r\n",
    "                self.total_prediction[_class] = self.total_prediction[_class] + tf.where(ypred == _class).shape[0]\r\n",
    "                self.total_truepositives[_class] = self.total_truepositives[_class] + len([i for i in tf.where(ypred == _class) if i in tf.where(ytrue == _class)])\r\n",
    "            #return [self.total_prediction,self.total_truepositives]\r\n",
    "        except:\r\n",
    "            self.unique,_,_ = tf.unique_with_counts(ytrue)\r\n",
    "            #obtains the unique classes to a list that we can loop through\r\n",
    "            self.unique_classes = self.unique.numpy()\r\n",
    "            self.total_truepositives = {_class:0 for _class in self.unique_classes}\r\n",
    "            self.total_prediction = {_class:0 for _class in self.unique_classes}\r\n",
    "            self.precision_classbase = {_class:0 for _class in self.unique_classes}\r\n",
    "            self.total_true = {_class:0 for _class in self.unique_classes}\r\n",
    "            for _class in self.unique_classes:\r\n",
    "                self.total_true[_class] = self.total_true[_class] + tf.where(ytrue == _class).shape[0]\r\n",
    "                self.total_prediction[_class] = self.total_prediction[_class] + tf.where(ypred == _class).shape[0]\r\n",
    "                self.total_truepositives[_class] = self.total_truepositives[_class] + len([i for i in tf.where(ypred == _class) if i in tf.where(ytrue == _class)])\r\n",
    "            #return [self.total_prediction,self.total_truepositives]\r\n",
    "    def result(self):\r\n",
    "        for _class in self.unique_classes:\r\n",
    "            self.precision_classbase[_class] = self.total_truepositives[_class]/self.total_prediction[_class]\r\n",
    "            self.precision_classbase[_class] = self.precision_classbase[_class] * (self.total_true[_class]/sum(self.total_true.values()))\r\n",
    "        return sum(self.precision_classbase.values())\r\n",
    "    def get_config(self):\r\n",
    "        base_config = self.get_config()\r\n",
    "        return {**base_config}       "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "ytrue1 = tf.Variable([0, 1, 1, 1, 0, 1, 0, 1])\r\n",
    "ypred1 = tf.Variable([1, 1, 0, 1, 0, 1, 0, 1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "prec_test1 = customPrecision()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "prec_test1(ytrue1,ypred1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.75>"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "prec_test1.precision_classbase"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{0: 0.25, 1: 0.5}"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "ytrue2 = tf.Variable([0, 1, 0, 0, 1, 0, 1, 1])\r\n",
    "ypred2 = tf.Variable([1, 0, 1, 1, 0, 0, 0, 0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "prec_test1(ytrue2,ypred2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.4453125>"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "a = {0:4,1:3}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "prec_test1.total_true"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{0: 7, 1: 9}"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> Custom Layers </h3>\n",
    "\n",
    "Sometimes we need to make layers which Tensorflow does not provide  default implementation for. In this case we need to create a custom layer.\n",
    "\n",
    "Also we might want to club repeating layer patterns into one. \n",
    "E.g.: If you have layer pattern A,B,C,A,B,C,A,B,C we can make one layer D containing A,B,C and make D,D,D."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "#to build custom layers with weights in them, we need to subclass of the keras.layers.Layer class\r\n",
    "class MyDense(keras.layers.Layer):\r\n",
    "    def __init__(self,units,activation = None, **kwargs):\r\n",
    "        super().__init__(**kwargs)\r\n",
    "        self.units = units\r\n",
    "        self.activation = keras.activations.get(activation)\r\n",
    "        #in build you build your kernel (weight matrix) and bias\r\n",
    "    def build(self,batch_input_shape):\r\n",
    "        #batch input shape is the number of features in the input to this layer\r\n",
    "        #batch_input_shape = [batch_size, no.of features in each batch instance]\r\n",
    "        self.kernel = self.add_weight(name = \"kernel\",\r\n",
    "        shape = [batch_input_shape[-1],self.units],\r\n",
    "        initializer=\"glorot_normal\")\r\n",
    "        self.bias = self.add_weight(name = \"bias\",\r\n",
    "        shape = [self.units], initializer = \"zeros\")\r\n",
    "        #sets self.built = True in parent class\r\n",
    "        super().build(batch_input_shape)\r\n",
    "    #call returns the output of the matrix multiplication or any result\r\n",
    "    def call(self,X):\r\n",
    "        return self.activation(tf.matmul(X,self.kernel) + self.bias)\r\n",
    "    def compute_output_shape(self,batch_input_shape):\r\n",
    "        #output shape will be [batch_size, no. of units]\r\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\r\n",
    "    def get_config(self):\r\n",
    "        base_config = super().get_config()\r\n",
    "        return {**base_config,\"units\":self.units,\r\n",
    "        \"activation\":keras.activations.serialize(self.activation)}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "#using the above custom layer\r\n",
    "model = keras.models.Sequential()\r\n",
    "#notice that input_shape is part of the **kwargs argument\r\n",
    "model.add(MyDense(30, activation = \"relu\", input_shape = (8,)))\r\n",
    "model.add(MyDense(1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "my_dense (MyDense)           (None, 30)                270       \n",
      "_________________________________________________________________\n",
      "my_dense_1 (MyDense)         (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "model.layers[0].bias"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Variable 'my_dense/bias:0' shape=(30,) dtype=float32, numpy=\n",
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> Custom Models </h3> \n",
    "\n",
    "Lets say we are building a model with one dense input, which passes through an identical block(Residual Block), containing 2 dense layers and an addition of the input layer to the result of the 2 dense layers. The identical block is used 3 times."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "#first we build this Residual Block\r\n",
    "class ResidualBlock(keras.layers.Layer):\r\n",
    "    def __init__(self,n_neurons,n_layers,**kwargs):\r\n",
    "        super().__init__(**kwargs)\r\n",
    "        self.hidden = [keras.layers.Dense(n_neurons,activation = \"elu\",kernel_initializer=\"he_normal\") for _ in range(n_layers)]\r\n",
    "    #call returns the output of the matrix multiplication or any result\r\n",
    "    def call(self,inputs):\r\n",
    "        Z = inputs\r\n",
    "        for layer in self.hidden:\r\n",
    "            Z = layer(Z)\r\n",
    "        #as the Residual block adds the output of the layers into\r\n",
    "        return inputs + Z"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "#notice that although we inherit from keras.layers.Layer, we can still use normal keras.layers.Dense in it, and implement calculations using these Dense layers from call()\r\n",
    "class ResidualRegressor(keras.models.Model):\r\n",
    "    def __init__(self,output_dim,**kwargs):\r\n",
    "        super().__init__(**kwargs)\r\n",
    "        self.hidden1 = keras.layers.Dense(30,activation = \"elu\",kernel_initializer=\"he_normal\")\r\n",
    "        self.block1 = ResidualBlock(2,30)\r\n",
    "        self.block2 = ResidualBlock(2,30)\r\n",
    "        self.out = keras.layers.Dense(output_dim)\r\n",
    "    def call(self, inputs):\r\n",
    "        Z = self.hidden1(inputs)\r\n",
    "        for _ in range(1 + 3):\r\n",
    "            Z = self.block1(Z)\r\n",
    "        Z = self.block2(Z)\r\n",
    "        return self.out(Z)\r\n",
    "#we can use keras.layers.Layer to create custom Layers. we can use keras.models.Model to create custom models with custom layers in it. \r\n",
    "\r\n",
    "#then we can create instance of the keras.models.Model class and compile, fit as required\r\n",
    "\r\n",
    "#Remember than keras.models.Model is a subclass of keras.layers.Layer with more functionality"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "model2 = ResidualRegressor(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "\r\n",
    "block1 = ResidualBlock(2, 30)\r\n",
    "model3 = keras.models.Sequential([\r\n",
    "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\"),\r\n",
    "    block1, block1, block1, block1,\r\n",
    "    ResidualBlock(2, 30),\r\n",
    "    keras.layers.Dense(1)\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> Losses and Metrics Based on Model Internals </h3> \n",
    "\n",
    "Usually losses and metrics are based on predictions and ocassionaly sample weights. However we sometimes want it to be based on other parts of the model. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "class ReconstructingInputRegressor(keras.models.Model):\r\n",
    "    def __init__(self,output_dim,**kwargs):\r\n",
    "        super().__init__(**kwargs)\r\n",
    "        self.hidden = [keras.layers.Dense(30, activation = \"selu\", kernel_initializer = \"lecun_normal\") for _ in range(5)]\r\n",
    "        self.out = keras.layers.Dense(output_dim)\r\n",
    "    def build(self,batch_input_shape):\r\n",
    "        self.reconstructlayer = keras.layers.Dense(batch_input_shape[-1])\r\n",
    "        super().build(batch_input_shape)\r\n",
    "    def call(self,inputs):\r\n",
    "        Z = inputs\r\n",
    "        for layer in self.hidden:\r\n",
    "            Z = layer(Z)\r\n",
    "        reconstruct = self.reconstructlayer(Z)\r\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruct - inputs))\r\n",
    "        self.add_loss(0.05*recon_loss)\r\n",
    "        return self.out(Z)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b06a6ae6d570>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mReconstructingInputRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"selu\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"lecun_normal\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "#the build method is skipped in here, and directly it is done in __init__ method,\r\n",
    "#due to a TF issue\r\n",
    "class ReconstructingInputRegressor2(keras.models.Model):\r\n",
    "    def __init__(self,output_dim,input_dim,**kwargs):\r\n",
    "        super().__init__(**kwargs)\r\n",
    "        self.hidden = [keras.layers.Dense(30, activation = \"selu\", kernel_initializer = \"lecun_normal\") for _ in range(5)]\r\n",
    "        self.out = keras.layers.Dense(output_dim)\r\n",
    "        self.reconstructlayer = keras.layers.Dense(input_dim)\r\n",
    "        #adding a loss as a metric\r\n",
    "        #self.reconmetric = keras.metrics.Mean(name = \"recon_error\")\r\n",
    "    \"\"\" def build(self,batch_input_shape):\r\n",
    "        self.reconstructlayer = keras.layers.Dense(batch_input_shape[-1])\r\n",
    "        self.build(batch_input_shape) \"\"\"\r\n",
    "    def call(self,inputs):\r\n",
    "        Z = inputs\r\n",
    "        for layer in self.hidden:\r\n",
    "            Z = layer(Z)\r\n",
    "        reconstruct = self.reconstructlayer(Z)\r\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruct - inputs))\r\n",
    "        #self.add_loss(0.05*recon_loss)\r\n",
    "        self.add_metric(0.05*recon_loss, name = \"recon_error\")\r\n",
    "        return self.out(Z)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "class ReconstructingInputRegressor3(keras.layers.Layer):\n",
    "    def __init__(self,input_dims,no_units,no_hidden_layers,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layer_list = list()\n",
    "        for _ in range(1,no_hidden_layers+1):\n",
    "            if _ == 1:\n",
    "                self.layer_list.append(self.add_weight(name = \"kernel{}\".format(_), shape = [input_dims, no_units], initializer=\"he_normal\"))\n",
    "            else:\n",
    "                self.layer_list.append(self.add_weight(name = \"kernel{}\".format(_), shape = [no_units, no_units], initializer=\"he_normal\"))\n",
    "\n",
    "        self.reconstructlayer = self.add_weight(name = \"reconstruct\", \n",
    "        shape = [no_units, input_dims], initializer=\"he_normal\")\n",
    "\n",
    "        self.outputlayer = self.add_weight(name = \"output\", \n",
    "        shape = [no_units, 1], initializer=\"he_normal\")\n",
    "    def call(self,inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.layer_list:\n",
    "            Z = tf.matmul(Z,layer)\n",
    "        reconstruct = tf.matmul(Z,self.reconstructlayer)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruct - inputs))\n",
    "        self.add_loss(0.05*recon_loss)\n",
    "        self.add_metric(0.05*recon_loss,name = \"recon_error\")\n",
    "        return tf.matmul(reconstruct,self.outputlayer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> add_loss </h3> \n",
    "\n",
    "A typical loss function takes ypred and ytrue and calculates the loss and tries to minimize it. But some loss functions require more than just that and will require you to do calculations in the call method of your model and use it to calculate a different loss which can be minimized. \n",
    "\n",
    "https://stackoverflow.com/questions/50063613/what-is-the-purpose-of-the-add-loss-function-in-keras"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "import numpy as np\n",
    "X_dummy = np.random.randn(10,8)\n",
    "y_dummy = np.random.randn(10,1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "layer1 = ReconstructingInputRegressor3(8,8,5)\n",
    "model_recon = keras.models.Sequential()\n",
    "model_recon.add(layer1)\n",
    "model_recon.compile(loss = \"mse\",optimizer = \"adam\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "model_recon.fit(X_dummy,y_dummy, epochs = 3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/3\n",
      "1/1 [==============================] - 1s 1s/step - loss: 13.0331 - recon_error: 2.0974\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 11.8487 - recon_error: 2.0520\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 10.7768 - recon_error: 2.0078\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fdfaa39400>"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> Custom layer to simulate a forward pass only </h3> "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "#custom layer\n",
    "class forwardPasslayer(keras.layers.Layer):\n",
    "    def __init__(self,units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        #self.output_dim = output_dim\n",
    "    def build(self,batch_input_shape):\n",
    "        self.kernel1 = self.add_weight(name = \"kernel\",\n",
    "        shape = [batch_input_shape[-1],self.units],\n",
    "        initializer=\"zeros\")\n",
    "        self.kernel2= self.add_weight(name = \"kernel1\",\n",
    "        shape = [self.units,self.units], initializer = \"he_normal\")\n",
    "    def call(self,X):\n",
    "        intermediate = tf.math.add(tf.matmul(X,self.kernel1),1)\n",
    "        return tf.matmul(intermediate,self.kernel2)\n",
    "    def compute_output_shape(self,batch_input_shape):\n",
    "        return tf.TensorShape([self.units] + [self.units])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "X_test1 = np.random.randn(10,6)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "model_zeros = keras.models.Sequential()\n",
    "model_zeros.add(forwardPasslayer(8, input_shape = (6,)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "model_zeros.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "forward_passlayer (forwardPa (None, 8)                 112       \n",
      "=================================================================\n",
      "Total params: 112\n",
      "Trainable params: 112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "model_zeros(X_test1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 8), dtype=float32, numpy=\n",
       "array([[ 0.20135605,  2.5514088 , -1.3766464 , -0.53672874, -1.2045982 ,\n",
       "        -1.4586414 , -1.9787369 ,  2.1451645 ],\n",
       "       [ 0.20135605,  2.5514088 , -1.3766464 , -0.53672874, -1.2045982 ,\n",
       "        -1.4586414 , -1.9787369 ,  2.1451645 ],\n",
       "       [ 0.20135605,  2.5514088 , -1.3766464 , -0.53672874, -1.2045982 ,\n",
       "        -1.4586414 , -1.9787369 ,  2.1451645 ],\n",
       "       [ 0.20135605,  2.5514088 , -1.3766464 , -0.53672874, -1.2045982 ,\n",
       "        -1.4586414 , -1.9787369 ,  2.1451645 ],\n",
       "       [ 0.20135605,  2.5514088 , -1.3766464 , -0.53672874, -1.2045982 ,\n",
       "        -1.4586414 , -1.9787369 ,  2.1451645 ],\n",
       "       [ 0.20135605,  2.5514088 , -1.3766464 , -0.53672874, -1.2045982 ,\n",
       "        -1.4586414 , -1.9787369 ,  2.1451645 ],\n",
       "       [ 0.20135605,  2.5514088 , -1.3766464 , -0.53672874, -1.2045982 ,\n",
       "        -1.4586414 , -1.9787369 ,  2.1451645 ],\n",
       "       [ 0.20135605,  2.5514088 , -1.3766464 , -0.53672874, -1.2045982 ,\n",
       "        -1.4586414 , -1.9787369 ,  2.1451645 ],\n",
       "       [ 0.20135605,  2.5514088 , -1.3766464 , -0.53672874, -1.2045982 ,\n",
       "        -1.4586414 , -1.9787369 ,  2.1451645 ],\n",
       "       [ 0.20135605,  2.5514088 , -1.3766464 , -0.53672874, -1.2045982 ,\n",
       "        -1.4586414 , -1.9787369 ,  2.1451645 ]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> Computing Gradient using Autodiff </h3> \n",
    "\n",
    " "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "#toy function\n",
    "def f(w1,w2):\n",
    "    return 3*w1 ** 2 + 2*w1*w2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "source": [
    "w1 = 5\n",
    "w2 = 3\n",
    "eps = 1e-6\n",
    "(f(w1 + eps,w2) - f(w1,w2))/eps"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "36.000003007075065"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "(f(w1,w2 + eps) - f(w1,w2))/eps"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10.000000003174137"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> Using Autodiff with Tensorflow </h3> "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "w1 = tf.Variable(5.)\n",
    "w2 = tf.Variable(3.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1,w2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "gradients = tape.gradient(z,[w1,w2])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "gradients"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> Gradient Tapes </h3> \n",
    "\n",
    "Tensorflow records relevant operations executed inside the context of a tf.GradientTape onto a \"tape\". This tape is used to compute the gradients of a recorded computation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "w = tf.Variable(tf.random.normal((3,2)),name = \"w\")\n",
    "w"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Variable 'w:0' shape=(3, 2) dtype=float32, numpy=\n",
       "array([[-0.69147605,  0.5336185 ],\n",
       "       [ 2.988973  ,  0.34782115],\n",
       "       [-1.105531  , -0.52491236]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "b = tf.Variable(tf.zeros(2,dtype = tf.float32),name = 'b')\n",
    "b"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Variable 'b:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "x = [[1.,2.,3.]]\n",
    "x"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[1.0, 2.0, 3.0]]"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "#derivatives of tensors\n",
    "w = tf.Variable(tf.random.normal((3, 2)), name='w')\n",
    "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
    "x = [[1., 2., 3.]]\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  y = x @ w + b\n",
    "  loss = tf.reduce_mean(y**2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "[dl_dw, dl_db] = tape.gradient(loss, [w, b])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "print(dl_dw.shape)\n",
    "print(dl_db.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(3, 2)\n",
      "(2,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> Gradients with respect to a model </h3> "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "layer = keras.layers.Dense(2,activation = \"relu\")\n",
    "x = tf.constant([[1.,2.,3.]])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    y = layer(x)\n",
    "    loss = tf.reduce_mean(y**2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "grad = tape.gradient(loss,layer.trainable_variables)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "By default, the tape will only track operations involving variables, so if you try to compute the gradient of z with regards to anything else than a variable, the result will\n",
    "be None\n",
    "\n",
    "Sometimes you will have to track how the gradient changes with respect to constants too to penalize functions that vary a lot when the inputs vary a little."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "#specifying which gradients to watch\n",
    "w = tf.Variable(tf.random.normal((3, 2)), name='w')\n",
    "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
    "x = [[1., 2., 3.]]\n",
    "\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "  tape.watch(w)\n",
    "  y = x @ w + b\n",
    "  loss = tf.reduce_mean(y**2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "dy_dw,dy_db = tape.gradient(y,[w,b])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "dy_dw.numpy()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [2., 2.],\n",
       "       [3., 3.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "#this is None as it wasnt being watched\n",
    "dy_db"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "x1 = tf.Variable(20.0)\n",
    "x2 = tf.Variable(30.0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "def f1(w1,w2):\n",
    "    return w1**3 + w2**2 + w1*w2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "with tf.GradientTape(persistent=True) as hessian_tape:\n",
    "    with tf.GradientTape() as jacobian_tape:\n",
    "        z = f1(x1,x2)\n",
    "    jacobians = jacobian_tape.gradient(z,[x1,x2])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "jacobians"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=1230.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=80.0>]"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "hessians = [hessian_tape.gradient(jacobian,[x1,x2]) for jacobian in jacobians]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "hessians"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[<tf.Tensor: shape=(), dtype=float32, numpy=120.0>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=1.0>],\n",
       " [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>,\n",
       "  <tf.Tensor: shape=(), dtype=float32, numpy=2.0>]]"
      ]
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "hessian_2 = hessian_tape.gradient(jacobians,[x1,x2])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "hessian_2"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=121.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=3.0>]"
      ]
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we do not call the jacobian for each partial derivative in the list jacobians, the hessians become the some of each first partial derivative"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> Writing Custom Training Loops </h3> "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "#building a model\n",
    "l2_reg = keras.regularizers.l2(0.05)\n",
    "model_custom = keras.models.Sequential()\n",
    "model_custom.add(keras.layers.Dense(30,activation = 'elu',kernel_initializer = \"he_normal\", kernel_regularizer = l2_reg))\n",
    "model_custom.add(keras.layers.Dense(1, kernel_regularizer = l2_reg))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "def random_batch(X,y, batch_size = 32):\n",
    "    idx = np.random.randint(len(X), size = batch_size)\n",
    "    return X[idx],y[idx]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "def print_status_bar(iteration, total, loss, metrics = None):\n",
    "    metrics_or_loss = '-'.join(['{}:{:.4f}'.format(m.name,m.result()) for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{} - \".format(iteration,total) + metrics_or_loss, end = end)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps_per_epoch = X_train_scaled.shape[0]//batch_size\n",
    "optimizer = keras.optimizers.Nadam(lr = 0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.MeanAbsoluteError()]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "source": [
    "for epoch in range(1,n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1,n_steps_per_epoch + 1):\n",
    "        X_batch,y_batch = random_batch(X_train_scaled,y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model_custom(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch,y_pred))\n",
    "            #as we have not used model.add_losses, the only other loss is the regularization loss in this case. To add this in too, we have to use tf.add_n\n",
    "            loss = tf.add_n([main_loss] + model_custom.losses)\n",
    "        grad = tape.gradient(loss, model_custom.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grad,model_custom.trainable_variables))\n",
    "        mean_loss(main_loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch,y_pred)\n",
    "        print_status_bar(step*batch_size,len(y_train),mean_loss, metrics)\n",
    "    print_status_bar(len(y_train),len(y_train),mean_loss,metrics)\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "11610/11610 - mean:1.3382-mean_absolute_error:0.6340\n",
      "Epoch 2/5\n",
      "11610/11610 - mean:0.5440-mean_absolute_error:0.5290\n",
      "Epoch 3/5\n",
      "11610/11610 - mean:0.5490-mean_absolute_error:0.5273\n",
      "Epoch 4/5\n",
      "11610/11610 - mean:0.5184-mean_absolute_error:0.5260\n",
      "Epoch 5/5\n",
      "11610/11610 - mean:0.5293-mean_absolute_error:0.5272\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> Tensorflow functions and graphs </h3> "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "source": [
    "def cube(x):\n",
    "    return x**3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "source": [
    "#converting normal python function to TF function\n",
    "tf_cube = tf.function(cube)\n",
    "tf_cube"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.def_function.Function at 0x1fd89781760>"
      ]
     },
     "metadata": {},
     "execution_count": 138
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "source": [
    "#alternate way to produce tensor function\n",
    "@tf.function\n",
    "def tf_cube(x):\n",
    "    return x**3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "source": [
    "tf_cube.python_function(3)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "metadata": {},
     "execution_count": 141
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "TF functions create new graphs when you call a function with a tensorflow variable/constant. When you call a TF function with a tf constant as the parameter, you will generate a graph for int32 tensors of shape []. When you call the same function with a different TF constant/variable, it will use the same graph generated earlier. Hence it saves RAM. \n",
    "\n",
    "When you call the same TF function with a tf tensor **tf.constant([10,20]), it will produce a new graph for int32 tensors of shape 2. \n",
    "\n",
    "BUT \n",
    "\n",
    "If we call a tensorflow function with normal python integers, it ends up creating a graph for each call, making use inefficient. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> Autograph and Tracing </h3> "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tensorflow starts by analyzing the python function's source code to capture all the control flow statements (for loops, while loops, if statements, break, continue, return). This is done in the tracing stage where the graph is built. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "965ba1120b0c101b3f715b6e258a73742ec1cf86f2c8b04492724c87d9f112c3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}