{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<h2> Naive Bayes </h2> \n",
    "\n",
    "Lets say we get spam and valid emails. \n",
    "\n",
    "The first thing we do is make a histogram of all the words that occur in the valid emails. We can use the histogram to calculate the probability that a word occurs in a normal message. \n",
    "\n",
    "We calculate the probability of each word occuring given the total number of occurences. \n",
    "\n",
    "We do the same for the spam emails too \n",
    "\n",
    "As the probabilities of these words are discrete probabilities, they are called likelihoods. \n",
    "\n",
    "\n",
    "We start with assuming that any message is a normal valid email. \n",
    "\n",
    "Given that we have 8 emails of Valid messages and 4 of Spam messages in this example, the probability of a Valid email is 8/12 = 0.67. This initial guess is called a prior probability.  \n",
    "\n",
    "We also calculate the probability o the incoming email being spam, the same way. Given 4 are spam, your probability is 4/12 = 0.33.\n",
    "\n",
    "\n",
    "There can be situations where a word doesnt exist in the spam email histogram, resulting in the entire probability of a particular email being spam being zero. However this same email may have another word which is of high probability in the spam histogram. Yet as the probability of one word is zero, the entire probability becomes zero. To handle this we add one count to each word regardless. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h3> Why is Naive Byes naive </h3>\n",
    "\n",
    "It treats all word orders the same. \n",
    "\n",
    "P(N) x P(Friend|N) x P(Dear|N) == P(N) x P(Dear|N) x P(Friend|N) \n",
    "\n",
    "\n",
    "Due t ignoring relationships among words, Naive Bayes is said to have high bias , but as it performs surprisingly well, it is said to have low variance.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}