{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "965ba1120b0c101b3f715b6e258a73742ec1cf86f2c8b04492724c87d9f112c3"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full),(X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1cf9d2af820>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-04-06T12:24:25.830536</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 251.565 248.518125 \r\nL 251.565 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 26.925 224.64 \r\nL 244.365 224.64 \r\nL 244.365 7.2 \r\nL 26.925 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g clip-path=\"url(#pac3af20f15)\">\r\n    <image height=\"218\" id=\"imageb49345277b\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\r\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\" y=\"-6.64\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"ma2f4f31205\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#ma2f4f31205\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#ma2f4f31205\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#ma2f4f31205\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#ma2f4f31205\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#ma2f4f31205\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#ma2f4f31205\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m17db356b21\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m17db356b21\" y=\"11.082857\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m17db356b21\" y=\"49.911429\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m17db356b21\" y=\"88.74\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m17db356b21\" y=\"127.568571\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m17db356b21\" y=\"166.397143\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m17db356b21\" y=\"205.225714\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 26.925 224.64 \r\nL 26.925 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 244.365 224.64 \r\nL 244.365 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 26.925 224.64 \r\nL 244.365 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 26.925 7.2 \r\nL 244.365 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pac3af20f15\">\r\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.imshow(X_train_full[5,:].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a validation set \n",
    "X_valid,X_train = X_train_full[:5000]/255.0,X_train_full[5000:]/255.0\n",
    "y_valid,y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "\"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "source": [
    "Building a neural network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a single stack of layers connected sequentially\n",
    "model = keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a Flatten layer. It converts the specified input shape into a (-1,1) instance. \n",
    "model.add(keras.layers.Flatten(input_shape = [28,28]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next we add the first hidden layer containing 300 neurons. It will use the Rectified Linear Unit Activation Function(ReLU). \n",
    "#This does not include bias terms \n",
    "model.add(keras.layers.Dense(300, activation = \"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next we add another hidden layer with 100 neurons in it\n",
    "model.add(keras.layers.Dense(100, activation = \"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#last we add in the output layer which has 10 neurons for the 10 different classes\n",
    "model.add(keras.layers.Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten (Flatten)            (None, 784)               0         \n_________________________________________________________________\ndense (Dense)                (None, 300)               235500    \n_________________________________________________________________\ndense_1 (Dense)              (None, 100)               30100     \n_________________________________________________________________\ndense_2 (Dense)              (None, 10)                1010      \n=================================================================\nTotal params: 266,610\nTrainable params: 266,610\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Flatten at 0x1cf9d3b6ee0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1cfb3b2abb0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1cfb3b8b4f0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1cfb3b98e50>]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "source": [
    "The shape of the weight matrix is dependant on the number of inputs. This is why input_shape should be specified in keras.layers.Flatten. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using compile method to specify the loss function, optimizer to use and extra metrics to compute during training \n",
    "model.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "                optimizer = \"sgd\",\n",
    "                metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "y_train_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "keras.utils.to_categorical(y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.9927 - accuracy: 0.6880 - val_loss: 0.4842 - val_accuracy: 0.8444\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.5033 - accuracy: 0.8248 - val_loss: 0.4863 - val_accuracy: 0.8352\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4462 - accuracy: 0.8445 - val_loss: 0.4278 - val_accuracy: 0.8530\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4073 - accuracy: 0.8559 - val_loss: 0.4078 - val_accuracy: 0.8596\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3946 - accuracy: 0.8604 - val_loss: 0.4267 - val_accuracy: 0.8556\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3802 - accuracy: 0.8666 - val_loss: 0.3720 - val_accuracy: 0.8732\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3644 - accuracy: 0.8710 - val_loss: 0.3561 - val_accuracy: 0.8764\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3499 - accuracy: 0.8773 - val_loss: 0.3510 - val_accuracy: 0.8758\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3376 - accuracy: 0.8796 - val_loss: 0.3497 - val_accuracy: 0.8804\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.3317 - accuracy: 0.8811 - val_loss: 0.3594 - val_accuracy: 0.8738\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.3165 - accuracy: 0.8882 - val_loss: 0.3332 - val_accuracy: 0.8794\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.3127 - accuracy: 0.8883 - val_loss: 0.3253 - val_accuracy: 0.8836\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3057 - accuracy: 0.8911 - val_loss: 0.3246 - val_accuracy: 0.8828\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2979 - accuracy: 0.8939 - val_loss: 0.3289 - val_accuracy: 0.8832\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2916 - accuracy: 0.8943 - val_loss: 0.3197 - val_accuracy: 0.8874\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2868 - accuracy: 0.8960 - val_loss: 0.3337 - val_accuracy: 0.8804\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2856 - accuracy: 0.8984 - val_loss: 0.3144 - val_accuracy: 0.8848\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2776 - accuracy: 0.8996 - val_loss: 0.3250 - val_accuracy: 0.8870\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 9s 6ms/step - loss: 0.2711 - accuracy: 0.9010 - val_loss: 0.3098 - val_accuracy: 0.8898\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2664 - accuracy: 0.9030 - val_loss: 0.3172 - val_accuracy: 0.8866\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2592 - accuracy: 0.9067 - val_loss: 0.3036 - val_accuracy: 0.8936\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2556 - accuracy: 0.9074 - val_loss: 0.3083 - val_accuracy: 0.8916\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2531 - accuracy: 0.9090 - val_loss: 0.3074 - val_accuracy: 0.8900\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2431 - accuracy: 0.9109 - val_loss: 0.3090 - val_accuracy: 0.8896\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2445 - accuracy: 0.9138 - val_loss: 0.3048 - val_accuracy: 0.8936\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2397 - accuracy: 0.9132 - val_loss: 0.2979 - val_accuracy: 0.8950\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2329 - accuracy: 0.9183 - val_loss: 0.3011 - val_accuracy: 0.8922\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2333 - accuracy: 0.9167 - val_loss: 0.2991 - val_accuracy: 0.8964\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2273 - accuracy: 0.9189 - val_loss: 0.3100 - val_accuracy: 0.8902\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.2214 - accuracy: 0.9202 - val_loss: 0.2962 - val_accuracy: 0.8936\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train,epochs = 30, \n",
    "                    validation_data = (X_valid,y_valid))"
   ]
  },
  {
   "source": [
    "For skewed datasets with some classes being overrepresented while others are underrepresented, it would be useful to set the class_weight argument in the fit method. \n",
    "\n",
    "For cases where we want instances to be given different weights, like in the case of outliers (NBA dataset), we need to use sample_weight. For sample_weight we can either pass a flat Numpy array with the same length as the input samples. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        loss  accuracy  val_loss  val_accuracy\n",
       "0   0.710838  0.767945  0.484199        0.8444\n",
       "1   0.484503  0.830164  0.486281        0.8352\n",
       "2   0.438700  0.845473  0.427804        0.8530\n",
       "3   0.411235  0.855000  0.407846        0.8596\n",
       "4   0.390900  0.861945  0.426663        0.8556\n",
       "5   0.374824  0.866727  0.372038        0.8732\n",
       "6   0.361562  0.871636  0.356055        0.8764\n",
       "7   0.350971  0.876127  0.351028        0.8758\n",
       "8   0.339491  0.878545  0.349743        0.8804\n",
       "9   0.330113  0.882327  0.359438        0.8738\n",
       "10  0.320474  0.885891  0.333245        0.8794\n",
       "11  0.313341  0.887836  0.325336        0.8836\n",
       "12  0.306312  0.890655  0.324644        0.8828\n",
       "13  0.298783  0.893291  0.328875        0.8832\n",
       "14  0.293780  0.894236  0.319673        0.8874\n",
       "15  0.286795  0.897455  0.333719        0.8804\n",
       "16  0.281615  0.899000  0.314398        0.8848\n",
       "17  0.276009  0.900836  0.324981        0.8870\n",
       "18  0.270115  0.902218  0.309769        0.8898\n",
       "19  0.265474  0.903455  0.317170        0.8866\n",
       "20  0.260283  0.905764  0.303582        0.8936\n",
       "21  0.256729  0.907618  0.308288        0.8916\n",
       "22  0.252184  0.909364  0.307364        0.8900\n",
       "23  0.247301  0.910436  0.308961        0.8896\n",
       "24  0.243454  0.913236  0.304800        0.8936\n",
       "25  0.239139  0.912909  0.297939        0.8950\n",
       "26  0.233784  0.916473  0.301083        0.8922\n",
       "27  0.230425  0.917836  0.299132        0.8964\n",
       "28  0.226007  0.919655  0.310028        0.8902\n",
       "29  0.222734  0.919800  0.296214        0.8936"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n      <th>accuracy</th>\n      <th>val_loss</th>\n      <th>val_accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.710838</td>\n      <td>0.767945</td>\n      <td>0.484199</td>\n      <td>0.8444</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.484503</td>\n      <td>0.830164</td>\n      <td>0.486281</td>\n      <td>0.8352</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.438700</td>\n      <td>0.845473</td>\n      <td>0.427804</td>\n      <td>0.8530</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.411235</td>\n      <td>0.855000</td>\n      <td>0.407846</td>\n      <td>0.8596</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.390900</td>\n      <td>0.861945</td>\n      <td>0.426663</td>\n      <td>0.8556</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.374824</td>\n      <td>0.866727</td>\n      <td>0.372038</td>\n      <td>0.8732</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.361562</td>\n      <td>0.871636</td>\n      <td>0.356055</td>\n      <td>0.8764</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.350971</td>\n      <td>0.876127</td>\n      <td>0.351028</td>\n      <td>0.8758</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.339491</td>\n      <td>0.878545</td>\n      <td>0.349743</td>\n      <td>0.8804</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.330113</td>\n      <td>0.882327</td>\n      <td>0.359438</td>\n      <td>0.8738</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.320474</td>\n      <td>0.885891</td>\n      <td>0.333245</td>\n      <td>0.8794</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.313341</td>\n      <td>0.887836</td>\n      <td>0.325336</td>\n      <td>0.8836</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.306312</td>\n      <td>0.890655</td>\n      <td>0.324644</td>\n      <td>0.8828</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.298783</td>\n      <td>0.893291</td>\n      <td>0.328875</td>\n      <td>0.8832</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.293780</td>\n      <td>0.894236</td>\n      <td>0.319673</td>\n      <td>0.8874</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.286795</td>\n      <td>0.897455</td>\n      <td>0.333719</td>\n      <td>0.8804</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.281615</td>\n      <td>0.899000</td>\n      <td>0.314398</td>\n      <td>0.8848</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.276009</td>\n      <td>0.900836</td>\n      <td>0.324981</td>\n      <td>0.8870</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.270115</td>\n      <td>0.902218</td>\n      <td>0.309769</td>\n      <td>0.8898</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.265474</td>\n      <td>0.903455</td>\n      <td>0.317170</td>\n      <td>0.8866</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.260283</td>\n      <td>0.905764</td>\n      <td>0.303582</td>\n      <td>0.8936</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.256729</td>\n      <td>0.907618</td>\n      <td>0.308288</td>\n      <td>0.8916</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.252184</td>\n      <td>0.909364</td>\n      <td>0.307364</td>\n      <td>0.8900</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.247301</td>\n      <td>0.910436</td>\n      <td>0.308961</td>\n      <td>0.8896</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.243454</td>\n      <td>0.913236</td>\n      <td>0.304800</td>\n      <td>0.8936</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.239139</td>\n      <td>0.912909</td>\n      <td>0.297939</td>\n      <td>0.8950</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.233784</td>\n      <td>0.916473</td>\n      <td>0.301083</td>\n      <td>0.8922</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.230425</td>\n      <td>0.917836</td>\n      <td>0.299132</td>\n      <td>0.8964</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.226007</td>\n      <td>0.919655</td>\n      <td>0.310028</td>\n      <td>0.8902</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.222734</td>\n      <td>0.919800</td>\n      <td>0.296214</td>\n      <td>0.8936</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data = history.history)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1cffa0c0e80>"
      ]
     },
     "metadata": {},
     "execution_count": 22
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 864x720 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"574.678125pt\" version=\"1.1\" viewBox=\"0 0 706.903125 574.678125\" width=\"706.903125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-04-06T12:27:49.381969</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 574.678125 \r\nL 706.903125 574.678125 \r\nL 706.903125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 30.103125 550.8 \r\nL 699.703125 550.8 \r\nL 699.703125 7.2 \r\nL 30.103125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m64f0bb20e2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"60.539489\" xlink:href=\"#m64f0bb20e2\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(57.358239 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"165.492467\" xlink:href=\"#m64f0bb20e2\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(162.311217 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"270.445445\" xlink:href=\"#m64f0bb20e2\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(264.082945 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"375.398423\" xlink:href=\"#m64f0bb20e2\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(369.035923 565.398438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"480.351401\" xlink:href=\"#m64f0bb20e2\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(473.988901 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"585.304379\" xlink:href=\"#m64f0bb20e2\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(578.941879 565.398438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"690.257357\" xlink:href=\"#m64f0bb20e2\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 30 -->\r\n      <g transform=\"translate(683.894857 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_8\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"md380aa2967\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#md380aa2967\" y=\"542.20832\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(7.2 546.007539)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#md380aa2967\" y=\"471.313733\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.3 -->\r\n      <g transform=\"translate(7.2 475.112951)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#md380aa2967\" y=\"400.419145\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(7.2 404.218364)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#md380aa2967\" y=\"329.524558\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.5 -->\r\n      <g transform=\"translate(7.2 333.323776)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#md380aa2967\" y=\"258.62997\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.6 -->\r\n      <g transform=\"translate(7.2 262.429189)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#md380aa2967\" y=\"187.735383\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 0.7 -->\r\n      <g transform=\"translate(7.2 191.534601)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#md380aa2967\" y=\"116.840795\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 0.8 -->\r\n      <g transform=\"translate(7.2 120.640014)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#md380aa2967\" y=\"45.946208\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 0.9 -->\r\n      <g transform=\"translate(7.2 49.745426)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.984375 1.515625 \r\nL 10.984375 10.5 \r\nQ 14.703125 8.734375 18.5 7.8125 \r\nQ 22.3125 6.890625 25.984375 6.890625 \r\nQ 35.75 6.890625 40.890625 13.453125 \r\nQ 46.046875 20.015625 46.78125 33.40625 \r\nQ 43.953125 29.203125 39.59375 26.953125 \r\nQ 35.25 24.703125 29.984375 24.703125 \r\nQ 19.046875 24.703125 12.671875 31.3125 \r\nQ 6.296875 37.9375 6.296875 49.421875 \r\nQ 6.296875 60.640625 12.9375 67.421875 \r\nQ 19.578125 74.21875 30.609375 74.21875 \r\nQ 43.265625 74.21875 49.921875 64.515625 \r\nQ 56.59375 54.828125 56.59375 36.375 \r\nQ 56.59375 19.140625 48.40625 8.859375 \r\nQ 40.234375 -1.421875 26.421875 -1.421875 \r\nQ 22.703125 -1.421875 18.890625 -0.6875 \r\nQ 15.09375 0.046875 10.984375 1.515625 \r\nz\r\nM 30.609375 32.421875 \r\nQ 37.25 32.421875 41.125 36.953125 \r\nQ 45.015625 41.5 45.015625 49.421875 \r\nQ 45.015625 57.28125 41.125 61.84375 \r\nQ 37.25 66.40625 30.609375 66.40625 \r\nQ 23.96875 66.40625 20.09375 61.84375 \r\nQ 16.21875 57.28125 16.21875 49.421875 \r\nQ 16.21875 41.5 20.09375 36.953125 \r\nQ 23.96875 32.421875 30.609375 32.421875 \r\nz\r\n\" id=\"DejaVuSans-57\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_16\">\r\n    <path clip-path=\"url(#pfcb77dbbc2)\" d=\"M 60.539489 180.052024 \r\nL 81.530084 340.510922 \r\nL 102.52068 372.982883 \r\nL 123.511275 392.454156 \r\nL 144.501871 406.870309 \r\nL 165.492467 418.267512 \r\nL 186.483062 427.669534 \r\nL 207.473658 435.17836 \r\nL 228.464254 443.316448 \r\nL 249.454849 449.965503 \r\nL 270.445445 456.798987 \r\nL 291.43604 461.855861 \r\nL 312.426636 466.83887 \r\nL 333.417232 472.176728 \r\nL 354.407827 475.723292 \r\nL 375.398423 480.675137 \r\nL 396.389018 484.347626 \r\nL 417.379614 488.32191 \r\nL 438.37021 492.500673 \r\nL 459.360805 495.790867 \r\nL 480.351401 499.471046 \r\nL 501.341996 501.990398 \r\nL 522.332592 505.212644 \r\nL 543.323188 508.674262 \r\nL 564.313783 511.401948 \r\nL 585.304379 514.460651 \r\nL 606.294975 518.25749 \r\nL 627.28557 520.638621 \r\nL 648.276166 523.770765 \r\nL 669.266761 526.090909 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_17\">\r\n    <path clip-path=\"url(#pfcb77dbbc2)\" d=\"M 60.539489 139.565723 \r\nL 81.530084 95.456394 \r\nL 102.52068 84.603074 \r\nL 123.511275 77.848758 \r\nL 144.501871 72.924824 \r\nL 165.492467 69.534756 \r\nL 186.483062 66.054471 \r\nL 207.473658 62.8707 \r\nL 228.464254 61.156313 \r\nL 249.454849 58.475225 \r\nL 270.445445 55.948795 \r\nL 291.43604 54.569586 \r\nL 312.426636 52.571616 \r\nL 333.417232 50.70257 \r\nL 354.407827 50.032298 \r\nL 375.398423 47.750787 \r\nL 396.389018 46.655161 \r\nL 417.379614 45.353282 \r\nL 438.37021 44.37365 \r\nL 459.360805 43.497124 \r\nL 480.351401 41.860109 \r\nL 501.341996 40.545341 \r\nL 522.332592 39.307902 \r\nL 543.323188 38.547371 \r\nL 564.313783 36.562331 \r\nL 585.304379 36.794361 \r\nL 606.294975 34.267931 \r\nL 627.28557 33.301188 \r\nL 648.276166 32.012197 \r\nL 669.266761 31.909091 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_18\">\r\n    <path clip-path=\"url(#pfcb77dbbc2)\" d=\"M 60.539489 340.726831 \r\nL 81.530084 339.250517 \r\nL 102.52068 380.707365 \r\nL 123.511275 394.856774 \r\nL 144.501871 381.516386 \r\nL 165.492467 420.242431 \r\nL 186.483062 431.573524 \r\nL 207.473658 435.137857 \r\nL 228.464254 436.048399 \r\nL 249.454849 429.175089 \r\nL 270.445445 447.745011 \r\nL 291.43604 453.351577 \r\nL 312.426636 453.84264 \r\nL 333.417232 450.843191 \r\nL 354.407827 457.366343 \r\nL 375.398423 447.408903 \r\nL 396.389018 461.106379 \r\nL 417.379614 453.603595 \r\nL 438.37021 464.387805 \r\nL 459.360805 459.141009 \r\nL 480.351401 468.774132 \r\nL 501.341996 465.43811 \r\nL 522.332592 466.093212 \r\nL 543.323188 464.96095 \r\nL 564.313783 467.911043 \r\nL 585.304379 472.77491 \r\nL 606.294975 470.546051 \r\nL 627.28557 471.929211 \r\nL 648.276166 464.204686 \r\nL 669.266761 473.997686 \r\n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_19\">\r\n    <path clip-path=\"url(#pfcb77dbbc2)\" d=\"M 60.539489 85.363606 \r\nL 81.530084 91.885892 \r\nL 102.52068 79.266674 \r\nL 123.511275 74.587616 \r\nL 144.501871 77.423405 \r\nL 165.492467 64.945957 \r\nL 186.483062 62.677335 \r\nL 207.473658 63.102688 \r\nL 228.464254 59.841545 \r\nL 249.454849 64.520604 \r\nL 270.445445 60.550482 \r\nL 291.43604 57.572922 \r\nL 312.426636 58.140089 \r\nL 333.417232 57.856505 \r\nL 354.407827 54.878946 \r\nL 375.398423 59.841545 \r\nL 396.389018 56.722173 \r\nL 417.379614 55.162487 \r\nL 438.37021 53.177447 \r\nL 459.360805 55.44607 \r\nL 480.351401 50.48347 \r\nL 501.341996 51.901344 \r\nL 522.332592 53.035676 \r\nL 543.323188 53.31926 \r\nL 564.313783 50.48347 \r\nL 585.304379 49.49095 \r\nL 606.294975 51.47599 \r\nL 627.28557 48.498431 \r\nL 648.276166 52.893864 \r\nL 669.266761 50.48347 \r\n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 30.103125 550.8 \r\nL 30.103125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 699.703125 550.8 \r\nL 699.703125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 30.103125 550.8 \r\nL 699.703125 550.8 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 30.103125 7.2 \r\nL 699.703125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 37.103125 74.46875 \r\nL 134.046875 74.46875 \r\nQ 136.046875 74.46875 136.046875 72.46875 \r\nL 136.046875 14.2 \r\nQ 136.046875 12.2 134.046875 12.2 \r\nL 37.103125 12.2 \r\nQ 35.103125 12.2 35.103125 14.2 \r\nL 35.103125 72.46875 \r\nQ 35.103125 74.46875 37.103125 74.46875 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_20\">\r\n     <path d=\"M 39.103125 20.298437 \r\nL 59.103125 20.298437 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_21\"/>\r\n    <g id=\"text_16\">\r\n     <!-- loss -->\r\n     <g transform=\"translate(67.103125 23.798437)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n       <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_22\">\r\n     <path d=\"M 39.103125 34.976562 \r\nL 59.103125 34.976562 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_23\"/>\r\n    <g id=\"text_17\">\r\n     <!-- accuracy -->\r\n     <g transform=\"translate(67.103125 38.476562)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n       <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n       <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n       <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n       <path d=\"M 32.171875 -5.078125 \r\nQ 28.375 -14.84375 24.75 -17.8125 \r\nQ 21.140625 -20.796875 15.09375 -20.796875 \r\nL 7.90625 -20.796875 \r\nL 7.90625 -13.28125 \r\nL 13.1875 -13.28125 \r\nQ 16.890625 -13.28125 18.9375 -11.515625 \r\nQ 21 -9.765625 23.484375 -3.21875 \r\nL 25.09375 0.875 \r\nL 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 11.921875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nz\r\n\" id=\"DejaVuSans-121\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-121\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_24\">\r\n     <path d=\"M 39.103125 49.654688 \r\nL 59.103125 49.654688 \r\n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_25\"/>\r\n    <g id=\"text_18\">\r\n     <!-- val_loss -->\r\n     <g transform=\"translate(67.103125 53.154688)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n       <path d=\"M 50.984375 -16.609375 \r\nL 50.984375 -23.578125 \r\nL -0.984375 -23.578125 \r\nL -0.984375 -16.609375 \r\nz\r\n\" id=\"DejaVuSans-95\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"226.025391\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"287.207031\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"339.306641\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_26\">\r\n     <path d=\"M 39.103125 64.610937 \r\nL 59.103125 64.610937 \r\n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_27\"/>\r\n    <g id=\"text_19\">\r\n     <!-- val_accuracy -->\r\n     <g transform=\"translate(67.103125 68.110937)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"259.521484\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"314.501953\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"369.482422\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"432.861328\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"473.974609\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"535.253906\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"590.234375\" xlink:href=\"#DejaVuSans-121\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pfcb77dbbc2\">\r\n   <rect height=\"543.6\" width=\"669.6\" x=\"30.103125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig,ax = plt.subplots(1,1,figsize = (12,10))\n",
    "ax.plot(df.index.values.tolist(),df[\"loss\"], label = \"loss\")\n",
    "ax.plot(df.index.values.tolist(),df[\"accuracy\"], label = \"accuracy\")\n",
    "ax.plot(df.index.values.tolist(),df[\"val_loss\"], label = \"val_loss\")\n",
    "ax.plot(df.index.values.tolist(),df[\"val_accuracy\"], label = \"val_accuracy\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 59.2120 - accuracy: 0.8486\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'loss': 59.21195602416992, 'accuracy': 0.8485999703407288}"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, return_dict=True)"
   ]
  },
  {
   "source": [
    "Making predictions using a model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "#y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([9, 2, 1], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "model.predict_classes(X_new)"
   ]
  },
  {
   "source": [
    "<h2> Building a Regression MLP using the Sequential API </h2> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using a regression neural network\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "x_train_full_reg,x_test_reg,y_train_full_reg,y_test_reg = train_test_split(housing.data, housing.target)\n",
    "\n",
    "\n",
    "x_train_reg, x_valid_reg, y_train_reg, y_valid_reg = train_test_split(x_train_full_reg, y_train_full_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "#notice that we fit_transform on the train set and transform on the validation and test sets\n",
    "x_train_reg_scaled = scaler.fit_transform(x_train_reg)\n",
    "x_valid_reg_scaled = scaler.transform(x_valid_reg)\n",
    "x_test_reg_scaled = scaler.transform(x_test_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reg = keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding input layer\n",
    "model_reg.add(keras.layers.InputLayer(input_shape = x_train_reg_scaled.shape[1:]))\n",
    "model_reg.add(keras.layers.Dense(30, activation = \"relu\"))\n",
    "#the output neuron has no activation function to it\n",
    "model_reg.add(keras.layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_3 (Dense)              (None, 30)                270       \n_________________________________________________________________\ndense_4 (Dense)              (None, 1)                 31        \n=================================================================\nTotal params: 301\nTrainable params: 301\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no need to always add the input_layer. We can always user keras.layers.Dense\n",
    "model_test = keras.models.Sequential([\n",
    "keras.layers.Dense(30, activation=\"relu\", input_shape=x_train_reg_scaled.shape[1:]),\n",
    "keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "x_train_reg_scaled.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_5 (Dense)              (None, 30)                270       \n_________________________________________________________________\ndense_6 (Dense)              (None, 1)                 31        \n=================================================================\nTotal params: 301\nTrainable params: 301\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_test.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reg.compile(loss = \"mean_squared_error\", optimizer = \"sgd\", metrics=tf.keras.metrics.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 3ms/step - loss: 1.4423 - mean_squared_error: 1.4423 - val_loss: 0.5249 - val_mean_squared_error: 0.5249\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4790 - mean_squared_error: 0.4790 - val_loss: 0.4471 - val_mean_squared_error: 0.4471\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4297 - mean_squared_error: 0.4297 - val_loss: 0.4262 - val_mean_squared_error: 0.4262\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4013 - mean_squared_error: 0.4013 - val_loss: 0.4228 - val_mean_squared_error: 0.4228\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.4015 - mean_squared_error: 0.4015 - val_loss: 0.4106 - val_mean_squared_error: 0.4106\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3835 - mean_squared_error: 0.3835 - val_loss: 0.4033 - val_mean_squared_error: 0.4033\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3920 - mean_squared_error: 0.3920 - val_loss: 0.4058 - val_mean_squared_error: 0.4058\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3802 - mean_squared_error: 0.3802 - val_loss: 0.4116 - val_mean_squared_error: 0.4116\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4130 - mean_squared_error: 0.4130 - val_loss: 0.4013 - val_mean_squared_error: 0.4013\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3800 - mean_squared_error: 0.3800 - val_loss: 0.3918 - val_mean_squared_error: 0.3918\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3964 - mean_squared_error: 0.3964 - val_loss: 0.3831 - val_mean_squared_error: 0.3831\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3626 - mean_squared_error: 0.3626 - val_loss: 0.3882 - val_mean_squared_error: 0.3882\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3534 - mean_squared_error: 0.3534 - val_loss: 0.3800 - val_mean_squared_error: 0.3800\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3648 - mean_squared_error: 0.3648 - val_loss: 0.3764 - val_mean_squared_error: 0.3764\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3475 - mean_squared_error: 0.3475 - val_loss: 0.3750 - val_mean_squared_error: 0.3750\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3474 - mean_squared_error: 0.3474 - val_loss: 0.3698 - val_mean_squared_error: 0.3698\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3570 - mean_squared_error: 0.3570 - val_loss: 0.3695 - val_mean_squared_error: 0.3695\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3608 - mean_squared_error: 0.3608 - val_loss: 0.3659 - val_mean_squared_error: 0.3659\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3551 - mean_squared_error: 0.3551 - val_loss: 0.3687 - val_mean_squared_error: 0.3687\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3481 - mean_squared_error: 0.3481 - val_loss: 0.3974 - val_mean_squared_error: 0.3974\n"
     ]
    }
   ],
   "source": [
    "\"\"\" for epoch in range(n_epochs):\n",
    "for i in range(m):\n",
    "random_index = np.random.randint(m)\n",
    "xi = X_b[random_index:random_index+1]\n",
    "yi = y[random_index:random_index+1]\n",
    "gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "eta = learning_schedule(epoch * m + i)\n",
    "theta = theta - eta * gradients \"\"\"\n",
    "\n",
    "#The above code shows how epochs work. For each epoch, It selects batches of instances and then applies the training for a stochastic instance in each batch.\n",
    "\n",
    "history = model_reg.fit(x_train_reg_scaled, y_train_reg, epochs = 20, validation_data = (x_valid_reg_scaled, y_valid_reg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'loss': [1.1036250591278076,\n",
       "  0.4654254913330078,\n",
       "  0.4193516671657562,\n",
       "  0.41009992361068726,\n",
       "  0.3951289653778076,\n",
       "  0.38914090394973755,\n",
       "  0.41549891233444214,\n",
       "  0.39577385783195496,\n",
       "  0.4018981456756592,\n",
       "  0.3758843243122101,\n",
       "  0.3750179708003998,\n",
       "  0.3687768876552582,\n",
       "  0.3656551241874695,\n",
       "  0.36458173394203186,\n",
       "  0.35990020632743835,\n",
       "  0.35984402894973755,\n",
       "  0.356900691986084,\n",
       "  0.35531505942344666,\n",
       "  0.3517639935016632,\n",
       "  0.3506520092487335],\n",
       " 'mean_squared_error': [1.1036250591278076,\n",
       "  0.4654254913330078,\n",
       "  0.4193516671657562,\n",
       "  0.41009992361068726,\n",
       "  0.3951289653778076,\n",
       "  0.38914090394973755,\n",
       "  0.41549891233444214,\n",
       "  0.39577385783195496,\n",
       "  0.4018981456756592,\n",
       "  0.3758843243122101,\n",
       "  0.3750179708003998,\n",
       "  0.3687768876552582,\n",
       "  0.3656551241874695,\n",
       "  0.36458173394203186,\n",
       "  0.35990020632743835,\n",
       "  0.35984402894973755,\n",
       "  0.356900691986084,\n",
       "  0.35531505942344666,\n",
       "  0.3517639935016632,\n",
       "  0.3506520092487335],\n",
       " 'val_loss': [0.524940013885498,\n",
       "  0.4470668137073517,\n",
       "  0.426176518201828,\n",
       "  0.42276883125305176,\n",
       "  0.41061943769454956,\n",
       "  0.40329352021217346,\n",
       "  0.4058232307434082,\n",
       "  0.411607950925827,\n",
       "  0.40130361914634705,\n",
       "  0.39180299639701843,\n",
       "  0.3830645680427551,\n",
       "  0.3882374167442322,\n",
       "  0.3800026476383209,\n",
       "  0.37643080949783325,\n",
       "  0.37495291233062744,\n",
       "  0.36975622177124023,\n",
       "  0.369485080242157,\n",
       "  0.36591216921806335,\n",
       "  0.36866089701652527,\n",
       "  0.39739251136779785],\n",
       " 'val_mean_squared_error': [0.524940013885498,\n",
       "  0.4470668137073517,\n",
       "  0.426176518201828,\n",
       "  0.42276883125305176,\n",
       "  0.41061943769454956,\n",
       "  0.40329352021217346,\n",
       "  0.4058232307434082,\n",
       "  0.411607950925827,\n",
       "  0.40130361914634705,\n",
       "  0.39180299639701843,\n",
       "  0.3830645680427551,\n",
       "  0.3882374167442322,\n",
       "  0.3800026476383209,\n",
       "  0.37643080949783325,\n",
       "  0.37495291233062744,\n",
       "  0.36975622177124023,\n",
       "  0.369485080242157,\n",
       "  0.36591216921806335,\n",
       "  0.36866089701652527,\n",
       "  0.39739251136779785]}"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test.compile(loss = \"mean_squared_error\", optimizer = \"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 3ms/step - loss: 1.2975 - val_loss: 1.0221\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5514 - val_loss: 0.4875\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4545 - val_loss: 0.4674\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4381 - val_loss: 0.4462\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4218 - val_loss: 0.4388\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4138 - val_loss: 0.4364\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4055 - val_loss: 0.4240\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3903 - val_loss: 0.4195\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4038 - val_loss: 0.4172\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4041 - val_loss: 0.4093\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3881 - val_loss: 0.5169\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4109 - val_loss: 0.4048\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4031 - val_loss: 0.4076\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3766 - val_loss: 0.4014\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3666 - val_loss: 0.3980\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3703 - val_loss: 0.3975\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3798 - val_loss: 0.4011\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3851 - val_loss: 0.3951\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3726 - val_loss: 0.3915\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3770 - val_loss: 0.3885\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model_test.fit(x_train_reg_scaled, y_train_reg, epochs = 20, validation_data = (x_valid_reg_scaled, y_valid_reg))"
   ]
  },
  {
   "source": [
    "<h3> Building complex models using the Functional API </h3> \n",
    "\n",
    "These are non sequential neural networks. This architecture allows a neural network to learn both deep patterns (using the deep path) and simple rules (through the short path)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for functional APIS\n",
    "#new layer = newlayerconfig(previous_layer)\n",
    "#create an Input opbject\n",
    "inputlayer = keras.layers.Input(shape = x_train_full_reg.shape[1:])\n",
    "#30 neuron dense layer using ReLU activation. Once created we call it like a function and pass in the input layer as a parameter\n",
    "hidden1 = keras.layers.Dense(30, activation = \"relu\")(inputlayer)\n",
    "#second hidden layer\n",
    "hidden2 = keras.layers.Dense(30, activation = \"relu\")(hidden1)\n",
    "#this concatenates the input and the output of the second layer\n",
    "concat = keras.layers.concatenate([inputlayer,hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.models.Model(inputs = [inputlayer], outputs = [output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_2 (InputLayer)            [(None, 8)]          0                                            \n__________________________________________________________________________________________________\ndense_7 (Dense)                 (None, 30)           270         input_2[0][0]                    \n__________________________________________________________________________________________________\ndense_8 (Dense)                 (None, 30)           930         dense_7[0][0]                    \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 38)           0           input_2[0][0]                    \n                                                                 dense_8[0][0]                    \n__________________________________________________________________________________________________\ndense_9 (Dense)                 (None, 1)            39          concatenate[0][0]                \n==================================================================================================\nTotal params: 1,239\nTrainable params: 1,239\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sending a subset of features through the wide path and a different subset through the deep path\n",
    "input_A = keras.layers.Input(shape = [5])\n",
    "input_B = keras.layers.Input(shape = [6])\n",
    "hidden1 = keras.layers.Dense(30, activation = 'relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation = \"relu\")(hidden1)\n",
    "concat_2 = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1)(concat_2)\n",
    "model = keras.models.Model(inputs = [input_A,input_B], outputs = [output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 3ms/step - loss: 1.6851 - mean_squared_error: 1.6851 - val_loss: 0.6569 - val_mean_squared_error: 0.6569\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.5869 - mean_squared_error: 0.5869 - val_loss: 0.5395 - val_mean_squared_error: 0.5395\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.5018 - mean_squared_error: 0.5018 - val_loss: 0.5859 - val_mean_squared_error: 0.5859\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.6236 - mean_squared_error: 0.6236 - val_loss: 0.4936 - val_mean_squared_error: 0.4936\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4751 - mean_squared_error: 0.4751 - val_loss: 0.4825 - val_mean_squared_error: 0.4825\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4527 - mean_squared_error: 0.4527 - val_loss: 0.4766 - val_mean_squared_error: 0.4766\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4423 - mean_squared_error: 0.4423 - val_loss: 0.4763 - val_mean_squared_error: 0.4763\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4377 - mean_squared_error: 0.4377 - val_loss: 0.4623 - val_mean_squared_error: 0.4623\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4257 - mean_squared_error: 0.4257 - val_loss: 0.4584 - val_mean_squared_error: 0.4584\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4210 - mean_squared_error: 0.4210 - val_loss: 0.4469 - val_mean_squared_error: 0.4469\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4084 - mean_squared_error: 0.4084 - val_loss: 0.4451 - val_mean_squared_error: 0.4451\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4029 - mean_squared_error: 0.4029 - val_loss: 0.4317 - val_mean_squared_error: 0.4317\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4397 - mean_squared_error: 0.4397 - val_loss: 0.4269 - val_mean_squared_error: 0.4269\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4051 - mean_squared_error: 0.4051 - val_loss: 0.5006 - val_mean_squared_error: 0.5006\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3946 - mean_squared_error: 0.3946 - val_loss: 0.4291 - val_mean_squared_error: 0.4291\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3941 - mean_squared_error: 0.3941 - val_loss: 0.4078 - val_mean_squared_error: 0.4078\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4379 - mean_squared_error: 0.4379 - val_loss: 0.4077 - val_mean_squared_error: 0.4077\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3929 - mean_squared_error: 0.3929 - val_loss: 0.4011 - val_mean_squared_error: 0.4011\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3729 - mean_squared_error: 0.3729 - val_loss: 0.4111 - val_mean_squared_error: 0.4111\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3494 - mean_squared_error: 0.3494 - val_loss: 0.3892 - val_mean_squared_error: 0.3892\n",
      "162/162 [==============================] - 0s 1ms/step - loss: 0.3781 - mean_squared_error: 0.3781\n"
     ]
    }
   ],
   "source": [
    "#instead of passing a single input matrix, we need to input 2 different matrices. \n",
    "#using a regression neural network\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "x_train_full_reg,x_test_reg,y_train_full_reg,y_test_reg = train_test_split(housing.data, housing.target)\n",
    "\n",
    "\n",
    "x_train_reg, x_valid_reg, y_train_reg, y_valid_reg = train_test_split(x_train_full_reg, y_train_full_reg)\n",
    "scaler = StandardScaler()\n",
    "#notice that we fit_transform on the train set and transform on the validation and test sets\n",
    "x_train_reg = scaler.fit_transform(x_train_reg)\n",
    "x_valid_reg = scaler.transform(x_valid_reg)\n",
    "x_test_reg= scaler.transform(x_test_reg)\n",
    "\n",
    "model.compile(loss = \"mse\", optimizer = \"sgd\", metrics= tf.keras.metrics.MeanSquaredError())\n",
    "\n",
    "X_train_A, X_train_B = x_train_reg[:,:5],x_train_reg[:,2:]\n",
    "X_valid_A, X_valid_B = x_valid_reg[:,:5],x_valid_reg[:,2:]\n",
    "X_test_A,X_test_B = x_test_reg[:,:5],x_test_reg[:,2:]\n",
    "X_new_A,X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "model.fit((X_train_A,X_train_B),y_train_reg, epochs = 20, validation_data = ((X_valid_A, X_valid_B),y_valid_reg))\n",
    "\n",
    "mse_test = model.evaluate((X_test_A,X_test_B),y_test_reg)\n",
    "\n",
    "y_pred_test = model.predict((X_test_A, X_test_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1.0624081],\n",
       "       [3.7746255],\n",
       "       [2.4923341],\n",
       "       ...,\n",
       "       [3.5679145],\n",
       "       [0.5882095],\n",
       "       [1.1220242]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "y_pred_test"
   ]
  },
  {
   "source": [
    "<h3> There can be situations in which you want multiple outputs </h3> \n",
    "\n",
    "- Locating an object in a picture (regression to find coordinates and classification to classify image)\n",
    "\n",
    "- Multiple independent tasks to perform based on the same data. Train one neural network per task is possible, but this is inefficient. Also training one neural network with mutltiple outputs for the entire set of tasks allows higher accuracy too. This is because the neural network can learn data useful across the tasks. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multioutput\n",
    "input_A_multi = keras.layers.Input(shape = [5])\n",
    "input_B_mutli = keras.layers.Input(shape = [6])\n",
    "hidden1_multi = keras.layers.Dense(30, activation = 'relu')(input_B_mutli)\n",
    "hidden2_multi = keras.layers.Dense(30, activation = 'relu')(hidden1_multi)\n",
    "concat_multi = keras.layers.concatenate([input_A_multi, hidden2_multi])\n",
    "output1 = keras.layers.Dense(1)(concat_multi)\n",
    "output_aux = keras.layers.Dense(1)(hidden1_multi)\n",
    "model_multi = keras.models.Model(inputs = [input_A_multi,input_B_mutli], outputs = [output1,output_aux])"
   ]
  },
  {
   "source": [
    "For multi output neural networks, each output requires its own loss function. If one is passed, keras assumes its the same for all."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usually different outputs will be given different weights\n",
    "model_multi.compile(loss = ['mse','mse'],loss_weights = [0.9,0.1],optimizer = \"sgd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 3ms/step - loss: 1.9333 - dense_15_loss: 1.7648 - dense_16_loss: 3.4497 - val_loss: 0.6748 - val_dense_15_loss: 0.5889 - val_dense_16_loss: 1.4481\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.6422 - dense_15_loss: 0.5659 - dense_16_loss: 1.3286 - val_loss: 0.5806 - val_dense_15_loss: 0.5053 - val_dense_16_loss: 1.2585\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5472 - dense_15_loss: 0.4771 - dense_16_loss: 1.1780 - val_loss: 0.5552 - val_dense_15_loss: 0.4905 - val_dense_16_loss: 1.1366\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.5099 - dense_15_loss: 0.4487 - dense_16_loss: 1.0608 - val_loss: 0.5301 - val_dense_15_loss: 0.4732 - val_dense_16_loss: 1.0415\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4798 - dense_15_loss: 0.4265 - dense_16_loss: 0.9594 - val_loss: 0.5139 - val_dense_15_loss: 0.4633 - val_dense_16_loss: 0.9692\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4948 - dense_15_loss: 0.4483 - dense_16_loss: 0.9136 - val_loss: 0.7433 - val_dense_15_loss: 0.7225 - val_dense_16_loss: 0.9308\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.5420 - dense_15_loss: 0.5025 - dense_16_loss: 0.8977 - val_loss: 0.7102 - val_dense_15_loss: 0.6916 - val_dense_16_loss: 0.8776\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.5104 - dense_15_loss: 0.4745 - dense_16_loss: 0.8333 - val_loss: 0.4925 - val_dense_15_loss: 0.4548 - val_dense_16_loss: 0.8315\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4442 - dense_15_loss: 0.4072 - dense_16_loss: 0.7772 - val_loss: 0.4726 - val_dense_15_loss: 0.4356 - val_dense_16_loss: 0.8059\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4292 - dense_15_loss: 0.3939 - dense_16_loss: 0.7472 - val_loss: 0.4689 - val_dense_15_loss: 0.4340 - val_dense_16_loss: 0.7825\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4370 - dense_15_loss: 0.4018 - dense_16_loss: 0.7535 - val_loss: 0.4594 - val_dense_15_loss: 0.4248 - val_dense_16_loss: 0.7708\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4297 - dense_15_loss: 0.3959 - dense_16_loss: 0.7345 - val_loss: 0.4539 - val_dense_15_loss: 0.4204 - val_dense_16_loss: 0.7562\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4238 - dense_15_loss: 0.3905 - dense_16_loss: 0.7240 - val_loss: 0.4542 - val_dense_15_loss: 0.4213 - val_dense_16_loss: 0.7502\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.4152 - dense_15_loss: 0.3814 - dense_16_loss: 0.7196 - val_loss: 0.4470 - val_dense_15_loss: 0.4148 - val_dense_16_loss: 0.7367\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4348 - dense_15_loss: 0.4029 - dense_16_loss: 0.7219 - val_loss: 0.4402 - val_dense_15_loss: 0.4082 - val_dense_16_loss: 0.7281\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4070 - dense_15_loss: 0.3760 - dense_16_loss: 0.6860 - val_loss: 0.4315 - val_dense_15_loss: 0.3994 - val_dense_16_loss: 0.7211\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3940 - dense_15_loss: 0.3640 - dense_16_loss: 0.6642 - val_loss: 0.4319 - val_dense_15_loss: 0.4004 - val_dense_16_loss: 0.7154\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3915 - dense_15_loss: 0.3607 - dense_16_loss: 0.6687 - val_loss: 0.4225 - val_dense_15_loss: 0.3907 - val_dense_16_loss: 0.7090\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3831 - dense_15_loss: 0.3519 - dense_16_loss: 0.6645 - val_loss: 0.4271 - val_dense_15_loss: 0.3962 - val_dense_16_loss: 0.7044\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3858 - dense_15_loss: 0.3551 - dense_16_loss: 0.6614 - val_loss: 0.4146 - val_dense_15_loss: 0.3829 - val_dense_16_loss: 0.7004\n"
     ]
    }
   ],
   "source": [
    "#as we are using auxillary outputs, we will need to pass in auxillary output labels too\n",
    "history = model_multi.fit([X_train_A,X_train_B],[y_train_reg,y_train_reg], epochs = 20, validation_data= ([X_valid_A, X_valid_B],[y_valid_reg, y_valid_reg]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "162/162 [==============================] - 0s 2ms/step - loss: 0.4114 - dense_15_loss: 0.3796 - dense_16_loss: 0.6980\n"
     ]
    }
   ],
   "source": [
    "#if you have more than one output, you will have more than one loss\n",
    "total_loss, main_loss, aux_loss = model_multi.evaluate([X_test_A,X_test_B],[y_test_reg,y_test_reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.37959617376327515"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "main_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_main, y_pred_aux = model_multi.predict([X_test_A,X_test_B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1d001dfa040>"
      ]
     },
     "metadata": {},
     "execution_count": 52
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 576x720 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"574.678125pt\" version=\"1.1\" viewBox=\"0 0 500.565625 574.678125\" width=\"500.565625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-04-06T12:29:43.553922</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 574.678125 \r\nL 500.565625 574.678125 \r\nL 500.565625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 46.965625 550.8 \r\nL 493.365625 550.8 \r\nL 493.365625 7.2 \r\nL 46.965625 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 67.256534 550.8 \r\nL 75.71108 550.8 \r\nL 75.71108 529.811583 \r\nL 67.256534 529.811583 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 75.71108 550.8 \r\nL 84.165626 550.8 \r\nL 84.165626 529.811583 \r\nL 75.71108 529.811583 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 84.165626 550.8 \r\nL 92.62017 550.8 \r\nL 92.62017 482.237838 \r\nL 84.165626 482.237838 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 92.62017 550.8 \r\nL 101.074717 550.8 \r\nL 101.074717 388.489575 \r\nL 92.62017 388.489575 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_7\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 101.074717 550.8 \r\nL 109.529261 550.8 \r\nL 109.529261 213.5861 \r\nL 101.074717 213.5861 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_8\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 109.529261 550.8 \r\nL 117.983809 550.8 \r\nL 117.983809 153.419305 \r\nL 109.529261 153.419305 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_9\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 117.983809 550.8 \r\nL 126.438353 550.8 \r\nL 126.438353 163.2139 \r\nL 117.983809 163.2139 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_10\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 126.438353 550.8 \r\nL 134.892897 550.8 \r\nL 134.892897 161.814672 \r\nL 126.438353 161.814672 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_11\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 134.892897 550.8 \r\nL 143.347441 550.8 \r\nL 143.347441 139.427027 \r\nL 134.892897 139.427027 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_12\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 143.347441 550.8 \r\nL 151.801985 550.8 \r\nL 151.801985 90.454054 \r\nL 143.347441 90.454054 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_13\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 151.801985 550.8 \r\nL 160.256536 550.8 \r\nL 160.256536 118.43861 \r\nL 151.801985 118.43861 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_14\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 160.256536 550.8 \r\nL 168.71108 550.8 \r\nL 168.71108 103.047104 \r\nL 160.256536 103.047104 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_15\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 168.71108 550.8 \r\nL 177.165625 550.8 \r\nL 177.165625 131.03166 \r\nL 168.71108 131.03166 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_16\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 177.165625 550.8 \r\nL 185.620175 550.8 \r\nL 185.620175 131.03166 \r\nL 177.165625 131.03166 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_17\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 185.620175 550.8 \r\nL 194.074713 550.8 \r\nL 194.074713 196.795367 \r\nL 185.620175 196.795367 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_18\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 194.074713 550.8 \r\nL 202.529264 550.8 \r\nL 202.529264 152.020077 \r\nL 194.074713 152.020077 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_19\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 202.529264 550.8 \r\nL 210.983801 550.8 \r\nL 210.983801 167.411583 \r\nL 202.529264 167.411583 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_20\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 210.983801 550.8 \r\nL 219.438352 550.8 \r\nL 219.438352 212.186873 \r\nL 210.983801 212.186873 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_21\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 219.438352 550.8 \r\nL 227.892903 550.8 \r\nL 227.892903 234.574517 \r\nL 219.438352 234.574517 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_22\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 227.892903 550.8 \r\nL 236.34744 550.8 \r\nL 236.34744 301.737452 \r\nL 227.892903 301.737452 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_23\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 236.34744 550.8 \r\nL 244.801991 550.8 \r\nL 244.801991 318.528185 \r\nL 236.34744 318.528185 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_24\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 244.801991 550.8 \r\nL 253.256528 550.8 \r\nL 253.256528 426.268726 \r\nL 244.801991 426.268726 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_25\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 253.256528 550.8 \r\nL 261.711079 550.8 \r\nL 261.711079 454.253282 \r\nL 253.256528 454.253282 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_26\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 261.711079 550.8 \r\nL 270.16563 550.8 \r\nL 270.16563 486.435521 \r\nL 261.711079 486.435521 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_27\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 270.16563 550.8 \r\nL 278.620167 550.8 \r\nL 278.620167 501.827027 \r\nL 270.16563 501.827027 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_28\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 278.620167 550.8 \r\nL 287.074705 550.8 \r\nL 287.074705 499.028571 \r\nL 278.620167 499.028571 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_29\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 287.074705 550.8 \r\nL 295.529269 550.8 \r\nL 295.529269 525.6139 \r\nL 287.074705 525.6139 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_30\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 295.529269 550.8 \r\nL 303.983807 550.8 \r\nL 303.983807 521.416216 \r\nL 295.529269 521.416216 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_31\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 303.983807 550.8 \r\nL 312.438344 550.8 \r\nL 312.438344 529.811583 \r\nL 303.983807 529.811583 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_32\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 312.438344 550.8 \r\nL 320.892908 550.8 \r\nL 320.892908 536.807722 \r\nL 312.438344 536.807722 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_33\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 320.892908 550.8 \r\nL 329.347446 550.8 \r\nL 329.347446 535.408494 \r\nL 320.892908 535.408494 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_34\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 329.347446 550.8 \r\nL 337.801983 550.8 \r\nL 337.801983 536.807722 \r\nL 329.347446 536.807722 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_35\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 337.801983 550.8 \r\nL 346.256547 550.8 \r\nL 346.256547 534.009266 \r\nL 337.801983 534.009266 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_36\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 346.256547 550.8 \r\nL 354.711085 550.8 \r\nL 354.711085 546.602317 \r\nL 346.256547 546.602317 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_37\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 354.711085 550.8 \r\nL 363.165622 550.8 \r\nL 363.165622 543.803861 \r\nL 354.711085 543.803861 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_38\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 363.165622 550.8 \r\nL 371.62016 550.8 \r\nL 371.62016 545.203089 \r\nL 363.165622 545.203089 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_39\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 371.62016 550.8 \r\nL 380.074724 550.8 \r\nL 380.074724 546.602317 \r\nL 371.62016 546.602317 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_40\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 380.074724 550.8 \r\nL 388.529261 550.8 \r\nL 388.529261 543.803861 \r\nL 380.074724 543.803861 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_41\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 388.529261 550.8 \r\nL 396.983799 550.8 \r\nL 396.983799 542.404633 \r\nL 388.529261 542.404633 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_42\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 396.983799 550.8 \r\nL 405.438363 550.8 \r\nL 405.438363 549.400772 \r\nL 396.983799 549.400772 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_43\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 405.438363 550.8 \r\nL 413.8929 550.8 \r\nL 413.8929 546.602317 \r\nL 405.438363 546.602317 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_44\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 413.8929 550.8 \r\nL 422.347438 550.8 \r\nL 422.347438 549.400772 \r\nL 413.8929 549.400772 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_45\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 422.347438 550.8 \r\nL 430.802002 550.8 \r\nL 430.802002 545.203089 \r\nL 422.347438 545.203089 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_46\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 430.802002 550.8 \r\nL 439.256539 550.8 \r\nL 439.256539 550.8 \r\nL 430.802002 550.8 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_47\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 439.256539 550.8 \r\nL 447.711077 550.8 \r\nL 447.711077 550.8 \r\nL 439.256539 550.8 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_48\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 447.711077 550.8 \r\nL 456.165614 550.8 \r\nL 456.165614 550.8 \r\nL 447.711077 550.8 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_49\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 456.165614 550.8 \r\nL 464.620178 550.8 \r\nL 464.620178 550.8 \r\nL 456.165614 550.8 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_50\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 464.620178 550.8 \r\nL 473.074716 550.8 \r\nL 473.074716 549.400772 \r\nL 464.620178 549.400772 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n   </g>\r\n   <g id=\"patch_51\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 69.754816 550.8 \r\nL 78.755402 550.8 \r\nL 78.755402 543.803861 \r\nL 69.754816 543.803861 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_52\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 78.755402 550.8 \r\nL 87.755988 550.8 \r\nL 87.755988 513.020849 \r\nL 78.755402 513.020849 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_53\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 87.755988 550.8 \r\nL 96.756574 550.8 \r\nL 96.756574 276.551351 \r\nL 87.755988 276.551351 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_54\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 96.756574 550.8 \r\nL 105.75716 550.8 \r\nL 105.75716 209.388417 \r\nL 96.756574 209.388417 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_55\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 105.75716 550.8 \r\nL 114.757746 550.8 \r\nL 114.757746 125.434749 \r\nL 105.75716 125.434749 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_56\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 114.757746 550.8 \r\nL 123.758332 550.8 \r\nL 123.758332 86.256371 \r\nL 114.757746 86.256371 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_57\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 123.758332 550.8 \r\nL 132.758918 550.8 \r\nL 132.758918 104.446332 \r\nL 123.758332 104.446332 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_58\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 132.758918 550.8 \r\nL 141.759504 550.8 \r\nL 141.759504 98.849421 \r\nL 132.758918 98.849421 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_59\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 141.759504 550.8 \r\nL 150.76009 550.8 \r\nL 150.76009 68.066409 \r\nL 141.759504 68.066409 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_60\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 150.76009 550.8 \r\nL 159.760676 550.8 \r\nL 159.760676 33.085714 \r\nL 150.76009 33.085714 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_61\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 159.760676 550.8 \r\nL 168.761262 550.8 \r\nL 168.761262 61.07027 \r\nL 159.760676 61.07027 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_62\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 168.761262 550.8 \r\nL 177.761848 550.8 \r\nL 177.761848 219.183012 \r\nL 168.761262 219.183012 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_63\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 177.761848 550.8 \r\nL 186.762433 550.8 \r\nL 186.762433 164.613127 \r\nL 177.761848 164.613127 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_64\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 186.762433 550.8 \r\nL 195.763019 550.8 \r\nL 195.763019 193.996911 \r\nL 186.762433 193.996911 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_65\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 195.763019 550.8 \r\nL 204.763605 550.8 \r\nL 204.763605 258.36139 \r\nL 195.763019 258.36139 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_66\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 204.763605 550.8 \r\nL 213.764191 550.8 \r\nL 213.764191 298.938996 \r\nL 204.763605 298.938996 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_67\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 213.764191 550.8 \r\nL 222.764777 550.8 \r\nL 222.764777 339.516602 \r\nL 213.764191 339.516602 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_68\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 222.764777 550.8 \r\nL 231.765363 550.8 \r\nL 231.765363 403.881081 \r\nL 222.764777 403.881081 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_69\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 231.765363 550.8 \r\nL 240.765949 550.8 \r\nL 240.765949 434.664093 \r\nL 231.765363 434.664093 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_70\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 240.765949 550.8 \r\nL 249.766535 550.8 \r\nL 249.766535 406.679537 \r\nL 240.765949 406.679537 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_71\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 249.766535 550.8 \r\nL 258.767121 550.8 \r\nL 258.767121 382.892664 \r\nL 249.766535 382.892664 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_72\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 258.767121 550.8 \r\nL 267.767707 550.8 \r\nL 267.767707 451.454826 \r\nL 258.767121 451.454826 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_73\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 267.767707 550.8 \r\nL 276.768293 550.8 \r\nL 276.768293 465.447104 \r\nL 267.767707 465.447104 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_74\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 276.768293 550.8 \r\nL 285.768879 550.8 \r\nL 285.768879 478.040154 \r\nL 276.768293 478.040154 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_75\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 285.768879 550.8 \r\nL 294.769465 550.8 \r\nL 294.769465 479.439382 \r\nL 285.768879 479.439382 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_76\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 294.769465 550.8 \r\nL 303.770051 550.8 \r\nL 303.770051 501.827027 \r\nL 294.769465 501.827027 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_77\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 303.770051 550.8 \r\nL 312.770637 550.8 \r\nL 312.770637 490.633205 \r\nL 303.770051 490.633205 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_78\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 312.770637 550.8 \r\nL 321.771223 550.8 \r\nL 321.771223 520.016988 \r\nL 312.770637 520.016988 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_79\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 321.771223 550.8 \r\nL 330.771809 550.8 \r\nL 330.771809 518.617761 \r\nL 321.771223 518.617761 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"patch_80\">\r\n    <path clip-path=\"url(#p0dd20c7351)\" d=\"M 330.771809 550.8 \r\nL 339.772395 550.8 \r\nL 339.772395 175.80695 \r\nL 330.771809 175.80695 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m714d087da6\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"61.404348\" xlink:href=\"#m714d087da6\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(58.223098 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"117.077846\" xlink:href=\"#m714d087da6\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 1 -->\r\n      <g transform=\"translate(113.896596 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"172.751344\" xlink:href=\"#m714d087da6\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 2 -->\r\n      <g transform=\"translate(169.570094 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"228.424842\" xlink:href=\"#m714d087da6\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 3 -->\r\n      <g transform=\"translate(225.243592 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"284.09834\" xlink:href=\"#m714d087da6\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 4 -->\r\n      <g transform=\"translate(280.91709 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"339.771838\" xlink:href=\"#m714d087da6\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(336.590588 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"395.445336\" xlink:href=\"#m714d087da6\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 6 -->\r\n      <g transform=\"translate(392.264086 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_8\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"451.118834\" xlink:href=\"#m714d087da6\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 7 -->\r\n      <g transform=\"translate(447.937584 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-55\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_9\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m52be77df08\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m52be77df08\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(33.603125 554.599219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m52be77df08\" y=\"480.83861\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(27.240625 484.637829)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m52be77df08\" y=\"410.87722\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(20.878125 414.676439)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m52be77df08\" y=\"340.91583\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 150 -->\r\n      <g transform=\"translate(20.878125 344.715049)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m52be77df08\" y=\"270.95444\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(20.878125 274.753659)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m52be77df08\" y=\"200.99305\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 250 -->\r\n      <g transform=\"translate(20.878125 204.792269)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m52be77df08\" y=\"131.03166\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 300 -->\r\n      <g transform=\"translate(20.878125 134.830879)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m52be77df08\" y=\"61.07027\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_16\">\r\n      <!-- 350 -->\r\n      <g transform=\"translate(20.878125 64.869489)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_17\">\r\n     <!-- Count -->\r\n     <g transform=\"translate(14.798437 293.848437)rotate(-90)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 64.40625 67.28125 \r\nL 64.40625 56.890625 \r\nQ 59.421875 61.53125 53.78125 63.8125 \r\nQ 48.140625 66.109375 41.796875 66.109375 \r\nQ 29.296875 66.109375 22.65625 58.46875 \r\nQ 16.015625 50.828125 16.015625 36.375 \r\nQ 16.015625 21.96875 22.65625 14.328125 \r\nQ 29.296875 6.6875 41.796875 6.6875 \r\nQ 48.140625 6.6875 53.78125 8.984375 \r\nQ 59.421875 11.28125 64.40625 15.921875 \r\nL 64.40625 5.609375 \r\nQ 59.234375 2.09375 53.4375 0.328125 \r\nQ 47.65625 -1.421875 41.21875 -1.421875 \r\nQ 24.65625 -1.421875 15.125 8.703125 \r\nQ 5.609375 18.84375 5.609375 36.375 \r\nQ 5.609375 53.953125 15.125 64.078125 \r\nQ 24.65625 74.21875 41.21875 74.21875 \r\nQ 47.75 74.21875 53.53125 72.484375 \r\nQ 59.328125 70.75 64.40625 67.28125 \r\nz\r\n\" id=\"DejaVuSans-67\"/>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n       <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n       <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-67\"/>\r\n      <use x=\"69.824219\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"131.005859\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"194.384766\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"257.763672\" xlink:href=\"#DejaVuSans-116\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_81\">\r\n    <path d=\"M 46.965625 550.8 \r\nL 46.965625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_82\">\r\n    <path d=\"M 493.365625 550.8 \r\nL 493.365625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_83\">\r\n    <path d=\"M 46.965625 550.8 \r\nL 493.365625 550.8 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_84\">\r\n    <path d=\"M 46.965625 7.2 \r\nL 493.365625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_85\">\r\n     <path d=\"M 419.926562 44.55625 \r\nL 486.365625 44.55625 \r\nQ 488.365625 44.55625 488.365625 42.55625 \r\nL 488.365625 14.2 \r\nQ 488.365625 12.2 486.365625 12.2 \r\nL 419.926562 12.2 \r\nQ 417.926562 12.2 417.926562 14.2 \r\nL 417.926562 42.55625 \r\nQ 417.926562 44.55625 419.926562 44.55625 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"patch_86\">\r\n     <path d=\"M 421.926562 23.798437 \r\nL 441.926562 23.798437 \r\nL 441.926562 16.798437 \r\nL 421.926562 16.798437 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.845454;\"/>\r\n    </g>\r\n    <g id=\"text_18\">\r\n     <!-- Predict -->\r\n     <g transform=\"translate(449.926562 23.798437)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 19.671875 64.796875 \r\nL 19.671875 37.40625 \r\nL 32.078125 37.40625 \r\nQ 38.96875 37.40625 42.71875 40.96875 \r\nQ 46.484375 44.53125 46.484375 51.125 \r\nQ 46.484375 57.671875 42.71875 61.234375 \r\nQ 38.96875 64.796875 32.078125 64.796875 \r\nz\r\nM 9.8125 72.90625 \r\nL 32.078125 72.90625 \r\nQ 44.34375 72.90625 50.609375 67.359375 \r\nQ 56.890625 61.8125 56.890625 51.125 \r\nQ 56.890625 40.328125 50.609375 34.8125 \r\nQ 44.34375 29.296875 32.078125 29.296875 \r\nL 19.671875 29.296875 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-80\"/>\r\n       <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n       <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n       <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n       <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n       <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-80\"/>\r\n      <use x=\"58.552734\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"97.416016\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"158.939453\" xlink:href=\"#DejaVuSans-100\"/>\r\n      <use x=\"222.416016\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"250.199219\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"305.179688\" xlink:href=\"#DejaVuSans-116\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"patch_87\">\r\n     <path d=\"M 421.926562 38.476562 \r\nL 441.926562 38.476562 \r\nL 441.926562 31.476562 \r\nL 421.926562 31.476562 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.900059;\"/>\r\n    </g>\r\n    <g id=\"text_19\">\r\n     <!-- Actual -->\r\n     <g transform=\"translate(449.926562 38.476562)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 34.1875 63.1875 \r\nL 20.796875 26.90625 \r\nL 47.609375 26.90625 \r\nz\r\nM 28.609375 72.90625 \r\nL 39.796875 72.90625 \r\nL 67.578125 0 \r\nL 57.328125 0 \r\nL 50.6875 18.703125 \r\nL 17.828125 18.703125 \r\nL 11.1875 0 \r\nL 0.78125 0 \r\nz\r\n\" id=\"DejaVuSans-65\"/>\r\n       <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n       <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"66.658203\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"121.638672\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"160.847656\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"224.226562\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"285.505859\" xlink:href=\"#DejaVuSans-108\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p0dd20c7351\">\r\n   <rect height=\"543.6\" width=\"446.4\" x=\"46.965625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "fig,ax_multi = plt.subplots(1,1,figsize = (8,10))\n",
    "sns.histplot(y_pred_main, ax = ax_multi, color = 'red', label = 'Predict')\n",
    "sns.histplot(y_test_reg, ax = ax_multi, color = 'blue', label = 'Actual')\n",
    "ax_multi.legend()"
   ]
  },
  {
   "source": [
    "<h2> Subclassing API </h2> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The sequential and functional APIs are easy to save, clone, its structure can be displayed and analyszed, its framework can infer shapes and check types, errors can be caught early. This is because the model is a static graph of layers. \n",
    "\n",
    "However as it is a static graph of layers, it cannot train models involving loops, varying shapes, conditional branching and other dynamic behaviors. \n",
    "\n",
    "Instead we use the Subclass API"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.models.Model):\n",
    "    def __init__(self, units = 30, activation = \"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        #only instantiates the layers\n",
    "        self.hidden1 = keras.layers.Dense(units, activation = activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation = activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output,aux_output"
   ]
  },
  {
   "source": [
    "Notice that now we have separated layer initialization from layer architecture. Hence we can call this function with the respective inputs whenever required during normal programming operations.\n",
    "\n",
    "However now your model's architecture is hidden within the call() method. Hence we cannot clone or save the model. And we cannot inspect the summary of the model. Hence Sequential API and Functional API are safer options"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving a Keras model\n",
    "model_multi.save(\"multi_output_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['model_weights', 'optimizer_weights']>"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "import h5py\n",
    "model_h5 = h5py.File('multi_output_model.h5','r')\n",
    "model_h5.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<HDF5 group \"/model_weights\" (7 members)>"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "model_h5['model_weights']"
   ]
  },
  {
   "source": [
    "There are 2 ways in which you can load a model. Either by loading the entire model or just the weights. Use load_weights to load only the weights. However here we have to create a model architecture which matches that of the weights."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\ndef create_model():\\n    .....model architecture...\\n\\nmodel = create_model()\\nmodel.load_weights(weights_path/h5 file name)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "\"\"\"\n",
    "def create_model():\n",
    "    .....model architecture...\n",
    "\n",
    "model = create_model()\n",
    "model.load_weights(weights_path/h5 file name)\n",
    "\"\"\""
   ]
  },
  {
   "source": [
    "Training a model can take several hours sometimes. It is important to ensure that you do not loose progress by not saving. Hence we have to save at checkpoints at regular intervals during training. For this we use callbacks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h3> Using Callbacks </h3> \n",
    "\n",
    "The fit() method has a callback argument which accepts a keras.callbacks object which specifies when the model should save. We can specify whether to save after each epoch, each batch or end of training. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3903 - dense_15_loss: 0.3601 - dense_16_loss: 0.6625 - val_loss: 0.4199 - val_dense_15_loss: 0.3893 - val_dense_16_loss: 0.6955\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3868 - dense_15_loss: 0.3568 - dense_16_loss: 0.6570 - val_loss: 0.4083 - val_dense_15_loss: 0.3771 - val_dense_16_loss: 0.6884\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3801 - dense_15_loss: 0.3499 - dense_16_loss: 0.6523 - val_loss: 0.4077 - val_dense_15_loss: 0.3770 - val_dense_16_loss: 0.6840\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3808 - dense_15_loss: 0.3507 - dense_16_loss: 0.6520 - val_loss: 0.4097 - val_dense_15_loss: 0.3791 - val_dense_16_loss: 0.6858\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3818 - dense_15_loss: 0.3522 - dense_16_loss: 0.6481 - val_loss: 0.4007 - val_dense_15_loss: 0.3698 - val_dense_16_loss: 0.6784\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3717 - dense_15_loss: 0.3419 - dense_16_loss: 0.6398 - val_loss: 0.4041 - val_dense_15_loss: 0.3739 - val_dense_16_loss: 0.6755\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3706 - dense_15_loss: 0.3413 - dense_16_loss: 0.6340 - val_loss: 0.3974 - val_dense_15_loss: 0.3671 - val_dense_16_loss: 0.6695\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3703 - dense_15_loss: 0.3414 - dense_16_loss: 0.6308 - val_loss: 0.3952 - val_dense_15_loss: 0.3652 - val_dense_16_loss: 0.6648\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3636 - dense_15_loss: 0.3342 - dense_16_loss: 0.6275 - val_loss: 0.3885 - val_dense_15_loss: 0.3581 - val_dense_16_loss: 0.6623\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3655 - dense_15_loss: 0.3368 - dense_16_loss: 0.6241 - val_loss: 0.3884 - val_dense_15_loss: 0.3584 - val_dense_16_loss: 0.6585\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3845 - dense_15_loss: 0.3579 - dense_16_loss: 0.6234 - val_loss: 0.3940 - val_dense_15_loss: 0.3639 - val_dense_16_loss: 0.6645\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3692 - dense_15_loss: 0.3411 - dense_16_loss: 0.6217 - val_loss: 0.3888 - val_dense_15_loss: 0.3591 - val_dense_16_loss: 0.6555\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3650 - dense_15_loss: 0.3371 - dense_16_loss: 0.6163 - val_loss: 0.3865 - val_dense_15_loss: 0.3568 - val_dense_16_loss: 0.6540\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3709 - dense_15_loss: 0.3439 - dense_16_loss: 0.6141 - val_loss: 0.3916 - val_dense_15_loss: 0.3628 - val_dense_16_loss: 0.6514\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3592 - dense_15_loss: 0.3314 - dense_16_loss: 0.6095 - val_loss: 0.3854 - val_dense_15_loss: 0.3566 - val_dense_16_loss: 0.6454\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3580 - dense_15_loss: 0.3302 - dense_16_loss: 0.6081 - val_loss: 0.4059 - val_dense_15_loss: 0.3790 - val_dense_16_loss: 0.6480\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3662 - dense_15_loss: 0.3394 - dense_16_loss: 0.6067 - val_loss: 0.3814 - val_dense_15_loss: 0.3528 - val_dense_16_loss: 0.6384\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3569 - dense_15_loss: 0.3298 - dense_16_loss: 0.6006 - val_loss: 0.3882 - val_dense_15_loss: 0.3608 - val_dense_16_loss: 0.6352\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3532 - dense_15_loss: 0.3260 - dense_16_loss: 0.5983 - val_loss: 0.3785 - val_dense_15_loss: 0.3504 - val_dense_16_loss: 0.6321\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3507 - dense_15_loss: 0.3235 - dense_16_loss: 0.5950 - val_loss: 0.3979 - val_dense_15_loss: 0.3720 - val_dense_16_loss: 0.6315\n"
     ]
    }
   ],
   "source": [
    "#saves at end of each epoch. \n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('multi_output_model_cb.h5')\n",
    "history = model_multi.fit([X_train_A,X_train_B],[y_train_reg,y_train_reg], epochs = 20, validation_data= ([X_valid_A, X_valid_B],[y_valid_reg, y_valid_reg]), callbacks=checkpoint_cb)"
   ]
  },
  {
   "source": [
    "Saving at checkpoints allows us to only save a model with the best score on the validation set. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        loss  dense_15_loss  dense_16_loss  val_loss  val_dense_15_loss  \\\n",
       "0   0.390345       0.360107       0.662486  0.419916           0.389295   \n",
       "1   0.386844       0.356831       0.656966  0.408251           0.377118   \n",
       "2   0.380135       0.349899       0.652252  0.407732           0.377040   \n",
       "3   0.380813       0.350679       0.652012  0.409727           0.379054   \n",
       "4   0.381829       0.352238       0.648145  0.400668           0.369806   \n",
       "5   0.371714       0.341931       0.639756  0.404069           0.373906   \n",
       "6   0.370607       0.341339       0.634024  0.397363           0.367122   \n",
       "7   0.370325       0.341382       0.630813  0.395186           0.365233   \n",
       "8   0.363559       0.334227       0.627542  0.388540           0.358122   \n",
       "9   0.365547       0.336823       0.624059  0.388394           0.358382   \n",
       "10  0.384484       0.357933       0.623450  0.393999           0.363939   \n",
       "11  0.369157       0.341100       0.621677  0.388755           0.359116   \n",
       "12  0.364990       0.337065       0.616310  0.386482           0.356762   \n",
       "13  0.370903       0.343878       0.614132  0.391647           0.362790   \n",
       "14  0.359204       0.331395       0.609482  0.385442           0.356560   \n",
       "15  0.357950       0.330155       0.608113  0.405888           0.378984   \n",
       "16  0.366163       0.339434       0.606723  0.381392           0.352838   \n",
       "17  0.356913       0.329834       0.600623  0.388210           0.360769   \n",
       "18  0.353193       0.325963       0.598263  0.378545           0.350370   \n",
       "19  0.350686       0.323541       0.594982  0.397925           0.371970   \n",
       "\n",
       "    val_dense_16_loss  \n",
       "0            0.695512  \n",
       "1            0.688448  \n",
       "2            0.683963  \n",
       "3            0.685776  \n",
       "4            0.678427  \n",
       "5            0.675540  \n",
       "6            0.669538  \n",
       "7            0.664765  \n",
       "8            0.662297  \n",
       "9            0.658499  \n",
       "10           0.664537  \n",
       "11           0.655500  \n",
       "12           0.653964  \n",
       "13           0.651357  \n",
       "14           0.645385  \n",
       "15           0.648030  \n",
       "16           0.638380  \n",
       "17           0.635182  \n",
       "18           0.632123  \n",
       "19           0.631516  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n      <th>dense_15_loss</th>\n      <th>dense_16_loss</th>\n      <th>val_loss</th>\n      <th>val_dense_15_loss</th>\n      <th>val_dense_16_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.390345</td>\n      <td>0.360107</td>\n      <td>0.662486</td>\n      <td>0.419916</td>\n      <td>0.389295</td>\n      <td>0.695512</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.386844</td>\n      <td>0.356831</td>\n      <td>0.656966</td>\n      <td>0.408251</td>\n      <td>0.377118</td>\n      <td>0.688448</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.380135</td>\n      <td>0.349899</td>\n      <td>0.652252</td>\n      <td>0.407732</td>\n      <td>0.377040</td>\n      <td>0.683963</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.380813</td>\n      <td>0.350679</td>\n      <td>0.652012</td>\n      <td>0.409727</td>\n      <td>0.379054</td>\n      <td>0.685776</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.381829</td>\n      <td>0.352238</td>\n      <td>0.648145</td>\n      <td>0.400668</td>\n      <td>0.369806</td>\n      <td>0.678427</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.371714</td>\n      <td>0.341931</td>\n      <td>0.639756</td>\n      <td>0.404069</td>\n      <td>0.373906</td>\n      <td>0.675540</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.370607</td>\n      <td>0.341339</td>\n      <td>0.634024</td>\n      <td>0.397363</td>\n      <td>0.367122</td>\n      <td>0.669538</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.370325</td>\n      <td>0.341382</td>\n      <td>0.630813</td>\n      <td>0.395186</td>\n      <td>0.365233</td>\n      <td>0.664765</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.363559</td>\n      <td>0.334227</td>\n      <td>0.627542</td>\n      <td>0.388540</td>\n      <td>0.358122</td>\n      <td>0.662297</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.365547</td>\n      <td>0.336823</td>\n      <td>0.624059</td>\n      <td>0.388394</td>\n      <td>0.358382</td>\n      <td>0.658499</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.384484</td>\n      <td>0.357933</td>\n      <td>0.623450</td>\n      <td>0.393999</td>\n      <td>0.363939</td>\n      <td>0.664537</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.369157</td>\n      <td>0.341100</td>\n      <td>0.621677</td>\n      <td>0.388755</td>\n      <td>0.359116</td>\n      <td>0.655500</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.364990</td>\n      <td>0.337065</td>\n      <td>0.616310</td>\n      <td>0.386482</td>\n      <td>0.356762</td>\n      <td>0.653964</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.370903</td>\n      <td>0.343878</td>\n      <td>0.614132</td>\n      <td>0.391647</td>\n      <td>0.362790</td>\n      <td>0.651357</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.359204</td>\n      <td>0.331395</td>\n      <td>0.609482</td>\n      <td>0.385442</td>\n      <td>0.356560</td>\n      <td>0.645385</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.357950</td>\n      <td>0.330155</td>\n      <td>0.608113</td>\n      <td>0.405888</td>\n      <td>0.378984</td>\n      <td>0.648030</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.366163</td>\n      <td>0.339434</td>\n      <td>0.606723</td>\n      <td>0.381392</td>\n      <td>0.352838</td>\n      <td>0.638380</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.356913</td>\n      <td>0.329834</td>\n      <td>0.600623</td>\n      <td>0.388210</td>\n      <td>0.360769</td>\n      <td>0.635182</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.353193</td>\n      <td>0.325963</td>\n      <td>0.598263</td>\n      <td>0.378545</td>\n      <td>0.350370</td>\n      <td>0.632123</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.350686</td>\n      <td>0.323541</td>\n      <td>0.594982</td>\n      <td>0.397925</td>\n      <td>0.371970</td>\n      <td>0.631516</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "#the two outputs have different weights for their losses. \n",
    "trainingdf = pd.DataFrame(data = history.history)\n",
    "trainingdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 30)           210         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 30)           930         dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 35)           0           input_5[0][0]                    \n",
      "                                                                 dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1)            36          concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 1)            31          dense_13[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,207\n",
      "Trainable params: 1,207\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_multi.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        loss  dense_15_loss  dense_16_loss  val_loss  val_dense_15_loss  \\\n",
       "18  0.353193       0.325963       0.598263  0.378545            0.35037   \n",
       "\n",
       "    val_dense_16_loss  \n",
       "18           0.632123  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n      <th>dense_15_loss</th>\n      <th>dense_16_loss</th>\n      <th>val_loss</th>\n      <th>val_dense_15_loss</th>\n      <th>val_dense_16_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>18</th>\n      <td>0.353193</td>\n      <td>0.325963</td>\n      <td>0.598263</td>\n      <td>0.378545</td>\n      <td>0.35037</td>\n      <td>0.632123</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "trainingdf.loc[trainingdf[\"val_loss\"] == trainingdf[\"val_loss\"].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3531 - dense_15_loss: 0.3267 - dense_16_loss: 0.5910 - val_loss: 0.3821 - val_dense_15_loss: 0.3550 - val_dense_16_loss: 0.6261\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3493 - dense_15_loss: 0.3228 - dense_16_loss: 0.5884 - val_loss: 0.3801 - val_dense_15_loss: 0.3531 - val_dense_16_loss: 0.6231\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3489 - dense_15_loss: 0.3225 - dense_16_loss: 0.5868 - val_loss: 0.3786 - val_dense_15_loss: 0.3517 - val_dense_16_loss: 0.6199\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3505 - dense_15_loss: 0.3245 - dense_16_loss: 0.5836 - val_loss: 0.3786 - val_dense_15_loss: 0.3519 - val_dense_16_loss: 0.6190\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3487 - dense_15_loss: 0.3227 - dense_16_loss: 0.5824 - val_loss: 0.3733 - val_dense_15_loss: 0.3465 - val_dense_16_loss: 0.6143\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3513 - dense_15_loss: 0.3254 - dense_16_loss: 0.5847 - val_loss: 0.3737 - val_dense_15_loss: 0.3473 - val_dense_16_loss: 0.6112\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3983 - dense_15_loss: 0.3779 - dense_16_loss: 0.5818 - val_loss: 0.3846 - val_dense_15_loss: 0.3591 - val_dense_16_loss: 0.6140\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3478 - dense_15_loss: 0.3224 - dense_16_loss: 0.5762 - val_loss: 0.3818 - val_dense_15_loss: 0.3564 - val_dense_16_loss: 0.6098\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3474 - dense_15_loss: 0.3225 - dense_16_loss: 0.5714 - val_loss: 0.3715 - val_dense_15_loss: 0.3454 - val_dense_16_loss: 0.6065\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3442 - dense_15_loss: 0.3191 - dense_16_loss: 0.5697 - val_loss: 0.3684 - val_dense_15_loss: 0.3425 - val_dense_16_loss: 0.6020\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3424 - dense_15_loss: 0.3175 - dense_16_loss: 0.5666 - val_loss: 0.4518 - val_dense_15_loss: 0.4322 - val_dense_16_loss: 0.6285\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3527 - dense_15_loss: 0.3277 - dense_16_loss: 0.5774 - val_loss: 0.3734 - val_dense_15_loss: 0.3483 - val_dense_16_loss: 0.5996\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3513 - dense_15_loss: 0.3277 - dense_16_loss: 0.5635 - val_loss: 0.3679 - val_dense_15_loss: 0.3424 - val_dense_16_loss: 0.5977\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3441 - dense_15_loss: 0.3201 - dense_16_loss: 0.5608 - val_loss: 0.3696 - val_dense_15_loss: 0.3445 - val_dense_16_loss: 0.5956\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3385 - dense_15_loss: 0.3141 - dense_16_loss: 0.5577 - val_loss: 0.3648 - val_dense_15_loss: 0.3397 - val_dense_16_loss: 0.5911\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3398 - dense_15_loss: 0.3160 - dense_16_loss: 0.5545 - val_loss: 0.3732 - val_dense_15_loss: 0.3493 - val_dense_16_loss: 0.5887\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3375 - dense_15_loss: 0.3135 - dense_16_loss: 0.5534 - val_loss: 0.3717 - val_dense_15_loss: 0.3481 - val_dense_16_loss: 0.5848\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3377 - dense_15_loss: 0.3141 - dense_16_loss: 0.5496 - val_loss: 0.3632 - val_dense_15_loss: 0.3387 - val_dense_16_loss: 0.5831\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3395 - dense_15_loss: 0.3163 - dense_16_loss: 0.5484 - val_loss: 0.3611 - val_dense_15_loss: 0.3368 - val_dense_16_loss: 0.5801\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3384 - dense_15_loss: 0.3155 - dense_16_loss: 0.5451 - val_loss: 0.3945 - val_dense_15_loss: 0.3741 - val_dense_16_loss: 0.5788\n"
     ]
    }
   ],
   "source": [
    "#saving the one with best score on the validation set\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('multi_output_model_cb.h5', save_best_only=True)\n",
    "history = model_multi.fit([X_train_A,X_train_B],[y_train_reg,y_train_reg], epochs = 20, validation_data= ([X_valid_A, X_valid_B],[y_valid_reg, y_valid_reg]), callbacks=checkpoint_cb)"
   ]
  },
  {
   "source": [
    "Another way to implement saving the iteration with the best score on the validation score is to use early stopping\n",
    "\n",
    "Using the EarlyStopping callback\n",
    "It will interrupt training when it measures no progress on the validation set for a number of epochs and also roll back to the best model.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3372 - dense_15_loss: 0.3142 - dense_16_loss: 0.5443 - val_loss: 0.3692 - val_dense_15_loss: 0.3464 - val_dense_16_loss: 0.5749\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3360 - dense_15_loss: 0.3130 - dense_16_loss: 0.5424 - val_loss: 0.3677 - val_dense_15_loss: 0.3447 - val_dense_16_loss: 0.5743\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3327 - dense_15_loss: 0.3098 - dense_16_loss: 0.5383 - val_loss: 0.3715 - val_dense_15_loss: 0.3491 - val_dense_16_loss: 0.5726\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3328 - dense_15_loss: 0.3102 - dense_16_loss: 0.5356 - val_loss: 0.3822 - val_dense_15_loss: 0.3612 - val_dense_16_loss: 0.5717\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3373 - dense_15_loss: 0.3152 - dense_16_loss: 0.5357 - val_loss: 0.3681 - val_dense_15_loss: 0.3458 - val_dense_16_loss: 0.5694\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3320 - dense_15_loss: 0.3097 - dense_16_loss: 0.5327 - val_loss: 0.3589 - val_dense_15_loss: 0.3360 - val_dense_16_loss: 0.5656\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3296 - dense_15_loss: 0.3073 - dense_16_loss: 0.5304 - val_loss: 0.3748 - val_dense_15_loss: 0.3535 - val_dense_16_loss: 0.5668\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3320 - dense_15_loss: 0.3101 - dense_16_loss: 0.5298 - val_loss: 0.3570 - val_dense_15_loss: 0.3343 - val_dense_16_loss: 0.5615\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3295 - dense_15_loss: 0.3074 - dense_16_loss: 0.5278 - val_loss: 0.3618 - val_dense_15_loss: 0.3395 - val_dense_16_loss: 0.5625\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3317 - dense_15_loss: 0.3101 - dense_16_loss: 0.5268 - val_loss: 0.3580 - val_dense_15_loss: 0.3356 - val_dense_16_loss: 0.5594\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3295 - dense_15_loss: 0.3075 - dense_16_loss: 0.5274 - val_loss: 0.3540 - val_dense_15_loss: 0.3315 - val_dense_16_loss: 0.5571\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3280 - dense_15_loss: 0.3064 - dense_16_loss: 0.5218 - val_loss: 0.3565 - val_dense_15_loss: 0.3343 - val_dense_16_loss: 0.5557\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3281 - dense_15_loss: 0.3067 - dense_16_loss: 0.5207 - val_loss: 0.3618 - val_dense_15_loss: 0.3405 - val_dense_16_loss: 0.5535\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3291 - dense_15_loss: 0.3080 - dense_16_loss: 0.5194 - val_loss: 0.3958 - val_dense_15_loss: 0.3786 - val_dense_16_loss: 0.5512\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3459 - dense_15_loss: 0.3266 - dense_16_loss: 0.5192 - val_loss: 0.3547 - val_dense_15_loss: 0.3328 - val_dense_16_loss: 0.5521\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3305 - dense_15_loss: 0.3094 - dense_16_loss: 0.5208 - val_loss: 0.3824 - val_dense_15_loss: 0.3638 - val_dense_16_loss: 0.5499\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3266 - dense_15_loss: 0.3056 - dense_16_loss: 0.5156 - val_loss: 0.3643 - val_dense_15_loss: 0.3440 - val_dense_16_loss: 0.5475\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3256 - dense_15_loss: 0.3046 - dense_16_loss: 0.5147 - val_loss: 0.3558 - val_dense_15_loss: 0.3346 - val_dense_16_loss: 0.5466\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3300 - dense_15_loss: 0.3098 - dense_16_loss: 0.5114 - val_loss: 0.3596 - val_dense_15_loss: 0.3386 - val_dense_16_loss: 0.5493\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3242 - dense_15_loss: 0.3033 - dense_16_loss: 0.5114 - val_loss: 0.3531 - val_dense_15_loss: 0.3318 - val_dense_16_loss: 0.5452\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience = 10, restore_best_weights=True)\n",
    "\n",
    "#while checkpoint_cb saves the model(at end of each epoch by default) and saves only the best validation score if save_best_only = True\n",
    "#early_stoppin checks if the validation score improves over 10 iterations after each previous \n",
    "history = model_multi.fit([X_train_A,X_train_B],[y_train_reg,y_train_reg], epochs = 20, validation_data= ([X_valid_A, X_valid_B],[y_valid_reg, y_valid_reg]), callbacks=[checkpoint_cb,early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can also write our own call backs\n",
    "class PrintValTrainRatio(keras.callbacks.Callback):\n",
    "    #on_epoch_end is a function belonging to Callback which has been inherited\n",
    "    def on_epoch_end(self,epoch,logs):\n",
    "        print(\"\\n Val/Train : {}\".format(logs[\"val_loss\"]/logs[\"loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3233 - dense_15_loss: 0.3023 - dense_16_loss: 0.5126 - val_loss: 0.3544 - val_dense_15_loss: 0.3336 - val_dense_16_loss: 0.5418\n",
      "\n",
      " Val/Train : 1.0959780625784636\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3250 - dense_15_loss: 0.3046 - dense_16_loss: 0.5087 - val_loss: 0.3544 - val_dense_15_loss: 0.3337 - val_dense_16_loss: 0.5408\n",
      "\n",
      " Val/Train : 1.0903247516854215\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3227 - dense_15_loss: 0.3022 - dense_16_loss: 0.5072 - val_loss: 0.3509 - val_dense_15_loss: 0.3301 - val_dense_16_loss: 0.5376\n",
      "\n",
      " Val/Train : 1.087215565430975\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3233 - dense_15_loss: 0.3031 - dense_16_loss: 0.5054 - val_loss: 0.3494 - val_dense_15_loss: 0.3286 - val_dense_16_loss: 0.5368\n",
      "\n",
      " Val/Train : 1.0808478276800888\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3259 - dense_15_loss: 0.3061 - dense_16_loss: 0.5040 - val_loss: 0.3557 - val_dense_15_loss: 0.3355 - val_dense_16_loss: 0.5382\n",
      "\n",
      " Val/Train : 1.0914695280330904\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3420 - dense_15_loss: 0.3233 - dense_16_loss: 0.5105 - val_loss: 0.3546 - val_dense_15_loss: 0.3331 - val_dense_16_loss: 0.5483\n",
      "\n",
      " Val/Train : 1.0366878101345225\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3277 - dense_15_loss: 0.3070 - dense_16_loss: 0.5140 - val_loss: 0.3505 - val_dense_15_loss: 0.3299 - val_dense_16_loss: 0.5360\n",
      "\n",
      " Val/Train : 1.0695707881195726\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3249 - dense_15_loss: 0.3048 - dense_16_loss: 0.5067 - val_loss: 0.3559 - val_dense_15_loss: 0.3362 - val_dense_16_loss: 0.5329\n",
      "\n",
      " Val/Train : 1.095297137649322\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3229 - dense_15_loss: 0.3030 - dense_16_loss: 0.5018 - val_loss: 0.3495 - val_dense_15_loss: 0.3290 - val_dense_16_loss: 0.5344\n",
      "\n",
      " Val/Train : 1.0825417010051959\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3191 - dense_15_loss: 0.2992 - dense_16_loss: 0.4979 - val_loss: 0.3614 - val_dense_15_loss: 0.3423 - val_dense_16_loss: 0.5331\n",
      "\n",
      " Val/Train : 1.1326333656988277\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3203 - dense_15_loss: 0.3006 - dense_16_loss: 0.4972 - val_loss: 0.3546 - val_dense_15_loss: 0.3352 - val_dense_16_loss: 0.5289\n",
      "\n",
      " Val/Train : 1.1071002432438444\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3200 - dense_15_loss: 0.3006 - dense_16_loss: 0.4946 - val_loss: 0.3484 - val_dense_15_loss: 0.3283 - val_dense_16_loss: 0.5291\n",
      "\n",
      " Val/Train : 1.088903584982724\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3180 - dense_15_loss: 0.2984 - dense_16_loss: 0.4941 - val_loss: 0.3532 - val_dense_15_loss: 0.3339 - val_dense_16_loss: 0.5266\n",
      "\n",
      " Val/Train : 1.1107257929754448\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3186 - dense_15_loss: 0.2992 - dense_16_loss: 0.4939 - val_loss: 0.3545 - val_dense_15_loss: 0.3354 - val_dense_16_loss: 0.5265\n",
      "\n",
      " Val/Train : 1.1124926542710638\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3178 - dense_15_loss: 0.2984 - dense_16_loss: 0.4931 - val_loss: 0.3472 - val_dense_15_loss: 0.3275 - val_dense_16_loss: 0.5244\n",
      "\n",
      " Val/Train : 1.092228508623348\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3175 - dense_15_loss: 0.2983 - dense_16_loss: 0.4905 - val_loss: 0.3906 - val_dense_15_loss: 0.3757 - val_dense_16_loss: 0.5250\n",
      "\n",
      " Val/Train : 1.2301943910090358\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3186 - dense_15_loss: 0.2995 - dense_16_loss: 0.4901 - val_loss: 0.3431 - val_dense_15_loss: 0.3232 - val_dense_16_loss: 0.5222\n",
      "\n",
      " Val/Train : 1.0770236022539204\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3160 - dense_15_loss: 0.2967 - dense_16_loss: 0.4890 - val_loss: 0.3435 - val_dense_15_loss: 0.3237 - val_dense_16_loss: 0.5212\n",
      "\n",
      " Val/Train : 1.0870422116738234\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3160 - dense_15_loss: 0.2968 - dense_16_loss: 0.4892 - val_loss: 0.3436 - val_dense_15_loss: 0.3240 - val_dense_16_loss: 0.5204\n",
      "\n",
      " Val/Train : 1.0872304633649519\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3152 - dense_15_loss: 0.2960 - dense_16_loss: 0.4877 - val_loss: 0.3399 - val_dense_15_loss: 0.3200 - val_dense_16_loss: 0.5194\n",
      "\n",
      " Val/Train : 1.0786709161872747\n"
     ]
    }
   ],
   "source": [
    "printvaltrain = PrintValTrainRatio()\n",
    "history = model_multi.fit([X_train_A,X_train_B],[y_train_reg,y_train_reg], epochs = 20, validation_data= ([X_valid_A, X_valid_B],[y_valid_reg, y_valid_reg]), callbacks=[checkpoint_cb,early_stopping_cb,printvaltrain])"
   ]
  },
  {
   "source": [
    "<h3> Tensorboard </h3> \n",
    "\n",
    "To use Tensorboard, we must modify the program such that it outputs the data we want to visualize to special binary log files called **event files**. Each binary record is called a summary. \n",
    "\n",
    "The Tensorboard server will monitor the log directory, and it will automatically pick up the changes and update the visualizations. \n",
    "\n",
    "Usually we want the TensorBoard server to point to a root log directory and configure the program to write to a different sub directory everytime it runs. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_logdir = os.path.join(os.curdir,\"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    #time.strftime is used when we are getting the data from a function like time.localtime()\n",
    "    #time.strptime is used to convert an existing string of time to a format we want\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\", time.localtime())\n",
    "    return os.path.join(root_logdir,run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "time.struct_time(tm_year=2021, tm_mon=4, tm_mday=6, tm_hour=12, tm_min=31, tm_sec=46, tm_wday=1, tm_yday=96, tm_isdst=0)"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "time.localtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    " run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\", time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'run_2021_04_06-12_31_47'"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.3166 - dense_15_loss: 0.2976 - dense_16_loss: 0.4873 - val_loss: 0.3423 - val_dense_15_loss: 0.3226 - val_dense_16_loss: 0.5189\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3146 - dense_15_loss: 0.2957 - dense_16_loss: 0.4848 - val_loss: 0.3446 - val_dense_15_loss: 0.3253 - val_dense_16_loss: 0.5188\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3153 - dense_15_loss: 0.2963 - dense_16_loss: 0.4867 - val_loss: 0.3410 - val_dense_15_loss: 0.3212 - val_dense_16_loss: 0.5197\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3152 - dense_15_loss: 0.2962 - dense_16_loss: 0.4859 - val_loss: 0.3637 - val_dense_15_loss: 0.3464 - val_dense_16_loss: 0.5194\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3136 - dense_15_loss: 0.2947 - dense_16_loss: 0.4834 - val_loss: 0.3418 - val_dense_15_loss: 0.3224 - val_dense_16_loss: 0.5156\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3149 - dense_15_loss: 0.2962 - dense_16_loss: 0.4826 - val_loss: 0.3577 - val_dense_15_loss: 0.3401 - val_dense_16_loss: 0.5161\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3131 - dense_15_loss: 0.2943 - dense_16_loss: 0.4827 - val_loss: 0.3429 - val_dense_15_loss: 0.3239 - val_dense_16_loss: 0.5140\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3145 - dense_15_loss: 0.2960 - dense_16_loss: 0.4815 - val_loss: 0.3420 - val_dense_15_loss: 0.3231 - val_dense_16_loss: 0.5120\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3105 - dense_15_loss: 0.2917 - dense_16_loss: 0.4796 - val_loss: 0.3421 - val_dense_15_loss: 0.3233 - val_dense_16_loss: 0.5115\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3119 - dense_15_loss: 0.2933 - dense_16_loss: 0.4791 - val_loss: 0.3398 - val_dense_15_loss: 0.3207 - val_dense_16_loss: 0.5123\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 2s 4ms/step - loss: 0.3111 - dense_15_loss: 0.2926 - dense_16_loss: 0.4775 - val_loss: 0.3395 - val_dense_15_loss: 0.3204 - val_dense_16_loss: 0.5107\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3100 - dense_15_loss: 0.2914 - dense_16_loss: 0.4773 - val_loss: 0.3618 - val_dense_15_loss: 0.3449 - val_dense_16_loss: 0.5139\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3103 - dense_15_loss: 0.2919 - dense_16_loss: 0.4760 - val_loss: 0.3424 - val_dense_15_loss: 0.3238 - val_dense_16_loss: 0.5099\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3109 - dense_15_loss: 0.2924 - dense_16_loss: 0.4774 - val_loss: 0.3378 - val_dense_15_loss: 0.3188 - val_dense_16_loss: 0.5092\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3150 - dense_15_loss: 0.2969 - dense_16_loss: 0.4784 - val_loss: 0.3404 - val_dense_15_loss: 0.3217 - val_dense_16_loss: 0.5082\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3106 - dense_15_loss: 0.2922 - dense_16_loss: 0.4764 - val_loss: 0.3404 - val_dense_15_loss: 0.3220 - val_dense_16_loss: 0.5060\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3113 - dense_15_loss: 0.2933 - dense_16_loss: 0.4738 - val_loss: 0.3387 - val_dense_15_loss: 0.3200 - val_dense_16_loss: 0.5071\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3102 - dense_15_loss: 0.2920 - dense_16_loss: 0.4742 - val_loss: 0.3370 - val_dense_15_loss: 0.3182 - val_dense_16_loss: 0.5059\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3087 - dense_15_loss: 0.2905 - dense_16_loss: 0.4725 - val_loss: 0.3352 - val_dense_15_loss: 0.3164 - val_dense_16_loss: 0.5039\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3086 - dense_15_loss: 0.2904 - dense_16_loss: 0.4728 - val_loss: 0.3379 - val_dense_15_loss: 0.3192 - val_dense_16_loss: 0.5061\n"
     ]
    }
   ],
   "source": [
    "run_logdir = get_run_logdir()\n",
    "#this allows to create many subdirectories at the end of each epoch under the main directory \"my_logs\"\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model_multi.fit([X_train_A,X_train_B],[y_train_reg,y_train_reg], epochs = 20, validation_data= ([X_valid_A, X_valid_B],[y_valid_reg, y_valid_reg]), callbacks=[tensorboard_cb])"
   ]
  },
  {
   "source": [
    "<h2> Hyperparameter Tuning of Neural Networks</h2> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "One option is to use GridSearchCV or RandomizedSearchCV"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wrap our Keras models in objects that mimic regular Scikit-Learn regressors. Create a function that will build and compile a Keras model, given a set of hyperparameters\n",
    "def build_model(n_hidden = 1, n_neurons = 30, learning_rate = 3e-3, input_shape = [8]):\n",
    "    model = keras.models.Sequential()\n",
    "    options = {\"input_shape\":input_shape}\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation = \"relu\",**options))\n",
    "        options = {}\n",
    "    model.add(keras.layers.Dense(1 , **options))\n",
    "    #we add the optimizer like this so that we can specify the learning rate\n",
    "    optimizer = keras.optimizers.SGD(learning_rate)\n",
    "    model.compile(loss=\"mse\",optimizer = optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Functional API method with ability to specify varying neuron numbers\n",
    "def build_model_2(n_hidden = 1, n_neurons = 30, learning_rate = 3e-3, input_shape = [8]):\n",
    "    input_layer = keras.layers.Input(shape = input_shape)\n",
    "    hidden_layers = list()\n",
    "    type_of_n_neuron_entry = type(n_neurons)\n",
    "    prev_layer = input_layer\n",
    "    if type_of_n_neuron_entry == int:\n",
    "        for layer in range(n_hidden):\n",
    "            hidden_layers.append(keras.layers.Dense(n_neurons, activation = \"relu\")(prev_layer))\n",
    "            prev_layer = hidden_layers[-1]\n",
    "    elif type_of_n_neuron_entry == list:\n",
    "        for i in range(n_hidden):\n",
    "            hidden_layers.append(keras.layers.Dense(n_neurons[i],activation = \"relu\")(prev_layer))\n",
    "            prev_layer = hidden_layers[-1]\n",
    "    else:\n",
    "        raise TypeError('n_neuron should be int or list type')\n",
    "    output = keras.layers.Dense(1)(hidden_layers[-1])\n",
    "    model = keras.models.Model(inputs = input_layer, outputs = [output])\n",
    "    #we add the optimizer like this so that we can specify the learning rate\n",
    "    optimizer = keras.optimizers.SGD(learning_rate)\n",
    "    model.compile(loss=\"mse\",optimizer = optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a KerasRegressor based on the build_model function\n",
    "\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "source": [
    "KerasRegressor object is a thin wrapper around the Keras model built using build_model(). Since we did not specify any hyperparameter when creating it, it will just use the default hyperparameters we defined in build_model(). \n",
    "\n",
    "As we have specified build_model with certain parameters, now we can use **keras_reg** like a regular sklearn regressor.\n",
    "\n",
    "Any parameter we pass to the fit() method of keras_reg, will be passed to the underlying Keras model. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "param_distribs = {\n",
    "    \"n_hidden\":[0,1,2,3],\n",
    "    \"n_neurons\":np.arange(1,100),\n",
    "    \"learning_rate\":reciprocal(3e-4,3e-2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=] - 1s 3ms/step - loss: 1.1462 - val_loss: 1.0543\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.9520 - val_loss: 0.9163\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.8614 - val_loss: 0.8287\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.7737 - val_loss: 0.7768\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7241 - val_loss: 0.7434\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6784 - val_loss: 0.7217\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6821 - val_loss: 0.7045\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.7601\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.6720 - val_loss: 0.6001\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5503 - val_loss: 0.5144\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4633 - val_loss: 0.5049\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4249 - val_loss: 0.4351\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3998 - val_loss: 0.4196\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3819 - val_loss: 0.4102\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3591 - val_loss: 0.3923\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3593 - val_loss: 0.3843\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3387 - val_loss: 0.3876\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3363 - val_loss: 0.3795\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.3640\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 2s 4ms/step - loss: 1.3878 - val_loss: 0.5315\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5229 - val_loss: 0.4945\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4769 - val_loss: 0.4622\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4514 - val_loss: 0.4533\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4293 - val_loss: 0.4377\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4099 - val_loss: 0.4301\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3943 - val_loss: 0.4206\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3786 - val_loss: 0.4124\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3886 - val_loss: 0.4245\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4008 - val_loss: 0.4085\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3650\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 2s 3ms/step - loss: 1.7330 - val_loss: 0.5809\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5092 - val_loss: 0.5042\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4615 - val_loss: 0.4695\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4171 - val_loss: 0.4611\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4227 - val_loss: 0.4418\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4050 - val_loss: 0.4311\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3695 - val_loss: 0.4254\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3653 - val_loss: 0.4146\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3753 - val_loss: 0.4077\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3621 - val_loss: 0.4032\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.4004\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.7353 - val_loss: 0.6181\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5491 - val_loss: 0.5368\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4781 - val_loss: 0.4815\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4275 - val_loss: 0.4472\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4102 - val_loss: 0.4460\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3921 - val_loss: 0.4184\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3913 - val_loss: 0.4177\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.3902 - val_loss: 0.4082\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3666 - val_loss: 0.3977\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.3716 - val_loss: 0.3948\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.3787\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.8193 - val_loss: 0.6033\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6077 - val_loss: 0.5395\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4929 - val_loss: 0.5004\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4638 - val_loss: 0.4786\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.4325 - val_loss: 0.4580\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4440 - val_loss: 0.4503\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4297 - val_loss: 0.4413\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4234 - val_loss: 0.4415\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4109 - val_loss: 0.4323\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3726 - val_loss: 0.4237\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.3873\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.9996 - val_loss: 0.6535\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.5801 - val_loss: 0.5717\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.4890 - val_loss: 0.5221\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4607 - val_loss: 0.4931\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4450 - val_loss: 0.4748\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.4304 - val_loss: 0.4623\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.4234 - val_loss: 0.4499\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.3977 - val_loss: 0.4378\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.3950 - val_loss: 0.4313\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.3810 - val_loss: 0.4242\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.4186\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 3.6039 - val_loss: 1.1370\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0061 - val_loss: 0.7637\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.7124 - val_loss: 0.6987\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6435 - val_loss: 0.6710\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6185 - val_loss: 0.6512\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6035 - val_loss: 0.6353\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.6090 - val_loss: 0.6209\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5636 - val_loss: 0.6081\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5490 - val_loss: 0.5966\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5553 - val_loss: 0.5862\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.5493\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 3.3034 - val_loss: 1.0091\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8993 - val_loss: 0.7226\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6654 - val_loss: 0.6780\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6348 - val_loss: 0.6569\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5887 - val_loss: 0.6384\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5846 - val_loss: 0.6220\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5781 - val_loss: 0.6078\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5742 - val_loss: 0.5952\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5419 - val_loss: 0.5838\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5611 - val_loss: 0.5736\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.5284\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 2s 5ms/step - loss: 3.5347 - val_loss: 1.4015\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.1637 - val_loss: 0.9051\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7964 - val_loss: 0.7895\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7037 - val_loss: 0.7463\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6853 - val_loss: 0.7189\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6507 - val_loss: 0.6968\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6225 - val_loss: 0.6772\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6069 - val_loss: 0.6601\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5795 - val_loss: 0.6437\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5461 - val_loss: 0.6293\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.6257\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 5.8221 - val_loss: 3.9425\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 3.4745 - val_loss: 2.6912\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 2.4567 - val_loss: 1.9302\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.6823 - val_loss: 1.4645\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.3920 - val_loss: 1.1759\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.0987 - val_loss: 0.9958\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.9549 - val_loss: 0.8808\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7855 - val_loss: 0.8068\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7126 - val_loss: 0.7583\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7034 - val_loss: 0.7257\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.6855\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 2s 7ms/step - loss: 4.9725 - val_loss: 3.6608\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 3.1489 - val_loss: 2.4767\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 2.1549 - val_loss: 1.7920\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.6424 - val_loss: 1.3922\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.3194 - val_loss: 1.1546\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.0783 - val_loss: 1.0105\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.9055 - val_loss: 0.9214\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.8720 - val_loss: 0.8650\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.8166 - val_loss: 0.8278\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7803 - val_loss: 0.8021\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.7362\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 7.4749 - val_loss: 4.7578\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 4.0489 - val_loss: 2.8346\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 2.4397 - val_loss: 1.8461\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.6555 - val_loss: 1.3128\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.1748 - val_loss: 1.0168\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.9108 - val_loss: 0.8486\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7454 - val_loss: 0.7515\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7265 - val_loss: 0.6947\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6504 - val_loss: 0.6609\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6010 - val_loss: 0.6402\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.7057\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 3.8133 - val_loss: 2.3035\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.9382 - val_loss: 1.4247\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.2446 - val_loss: 1.0185\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 0.8858 - val_loss: 0.8417\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.7587 - val_loss: 0.7606\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7045 - val_loss: 0.7200\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6454 - val_loss: 0.6970\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6503 - val_loss: 0.6821\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.6469 - val_loss: 0.6710\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6152 - val_loss: 0.6619\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.6132\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 3.6581 - val_loss: 2.2374\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.8709 - val_loss: 1.4055\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 1.2612 - val_loss: 1.0535\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.9812 - val_loss: 0.9003\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.8283 - val_loss: 0.8286\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.7701 - val_loss: 0.7898\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.7615 - val_loss: 0.7651\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.7314 - val_loss: 0.7474\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6952 - val_loss: 0.7330\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6870 - val_loss: 0.7207\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.6645\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 3.9946 - val_loss: 2.2653\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.8397 - val_loss: 1.3125\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 1.1326 - val_loss: 0.9888\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.9013 - val_loss: 0.8726\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.8137 - val_loss: 0.8212\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.7390 - val_loss: 0.7921\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.7257 - val_loss: 0.7723\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.7131 - val_loss: 0.7571\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6893 - val_loss: 0.7442\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6900 - val_loss: 0.7328\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.8095\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 2s 5ms/step - loss: 3.4565 - val_loss: 1.1620\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.9982 - val_loss: 0.8102\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.7214 - val_loss: 0.7172\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6477 - val_loss: 0.6822\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6400 - val_loss: 0.6587\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5866 - val_loss: 0.6400\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.6054 - val_loss: 0.6227\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.5813 - val_loss: 0.6059\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.5551 - val_loss: 0.5917\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.5513 - val_loss: 0.5783\n",
      "121/121 [==============================] - 0s 1ms/step - loss: 0.5458\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 2s 4ms/step - loss: 3.8436 - val_loss: 1.4347\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.3200 - val_loss: 0.9324\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.8410 - val_loss: 0.7871\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7375 - val_loss: 0.7245\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 2s 6ms/step - loss: 0.6887 - val_loss: 0.6918\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6551 - val_loss: 0.6685\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6465 - val_loss: 0.6500\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 1s 6ms/step - loss: 0.6030 - val_loss: 0.6342\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 1s 5ms/step - loss: 0.6092 - val_loss: 0.6207\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.5807 - val_loss: 0.6080\n",
      "121/121 [==============================] - 0s 2ms/step - loss: 0.5663\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 2s 4ms/step - loss: 3.4244 - val_loss: 1.1849\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 1.1458 - val_loss: 0.8964\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.8544 - val_loss: 0.8119\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.7553 - val_loss: 0.7733\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.7084 - val_loss: 0.7437\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 1s 4ms/step - loss: 0.6940 - val_loss: 0.7205\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6535 - val_loss: 0.6998\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6606 - val_loss: 0.6811\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6445 - val_loss: 0.6635\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 1s 3ms/step - loss: 0.6304 - val_loss: 0.6472\n",
      "121/121 [==============================] - 0s 3ms/step - loss: 0.7016\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x000001D006A013D0>, as the constructor either does not set or modifies parameter learning_rate",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-a67985e9e532>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mrnd_search_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_distribs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#All the parameters in the fit method get relayed to the underlying Keras Mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m rnd_search_cv.fit(x_train_reg,y_train_reg, epochs = 10, validation_data = (x_valid_reg,y_valid_reg),\n\u001b[0m\u001b[0;32m      4\u001b[0m callbacks = [keras.callbacks.EarlyStopping(patience = 10)])\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    874\u001b[0m             \u001b[1;31m# we clone again after setting params in case some\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m             \u001b[1;31m# of the params are estimators as well.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 876\u001b[1;33m             self.best_estimator_ = clone(clone(base_estimator).set_params(\n\u001b[0m\u001b[0;32m    877\u001b[0m                 **self.best_params_))\n\u001b[0;32m    878\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mparam2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparam1\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mparam2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             raise RuntimeError('Cannot clone object %s, as the constructor '\n\u001b[0m\u001b[0;32m     86\u001b[0m                                \u001b[1;34m'either does not set or modifies parameter %s'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                                (estimator, name))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x000001D006A013D0>, as the constructor either does not set or modifies parameter learning_rate"
     ]
    }
   ],
   "source": [
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter = 10, cv = 3)\n",
    "#All the parameters in the fit method get relayed to the underlying Keras Mode\n",
    "rnd_search_cv.fit(x_train_reg,y_train_reg, epochs = 10, validation_data = (x_valid_reg,y_valid_reg),\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience = 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'learning_rate': 0.011133075366404921, 'n_hidden': 3, 'n_neurons': 33}"
      ]
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-0.37642330924669903"
      ]
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "source": [
    "The unified scoring API always maximizes the score, so scores which need to be minimized are negated in order for the unified scoring API to work correctly. While scores which need to be maximised are kept positive. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = build_model(n_hidden = 3, n_neurons = 33, learning_rate = 0.011133075366404921, input_shape = [8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 2s 3ms/step - loss: 1.2034 - val_loss: 0.5348\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.4747 - val_loss: 0.4675\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.4436 - val_loss: 0.4368\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 2s 6ms/step - loss: 0.4038 - val_loss: 0.4253\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3917 - val_loss: 0.4050\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3570 - val_loss: 0.3923\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3774 - val_loss: 0.3870\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3507 - val_loss: 0.3834\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 2s 5ms/step - loss: 0.3479 - val_loss: 0.3758\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 1s 4ms/step - loss: 0.3567 - val_loss: 0.3744\n"
     ]
    }
   ],
   "source": [
    "history_best_model = best_model.fit(x_train_reg,y_train_reg, epochs = 10, validation_data = (x_valid_reg,y_valid_reg),\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience = 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1d00419b190>"
      ]
     },
     "metadata": {},
     "execution_count": 120
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 576x720 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"574.678125pt\" version=\"1.1\" viewBox=\"0 0 500.565625 574.678125\" width=\"500.565625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-04-07T09:04:48.693309</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 574.678125 \r\nL 500.565625 574.678125 \r\nL 500.565625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 46.965625 550.8 \r\nL 493.365625 550.8 \r\nL 493.365625 7.2 \r\nL 46.965625 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 67.256534 550.8 \r\nL 80.372275 550.8 \r\nL 80.372275 544.308285 \r\nL 67.256534 544.308285 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 80.372275 550.8 \r\nL 93.488016 550.8 \r\nL 93.488016 484.259919 \r\nL 80.372275 484.259919 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 93.488016 550.8 \r\nL 106.603756 550.8 \r\nL 106.603756 265.164532 \r\nL 93.488016 265.164532 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 106.603756 550.8 \r\nL 119.719497 550.8 \r\nL 119.719497 180.772235 \r\nL 106.603756 180.772235 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_7\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 119.719497 550.8 \r\nL 132.835238 550.8 \r\nL 132.835238 114.232154 \r\nL 119.719497 114.232154 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_8\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 132.835238 550.8 \r\nL 145.950978 550.8 \r\nL 145.950978 158.051232 \r\nL 132.835238 158.051232 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_9\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 145.950978 550.8 \r\nL 159.066719 550.8 \r\nL 159.066719 107.740439 \r\nL 145.950978 107.740439 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_10\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 159.066719 550.8 \r\nL 172.18246 550.8 \r\nL 172.18246 33.085714 \r\nL 159.066719 33.085714 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_11\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 172.18246 550.8 \r\nL 185.2982 550.8 \r\nL 185.2982 59.052575 \r\nL 172.18246 59.052575 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_12\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 185.2982 550.8 \r\nL 198.413941 550.8 \r\nL 198.413941 86.642365 \r\nL 185.2982 86.642365 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_13\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 198.413941 550.8 \r\nL 211.529682 550.8 \r\nL 211.529682 229.460099 \r\nL 198.413941 229.460099 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_14\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 211.529682 550.8 \r\nL 224.645423 550.8 \r\nL 224.645423 200.24738 \r\nL 211.529682 200.24738 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_15\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 224.645423 550.8 \r\nL 237.761163 550.8 \r\nL 237.761163 283.016749 \r\nL 224.645423 283.016749 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_16\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 237.761163 550.8 \r\nL 250.876904 550.8 \r\nL 250.876904 308.983609 \r\nL 237.761163 308.983609 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_17\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 250.876904 550.8 \r\nL 263.992645 550.8 \r\nL 263.992645 274.902105 \r\nL 250.876904 274.902105 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_18\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 263.992645 550.8 \r\nL 277.108385 550.8 \r\nL 277.108385 401.490551 \r\nL 263.992645 401.490551 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_19\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 277.108385 550.8 \r\nL 290.224126 550.8 \r\nL 290.224126 420.965696 \r\nL 277.108385 420.965696 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_20\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 290.224126 550.8 \r\nL 303.339867 550.8 \r\nL 303.339867 412.851052 \r\nL 290.224126 412.851052 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_21\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 303.339867 550.8 \r\nL 316.455607 550.8 \r\nL 316.455607 396.621764 \r\nL 303.339867 396.621764 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_22\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 316.455607 550.8 \r\nL 329.571348 550.8 \r\nL 329.571348 443.6867 \r\nL 316.455607 443.6867 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_23\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 329.571348 550.8 \r\nL 342.687089 550.8 \r\nL 342.687089 481.014062 \r\nL 329.571348 481.014062 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_24\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 342.687089 550.8 \r\nL 355.80283 550.8 \r\nL 355.80283 497.24335 \r\nL 342.687089 497.24335 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_25\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 355.80283 550.8 \r\nL 368.91857 550.8 \r\nL 368.91857 500.489207 \r\nL 355.80283 500.489207 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_26\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 368.91857 550.8 \r\nL 382.034311 550.8 \r\nL 382.034311 495.620421 \r\nL 368.91857 495.620421 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_27\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 382.034311 550.8 \r\nL 395.150052 550.8 \r\nL 395.150052 490.751635 \r\nL 382.034311 490.751635 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_28\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 395.150052 550.8 \r\nL 408.265792 550.8 \r\nL 408.265792 521.587282 \r\nL 395.150052 521.587282 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_29\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 408.265792 550.8 \r\nL 421.381533 550.8 \r\nL 421.381533 198.624451 \r\nL 408.265792 198.624451 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_30\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 86.66375 550.8 \r\nL 99.128621 550.8 \r\nL 99.128621 506.980923 \r\nL 86.66375 506.980923 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_31\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 99.128621 550.8 \r\nL 111.59349 550.8 \r\nL 111.59349 231.083027 \r\nL 99.128621 231.083027 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_32\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 111.59349 550.8 \r\nL 124.058359 550.8 \r\nL 124.058359 166.165876 \r\nL 111.59349 166.165876 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_33\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 124.058359 550.8 \r\nL 136.523227 550.8 \r\nL 136.523227 98.002866 \r\nL 124.058359 98.002866 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_34\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 136.523227 550.8 \r\nL 148.988096 550.8 \r\nL 148.988096 93.13408 \r\nL 136.523227 93.13408 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_35\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 148.988096 550.8 \r\nL 161.452973 550.8 \r\nL 161.452973 114.232154 \r\nL 148.988096 114.232154 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_36\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 161.452973 550.8 \r\nL 173.917842 550.8 \r\nL 173.917842 114.232154 \r\nL 161.452973 114.232154 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_37\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 173.917842 550.8 \r\nL 186.382711 550.8 \r\nL 186.382711 102.871652 \r\nL 173.917842 102.871652 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_38\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 186.382711 550.8 \r\nL 198.847579 550.8 \r\nL 198.847579 119.10094 \r\nL 186.382711 119.10094 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_39\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 198.847579 550.8 \r\nL 211.312448 550.8 \r\nL 211.312448 154.805374 \r\nL 198.847579 154.805374 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_40\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 211.312448 550.8 \r\nL 223.777325 550.8 \r\nL 223.777325 162.920018 \r\nL 211.312448 162.920018 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_41\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 223.777325 550.8 \r\nL 236.242185 550.8 \r\nL 236.242185 244.066458 \r\nL 223.777325 244.066458 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_42\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 236.242185 550.8 \r\nL 248.707063 550.8 \r\nL 248.707063 274.902105 \r\nL 236.242185 274.902105 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_43\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 248.707063 550.8 \r\nL 261.171923 550.8 \r\nL 261.171923 284.639678 \r\nL 248.707063 284.639678 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_44\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 261.171923 550.8 \r\nL 273.6368 550.8 \r\nL 273.6368 339.819257 \r\nL 261.171923 339.819257 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_45\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 273.6368 550.8 \r\nL 286.10166 550.8 \r\nL 286.10166 346.310972 \r\nL 273.6368 346.310972 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_46\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 286.10166 550.8 \r\nL 298.566537 550.8 \r\nL 298.566537 394.998836 \r\nL 286.10166 394.998836 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_47\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 298.566537 550.8 \r\nL 311.031415 550.8 \r\nL 311.031415 417.719839 \r\nL 298.566537 417.719839 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_48\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 311.031415 550.8 \r\nL 323.496275 550.8 \r\nL 323.496275 468.030631 \r\nL 311.031415 468.030631 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_49\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 323.496275 550.8 \r\nL 335.961152 550.8 \r\nL 335.961152 477.768204 \r\nL 323.496275 477.768204 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_50\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 335.961152 550.8 \r\nL 348.426012 550.8 \r\nL 348.426012 477.768204 \r\nL 335.961152 477.768204 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_51\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 348.426012 550.8 \r\nL 360.890889 550.8 \r\nL 360.890889 487.505777 \r\nL 348.426012 487.505777 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_52\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 360.890889 550.8 \r\nL 373.355767 550.8 \r\nL 373.355767 500.489207 \r\nL 360.890889 500.489207 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_53\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 373.355767 550.8 \r\nL 385.820644 550.8 \r\nL 385.820644 500.489207 \r\nL 373.355767 500.489207 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_54\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 385.820644 550.8 \r\nL 398.285487 550.8 \r\nL 398.285487 510.22678 \r\nL 385.820644 510.22678 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_55\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 398.285487 550.8 \r\nL 410.750364 550.8 \r\nL 410.750364 521.587282 \r\nL 398.285487 521.587282 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_56\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 410.750364 550.8 \r\nL 423.215241 550.8 \r\nL 423.215241 516.718495 \r\nL 410.750364 516.718495 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_57\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 423.215241 550.8 \r\nL 435.680119 550.8 \r\nL 435.680119 537.81657 \r\nL 423.215241 537.81657 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_58\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 435.680119 550.8 \r\nL 448.144961 550.8 \r\nL 448.144961 537.81657 \r\nL 435.680119 537.81657 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_59\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 448.144961 550.8 \r\nL 460.609839 550.8 \r\nL 460.609839 544.308285 \r\nL 448.144961 544.308285 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_60\">\r\n    <path clip-path=\"url(#pd5c58939d9)\" d=\"M 460.609839 550.8 \r\nL 473.074716 550.8 \r\nL 473.074716 547.554142 \r\nL 460.609839 547.554142 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"mbde99baec5\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.30499\" xlink:href=\"#mbde99baec5\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(53.12374 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"129.320152\" xlink:href=\"#mbde99baec5\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 1 -->\r\n      <g transform=\"translate(126.138902 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"202.335315\" xlink:href=\"#mbde99baec5\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 2 -->\r\n      <g transform=\"translate(199.154065 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"275.350478\" xlink:href=\"#mbde99baec5\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 3 -->\r\n      <g transform=\"translate(272.169228 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"348.36564\" xlink:href=\"#mbde99baec5\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 4 -->\r\n      <g transform=\"translate(345.18439 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"421.380803\" xlink:href=\"#mbde99baec5\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(418.199553 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m4b8a5169ed\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m4b8a5169ed\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(33.603125 554.599219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m4b8a5169ed\" y=\"469.65356\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(27.240625 473.452779)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m4b8a5169ed\" y=\"388.50712\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(20.878125 392.306339)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m4b8a5169ed\" y=\"307.360681\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 150 -->\r\n      <g transform=\"translate(20.878125 311.159899)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m4b8a5169ed\" y=\"226.214241\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(20.878125 230.01346)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m4b8a5169ed\" y=\"145.067801\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 250 -->\r\n      <g transform=\"translate(20.878125 148.86702)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m4b8a5169ed\" y=\"63.921361\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 300 -->\r\n      <g transform=\"translate(20.878125 67.72058)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_14\">\r\n     <!-- Count -->\r\n     <g transform=\"translate(14.798437 293.848437)rotate(-90)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 64.40625 67.28125 \r\nL 64.40625 56.890625 \r\nQ 59.421875 61.53125 53.78125 63.8125 \r\nQ 48.140625 66.109375 41.796875 66.109375 \r\nQ 29.296875 66.109375 22.65625 58.46875 \r\nQ 16.015625 50.828125 16.015625 36.375 \r\nQ 16.015625 21.96875 22.65625 14.328125 \r\nQ 29.296875 6.6875 41.796875 6.6875 \r\nQ 48.140625 6.6875 53.78125 8.984375 \r\nQ 59.421875 11.28125 64.40625 15.921875 \r\nL 64.40625 5.609375 \r\nQ 59.234375 2.09375 53.4375 0.328125 \r\nQ 47.65625 -1.421875 41.21875 -1.421875 \r\nQ 24.65625 -1.421875 15.125 8.703125 \r\nQ 5.609375 18.84375 5.609375 36.375 \r\nQ 5.609375 53.953125 15.125 64.078125 \r\nQ 24.65625 74.21875 41.21875 74.21875 \r\nQ 47.75 74.21875 53.53125 72.484375 \r\nQ 59.328125 70.75 64.40625 67.28125 \r\nz\r\n\" id=\"DejaVuSans-67\"/>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n       <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n       <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-67\"/>\r\n      <use x=\"69.824219\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"131.005859\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"194.384766\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"257.763672\" xlink:href=\"#DejaVuSans-116\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_61\">\r\n    <path d=\"M 46.965625 550.8 \r\nL 46.965625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_62\">\r\n    <path d=\"M 493.365625 550.8 \r\nL 493.365625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_63\">\r\n    <path d=\"M 46.965625 550.8 \r\nL 493.365625 550.8 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_64\">\r\n    <path d=\"M 46.965625 7.2 \r\nL 493.365625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_65\">\r\n     <path d=\"M 407.425 44.55625 \r\nL 486.365625 44.55625 \r\nQ 488.365625 44.55625 488.365625 42.55625 \r\nL 488.365625 14.2 \r\nQ 488.365625 12.2 486.365625 12.2 \r\nL 407.425 12.2 \r\nQ 405.425 12.2 405.425 14.2 \r\nL 405.425 42.55625 \r\nQ 405.425 44.55625 407.425 44.55625 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"patch_66\">\r\n     <path d=\"M 409.425 23.798437 \r\nL 429.425 23.798437 \r\nL 429.425 16.798437 \r\nL 409.425 16.798437 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"text_15\">\r\n     <!-- Actual -->\r\n     <g transform=\"translate(437.425 23.798437)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 34.1875 63.1875 \r\nL 20.796875 26.90625 \r\nL 47.609375 26.90625 \r\nz\r\nM 28.609375 72.90625 \r\nL 39.796875 72.90625 \r\nL 67.578125 0 \r\nL 57.328125 0 \r\nL 50.6875 18.703125 \r\nL 17.828125 18.703125 \r\nL 11.1875 0 \r\nL 0.78125 0 \r\nz\r\n\" id=\"DejaVuSans-65\"/>\r\n       <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n       <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n       <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"66.658203\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"121.638672\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"160.847656\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"224.226562\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"285.505859\" xlink:href=\"#DejaVuSans-108\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"patch_67\">\r\n     <path d=\"M 409.425 38.476562 \r\nL 429.425 38.476562 \r\nL 429.425 31.476562 \r\nL 409.425 31.476562 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"text_16\">\r\n     <!-- Predicted -->\r\n     <g transform=\"translate(437.425 38.476562)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 19.671875 64.796875 \r\nL 19.671875 37.40625 \r\nL 32.078125 37.40625 \r\nQ 38.96875 37.40625 42.71875 40.96875 \r\nQ 46.484375 44.53125 46.484375 51.125 \r\nQ 46.484375 57.671875 42.71875 61.234375 \r\nQ 38.96875 64.796875 32.078125 64.796875 \r\nz\r\nM 9.8125 72.90625 \r\nL 32.078125 72.90625 \r\nQ 44.34375 72.90625 50.609375 67.359375 \r\nQ 56.890625 61.8125 56.890625 51.125 \r\nQ 56.890625 40.328125 50.609375 34.8125 \r\nQ 44.34375 29.296875 32.078125 29.296875 \r\nL 19.671875 29.296875 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-80\"/>\r\n       <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n       <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n       <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n       <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-80\"/>\r\n      <use x=\"58.552734\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"97.416016\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"158.939453\" xlink:href=\"#DejaVuSans-100\"/>\r\n      <use x=\"222.416016\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"250.199219\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"305.179688\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"344.388672\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"405.912109\" xlink:href=\"#DejaVuSans-100\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pd5c58939d9\">\r\n   <rect height=\"543.6\" width=\"446.4\" x=\"46.965625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "y_pred = best_model.predict(x_valid_reg)\n",
    "fig,ax_best = plt.subplots(1,1,figsize = (8,10))\n",
    "sns.histplot(y_valid_reg, ax = ax_best, label = 'Actual')\n",
    "sns.histplot(y_pred, ax = ax_best, label = \"Predicted\")\n",
    "ax_best.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       loss  val_loss\n",
       "0  0.756955  0.534796\n",
       "1  0.465520  0.467508\n",
       "2  0.427758  0.436776\n",
       "3  0.402379  0.425289\n",
       "4  0.383927  0.404964\n",
       "5  0.373410  0.392298\n",
       "6  0.364215  0.387033\n",
       "7  0.357938  0.383421\n",
       "8  0.348430  0.375755\n",
       "9  0.347998  0.374411"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n      <th>val_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.756955</td>\n      <td>0.534796</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.465520</td>\n      <td>0.467508</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.427758</td>\n      <td>0.436776</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.402379</td>\n      <td>0.425289</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.383927</td>\n      <td>0.404964</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.373410</td>\n      <td>0.392298</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.364215</td>\n      <td>0.387033</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.357938</td>\n      <td>0.383421</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.348430</td>\n      <td>0.375755</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.347998</td>\n      <td>0.374411</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 121
    }
   ],
   "source": [
    "df_best_model = pd.DataFrame(data = history_best_model.history)\n",
    "df_best_model"
   ]
  },
  {
   "source": [
    "With Randomized Search CV, when there is a very large parameter space, this approach will only explore a tiny portion of the hyperparameter space.      \n",
    "\n",
    "To overcome this, first run a quick random search using wide ranges of hyperparameter values, then run another search using smaller range of values centered on the best ones found during the first run. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h2>General Guidelines When Hyperparameter Tuning</h2>\n",
    "\n",
    "\n",
    "- Number of Hidden Layers\n",
    "\n",
    "Deep neural networks have a much higher parameter efficiency than shallow nets, allowing them to model complex functions using exponentially fewer neurons than shallow nets. \n",
    "\n",
    "Lower hidden layers tend to model low-level structures, intermediate hidden layers combine these low level strutures to model intermediate level structures and the higher hidden layers and the output layer combine these intermediate structures to model high level structures. \n",
    "\n",
    "For example, if you have already trained a model to recognize faces in pictures, and you now want to train a new neural network to recognize hairstyles, then you can kickstart training by reusing the lower layers of the first network.\n",
    "\n",
    "Using the lower layers of a previously built neural network, and attaching new layers to it, so that we can make the network to learn higher level structures using the deeper newer layers. This is called transfer learning. \n",
    "\n",
    "\n",
    "<h2> Learning Rate </h2> \n",
    "\n",
    "- The most important hyperparameter. The optimal learning rate is about half the maximum learning rate(rate at which the algorithm diverges). A simple approach is to start with a large value which makes the algorithm diverge. Divide this by 3 and try again. Repeat until algorithm stops diverging. \n",
    "\n",
    "- Choose a better optimizer.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}