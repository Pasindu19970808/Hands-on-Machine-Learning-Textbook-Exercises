{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0965ba1120b0c101b3f715b6e258a73742ec1cf86f2c8b04492724c87d9f112c3",
   "display_name": "Python 3.8.8 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "965ba1120b0c101b3f715b6e258a73742ec1cf86f2c8b04492724c87d9f112c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full),(X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2440843c850>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-04-10T11:31:08.083528</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 251.565 248.518125 \r\nL 251.565 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 26.925 224.64 \r\nL 244.365 224.64 \r\nL 244.365 7.2 \r\nL 26.925 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g clip-path=\"url(#p02d52d2e3f)\">\r\n    <image height=\"218\" id=\"imagefe298cffb7\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\r\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\" y=\"-6.64\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m6f968079ed\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#m6f968079ed\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#m6f968079ed\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#m6f968079ed\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#m6f968079ed\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#m6f968079ed\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#m6f968079ed\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m5046c8c913\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5046c8c913\" y=\"11.082857\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5046c8c913\" y=\"49.911429\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5046c8c913\" y=\"88.74\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5046c8c913\" y=\"127.568571\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5046c8c913\" y=\"166.397143\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m5046c8c913\" y=\"205.225714\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 26.925 224.64 \r\nL 26.925 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 244.365 224.64 \r\nL 244.365 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 26.925 224.64 \r\nL 244.365 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 26.925 7.2 \r\nL 244.365 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p02d52d2e3f\">\r\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAUmklEQVR4nO3da3Bc5XkH8P+zF2l1sSTLFyFsY242xEmIAQXawqQkNAzQTk1mWgbTZGhC63wIMzCl0zLkA3zoNDQtyeQDk45TmJhOSpIGKHTKJFA3qfGEGsuOYmwcsLn4Flm2KwvdtbenH3SgAvQ+r7xnd8/G7/83o5G0z549r87q0Vntc573FVUFEZ39UkkPgIjqg8lOFAgmO1EgmOxEgWCyEwUiU8+dNUmz5tBWz13+RpCmrBkvdDaZ8dySaWcsX0rbjz1t7xu+Yk3avkNX66QzNjLZam6bO+L+uQBAy2UzHqJpTCCvMzJfLFayi8iNAL4FIA3gn1T1Iev+ObThark+zi4rJ/P+/P8vwRJk5txVZnzw5pVmfO3nX3PGjox12Y99YJkZT83/e/OeUmfJjG+44hfO2DMD681tL73H/XMBQHlszIzH0sC/L5YdutUZq/hlvIikATwC4CYA6wBsFJF1lT4eEdVWnP/ZrwJwUFXfVNU8gO8D2FCdYRFRtcVJ9hUAjsz5/mh02/uIyCYR6ReR/gJmYuyOiOKo+bvxqrpZVftUtS+L5lrvjogc4iT7MQBz31laGd1GRA0oTrLvBLBGRC4QkSYAtwF4tjrDIqJqq7j0pqpFEbkLwE8wW3p7TFX3VW1kZ6rGpZLMyg+9HfGe/X9ll8b+8JpdZnxx5g0zPpQ/acYXZdz16K+ttP/+XnBZuxn3GS/btfDnJnucseJl9jUAy7bbpbX94+eY8f7/WeuMXfL3b5nbFo8PmfHfRLHq7Kr6HIDnqjQWIqohXi5LFAgmO1EgmOxEgWCyEwWCyU4UCCY7USCknrPLdki31qzFNWadPfWJj5jxP3hiuzO2450LzG1H8nbf9lTR08/u6UmfyLv73YdH7PkDWtvsfoVSyT4f5PN29TabdbfAntd92ty2OVM04+0Ze+yLsu5rAE5O29cXHN5ysRlf8uhLZjwpO3QrRnV43mTgmZ0oEEx2okAw2YkCwWQnCgSTnSgQTHaiQNR1KumaillCPP21ghl/aeQiZ+yt0W5z25ynhFRWu2w44ym9ibh/dl9pbWbG/hUoekprGaO0BgCLWt3lL1/JcaZk73t0JmfG06lFzlhbNm9ue/GX7JltR59abMZLp+2yYhJ4ZicKBJOdKBBMdqJAMNmJAsFkJwoEk50oEEx2okCcPXV2j8yF55vxjy8ZNONHJrqcsdasXaOfKdqHuTvnXtYYAJa12HX6jLiXLi6qp0XVU8vOl+0af1fTlBnvzb3jjM2U7Tr7VMlThy/bYx+actfZfTX6npw9jfVrt3/CjC9/5OdmPAk8sxMFgslOFAgmO1EgmOxEgWCyEwWCyU4UCCY7USCCqbMXl3eY8Ws67brof5UvdcY6PFMan9s8YsYny+6poAGgOzNhxgvqroWnjBo8AGTF7kcve+r0zSn7GoM03PsvqP3r5xu7r04P4ykfGLOX2e7I2NcPTF9n1+HxiB1OQqxkF5G3AYwBKAEoqmpfNQZFRNVXjTP7p1X1VBUeh4hqiP+zEwUibrIrgOdFZJeIbJrvDiKySUT6RaS/APt/WyKqnbgv469V1WMishzACyLyK1XdNvcOqroZwGZgdq23mPsjogrFOrOr6rHo8wkATwO4qhqDIqLqqzjZRaRNRBa9+zWAGwDsrdbAiKi64ryM7wHwtMwulZwB8C+q+uOqjKoGTl5uL12cE7te/Dudbzhjvlp1Vux+9FNF+xqA7cPuOesB4JeH3TXj9GG7bzszYc9Zn/a8zZKd8CyFbRzWUrO975GP2sft7t993oyfyLuP69q2E+a25zXZBaYXW+3npBFVnOyq+iYAu4OfiBoGS29EgWCyEwWCyU4UCCY7USCY7ESBEI251PGZ6JBuvVqur9v+zkR6zYVm/OAXe5yx5o+4p0sGgBV/a0/HrDtfMeNxpDvssp4sajfj2tZixssddrzU4m5DzYzZdb3ywKtm3OfKX7hbZG/osC8JOVa0l2TeN7nCjO+6PJnz6A7dilEdnremyTM7USCY7ESBYLITBYLJThQIJjtRIJjsRIFgshMFIpippF//R8+8Gp7LDXr/230HGbBr2fnFdqvmbfvtdktrOmYAeGN6uTP26qhdBz82ZtfZZ4qeawTUHpvItDPWs2jc3PbOlYfM+I9OXGnGd/+Z+9qIgXfsFlX99ZAZL0/ay2w3Ip7ZiQLBZCcKBJOdKBBMdqJAMNmJAsFkJwoEk50oEMH0s0/80dVm/NeftrfPdLvrxV/ve9Lc9t7/+LwZ733Rfg5mOu2/yaNGybjY5nl+feGMfQfN2nHJu6eLlrI9lXTXfjveNGbv+/Qt7qWuiwX7EpPyiL2M9n2f+Xcz/sxnLjPjxcHjZrxS7GcnIiY7USiY7ESBYLITBYLJThQIJjtRIJjsRIEIps5uzSEOAOOlZjO+69QqZ2xJi93bfGXXYTP+wLJ486OPl93XAAyX7V76abVr2SVPfFLtenXOWM66M2Uvdb0yY/fa78tPmfGvHrrFGTtwaqm5be55e46CQrt9XHof/rkZr5VYdXYReUxETojI3jm3dYvICyJyIPpsz6hPRIlbyMv47wK48QO33Qdgq6quAbA1+p6IGpg32VV1G4DhD9y8AcCW6OstAG6p7rCIqNoqnYOuR1UHo6+PA3BO9iUimwBsAoAcWivcHRHFFfvdeJ19h8/5Lp+qblbVPlXty8J+E4yIaqfSZB8SkV4AiD7b06MSUeIqTfZnAdwRfX0HgGeqMxwiqhVvnV1EngBwHYClAIYAPADg3wD8EMB5AA4BuFVVP/gm3ockWWd/8+9+24xfee1rZvy25S87Y3/58h+b2zbvtedun15mXwPQdtT+m6zG1O5lz7sypRZPv7o9bbyXFN316IxdJkeqYMcLdhke06vyztjBmzab237x8HVm/PHV28z4793+JTOe/tluM14pq87ufYNOVTc6QslkLRFVhJfLEgWCyU4UCCY7USCY7ESBYLITBSKYJZtbLhkx46en7Ut5Xxxd64y17bRLa1NXu6c0BoDfX2O3uJbV/pvc7KtRGQqe2ppv3ymxy4YpcZf2mlN2+22xbO9797C77RgARn90rjP2N5/8mLnty0dWm/GPH7/djK/afdCM2829tcEzO1EgmOxEgWCyEwWCyU4UCCY7USCY7ESBYLITBSKYOvunVrxpxlvS7nZIALixc48z9tLxq8xtR6eyZnyqZC8PfGyy04xnUu5a90zRfoqzabvi66t1q2eqaTHq7Etz9vUHk0X7uH20y172eOeku85+QbM938q6c+zHvqj9lBnfe/4lZhx7Ru14DfDMThQIJjtRIJjsRIFgshMFgslOFAgmO1EgmOxEgQimzp7xLA88nG8z49Pqrvk2jdqPnW2x+82Lnp7xJs/Ym9LuvvCUe7EeAP7jUhS7393Xz140+uWznn23Z+3H9vXxt560++Utly4ash/bc13G5Hn2ks8592UbNcMzO1EgmOxEgWCyEwWCyU4UCCY7USCY7ESBYLITBSKYOntW7JquNb85ABTUfaiaT02b2+Za7HpvoWzXsn218LKnpzzOtmXYcd/ZYsroSS9k7Z+7JW3X0a0+fgDIHR1zxk4V7Tr4jGeta9+c9/kO+8jkzGhteM/sIvKYiJwQkb1zbntQRI6JyED0cXNth0lEcS3kZfx3Adw4z+3fVNX10cdz1R0WEVWbN9lVdRuA4TqMhYhqKM4bdHeJyJ7oZf5i151EZJOI9ItIfwEzMXZHRHFUmuzfBnARgPUABgE87Lqjqm5W1T5V7cuiucLdEVFcFSW7qg6paklVywC+A8CeXpWIEldRsotI75xvPwdgr+u+RNQYvHV2EXkCwHUAlorIUQAPALhORNYDUABvA/hy7YZYH966qdGXnTlsz0G+KGf3ysdlXSPg65XPeWr4Gc9K4r5ad9rod897ri/wPSc+Mu1+j8jXh+/7uXx1+HK68msfasWb7Kq6cZ6bH63BWIiohni5LFEgmOxEgWCyEwWCyU4UCCY7USCCaXGN0wYKAGljSubicXva4VzmPDPuG1vRU6KyykgzJfspznhKUL4W13Kp8vPFdMlektk3tjTsuLa5G0lfnzzH3LYrM2nGfUpJ9LB68MxOFAgmO1EgmOxEgWCyEwWCyU4UCCY7USCY7ESBCKbOnqTOpikz7mtDjdOOabWYLoT3+gRPuGT8bGW1xzZetGc28i35XGprcsZ+duhic9vb1/ab8XeKLWY85mUdNcEzO1EgmOxEgWCyEwWCyU4UCCY7USCY7ESBYLITBSKYOvuRKecKVQCAc3KjZjwrlU9rvKTZ7o0e89STy546fDFGKd27JLNnKeuU0ecP2LVwXw3fWu55IfvWlPvxZ462m9u2Xpo346e11d63PQVBInhmJwoEk50oEEx2okAw2YkCwWQnCgSTnSgQTHaiQJw1dfZUzp6o21fTzYrdG31wxp5n3NKWcS8dDAATRXff9UJYdfjWjF0vznuWHvbV2X1y6ULF+y6V7XOR7xoBzbq3bztsP3Z7etqMz5TtawDK2cZraPee2UVklYj8VEReFZF9InJ3dHu3iLwgIgeiz/ZVK0SUqIW8jC8CuFdV1wH4LQBfEZF1AO4DsFVV1wDYGn1PRA3Km+yqOqiqu6OvxwDsB7ACwAYAW6K7bQFwS43GSERVcEb/s4vI+QAuB7ADQI+qDkah4wB6HNtsArAJAHKwrycmotpZ8LvxItIO4EkA96jq+7pGVFWB+bsSVHWzqvapal8WdsMHEdXOgpJdRLKYTfTvqepT0c1DItIbxXsBnKjNEImoGrwv40VEADwKYL+qfmNO6FkAdwB4KPr8TE1GuECzLy7cfKW3FqNEBADb/neNEbWXbG5O2e2xvhKSb6ppS6rGLay+sRWNJaOtKbAB/3M27Sl/5Tvd++5+zX6+21J2udRb9mu8ytuC/me/BsAXALwiIgPRbfdjNsl/KCJ3AjgE4NaajJCIqsKb7Kq6He6lAK6v7nCIqFZ4uSxRIJjsRIFgshMFgslOFAgmO1EgzpoWVx/fdMy+FtdfDS13xlZ76uy+x/bVk31tqhljWebmtF3jL5TjzXnsW07aOu55z77jttdOd7off8nAiLmtb+pw3/UHvqWsk8AzO1EgmOxEgWCyEwWCyU4UCCY7USCY7ESBYLITBSKcOrun8OmrhReOtlW875GCPR3XweGlZnxsvMWMl0uVF3W15Pl7n7LryeKrhRtDE8+ws012rburyV4Ku9Bu7ODgYXPbtKeOXvBct+GZJTsRPLMTBYLJThQIJjtRIJjsRIFgshMFgslOFAgmO1EgGrAaWBnxFG29/cce2fHKa9ldWbse3Npkz2Gez9lP08quEWdsxpi3HQDyJbunPG5bttWTnvbMG39q3L62oTc3asZ3nOPed3liwty2K23HfesMeKa0TwTP7ESBYLITBYLJThQIJjtRIJjsRIFgshMFgslOFIiFrM++CsDjAHoAKIDNqvotEXkQwJ8DOBnd9X5Vfa5WA/XK2oXNiWKTGZ8s2/E4623/4MfXmvFih91L33zKroW/le5wxjxt+l7qmVbee1ysfna7zA4p2g/+r6NXmPGVuyr/4SfKzWY872lY97S7J2IhF9UUAdyrqrtFZBGAXSLyQhT7pqr+Q+2GR0TVspD12QcBDEZfj4nIfgAraj0wIqquM3qxISLnA7gcwI7oprtEZI+IPCYiix3bbBKRfhHpL2Am3miJqGILTnYRaQfwJIB7VHUUwLcBXARgPWbP/A/Pt52qblbVPlXty8L+P4iIamdByS4iWcwm+vdU9SkAUNUhVS2pahnAdwBcVbthElFc3mSX2XayRwHsV9VvzLm9d87dPgdgb/WHR0TVspB3468B8AUAr4jIQHTb/QA2ish6zJbj3gbw5RqMb8FS7XY7ZNpT5/FOJd3pqRMZLrzvpYq3pWSUPedBX8t0oTNeS3UtLOTd+O2Yv1qaXE2diM5YA5b+iagWmOxEgWCyEwWCyU4UCCY7USCY7ESBOGumki4OHjfjr7/xSTN+cHC5GV+2M8bfRd/axD7aeDXbs91f/ORPzPji1afN+NKBxnvOeGYnCgSTnSgQTHaiQDDZiQLBZCcKBJOdKBBMdqJAiNaxhisiJwEcmnPTUgCn6jaAM9OoY2vUcQEcW6WqObbVqrpsvkBdk/1DOxfpV9W+xAZgaNSxNeq4AI6tUvUaG1/GEwWCyU4UiKSTfXPC+7c06tgadVwAx1apuowt0f/Ziah+kj6zE1GdMNmJApFIsovIjSLymogcFJH7khiDi4i8LSKviMiAiPQnPJbHROSEiOydc1u3iLwgIgeiz/OusZfQ2B4UkWPRsRsQkZsTGtsqEfmpiLwqIvtE5O7o9kSPnTGuuhy3uv/PLiJpAK8D+CyAowB2Atioqq/WdSAOIvI2gD5VTfwCDBH5FIBxAI+r6sei274OYFhVH4r+UC5W1b9ukLE9CGA86WW8o9WKeucuMw7gFgB/igSPnTGuW1GH45bEmf0qAAdV9U1VzQP4PoANCYyj4anqNgDDH7h5A4At0ddbMPvLUneOsTUEVR1U1d3R12MA3l1mPNFjZ4yrLpJI9hUAjsz5/igaa713BfC8iOwSkU1JD2YePao6GH19HEBPkoOZh3cZ73r6wDLjDXPsKln+PC6+Qfdh16rqFQBuAvCV6OVqQ9LZ/8EaqXa6oGW862WeZcbfk+Sxq3T587iSSPZjAFbN+X5ldFtDUNVj0ecTAJ5G4y1FPfTuCrrR5xMJj+c9jbSM93zLjKMBjl2Sy58nkew7AawRkQtEpAnAbQCeTWAcHyIibdEbJxCRNgA3oPGWon4WwB3R13cAeCbBsbxPoyzj7VpmHAkfu8SXP1fVun8AuBmz78i/AeCrSYzBMa4LAfwy+tiX9NgAPIHZl3UFzL63cSeAJQC2AjgA4D8BdDfQ2P4ZwCsA9mA2sXoTGtu1mH2JvgfAQPRxc9LHzhhXXY4bL5clCgTfoCMKBJOdKBBMdqJAMNmJAsFkJwoEk50oEEx2okD8H0RpcA72d9CGAAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.imshow(X_train_full[5,:].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a validation set \n",
    "X_valid,X_train = X_train_full[:5000]/255.0,X_train_full[5000:]/255.0\n",
    "y_valid,y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "\"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "source": [
    "Building a neural network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a single stack of layers connected sequentially\n",
    "model = keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a Flatten layer. It converts the specified input shape into a (-1,1) instance. \n",
    "model.add(keras.layers.Flatten(input_shape = [28,28]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next we add the first hidden layer containing 300 neurons. It will use the Rectified Linear Unit Activation Function(ReLU). \n",
    "#This does not include bias terms \n",
    "model.add(keras.layers.Dense(300, activation = \"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next we add another hidden layer with 100 neurons in it\n",
    "model.add(keras.layers.Dense(100, activation = \"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#last we add in the output layer which has 10 neurons for the 10 different classes\n",
    "model.add(keras.layers.Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten (Flatten)            (None, 784)               0         \n_________________________________________________________________\ndense (Dense)                (None, 300)               235500    \n_________________________________________________________________\ndense_1 (Dense)              (None, 100)               30100     \n_________________________________________________________________\ndense_2 (Dense)              (None, 10)                1010      \n=================================================================\nTotal params: 266,610\nTrainable params: 266,610\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Flatten at 0x244085527c0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x2441ecde160>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x2441ed67c70>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x2441ed72f70>]"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "source": [
    "The shape of the weight matrix is dependant on the number of inputs. This is why input_shape should be specified in keras.layers.Flatten. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using compile method to specify the loss function, optimizer to use and extra metrics to compute during training \n",
    "model.compile(loss = \"sparse_categorical_crossentropy\",\n",
    "                optimizer = \"sgd\",\n",
    "                metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "y_train_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "keras.utils.to_categorical(y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.9907 - accuracy: 0.6912 - val_loss: 0.4945 - val_accuracy: 0.8332\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4975 - accuracy: 0.8282 - val_loss: 0.4383 - val_accuracy: 0.8534\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4456 - accuracy: 0.8439 - val_loss: 0.4139 - val_accuracy: 0.8546\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4199 - accuracy: 0.8533 - val_loss: 0.3955 - val_accuracy: 0.8660\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3985 - accuracy: 0.8597 - val_loss: 0.4259 - val_accuracy: 0.8562\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3771 - accuracy: 0.8674 - val_loss: 0.3787 - val_accuracy: 0.8676\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3670 - accuracy: 0.8698 - val_loss: 0.4147 - val_accuracy: 0.8532\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3490 - accuracy: 0.8765 - val_loss: 0.3759 - val_accuracy: 0.8648\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3502 - accuracy: 0.8744 - val_loss: 0.3449 - val_accuracy: 0.8800\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3366 - accuracy: 0.8818 - val_loss: 0.3446 - val_accuracy: 0.8776\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3287 - accuracy: 0.8835 - val_loss: 0.3545 - val_accuracy: 0.8766\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3178 - accuracy: 0.8864 - val_loss: 0.3264 - val_accuracy: 0.8850\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3099 - accuracy: 0.8895 - val_loss: 0.3407 - val_accuracy: 0.8768\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2998 - accuracy: 0.8918 - val_loss: 0.3320 - val_accuracy: 0.8834\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2937 - accuracy: 0.8941 - val_loss: 0.3197 - val_accuracy: 0.8888\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2889 - accuracy: 0.8975 - val_loss: 0.3276 - val_accuracy: 0.8850\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2825 - accuracy: 0.8979 - val_loss: 0.3205 - val_accuracy: 0.8868\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2766 - accuracy: 0.9007 - val_loss: 0.3082 - val_accuracy: 0.8894\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2724 - accuracy: 0.9014 - val_loss: 0.3102 - val_accuracy: 0.8860\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2653 - accuracy: 0.9034 - val_loss: 0.3025 - val_accuracy: 0.8922\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2585 - accuracy: 0.9062 - val_loss: 0.3053 - val_accuracy: 0.8878\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2563 - accuracy: 0.9074 - val_loss: 0.3219 - val_accuracy: 0.8838\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.2525 - accuracy: 0.9087 - val_loss: 0.3067 - val_accuracy: 0.8882\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2516 - accuracy: 0.9105 - val_loss: 0.3165 - val_accuracy: 0.8852\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2376 - accuracy: 0.9136 - val_loss: 0.2999 - val_accuracy: 0.8936\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2400 - accuracy: 0.9137 - val_loss: 0.3158 - val_accuracy: 0.8814\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2354 - accuracy: 0.9159 - val_loss: 0.3403 - val_accuracy: 0.8758\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.2289 - accuracy: 0.9173 - val_loss: 0.2952 - val_accuracy: 0.8944\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2302 - accuracy: 0.9174 - val_loss: 0.3122 - val_accuracy: 0.8902\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.2232 - accuracy: 0.9196 - val_loss: 0.3010 - val_accuracy: 0.8894\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train,epochs = 30, \n",
    "                    validation_data = (X_valid,y_valid))"
   ]
  },
  {
   "source": [
    "For skewed datasets with some classes being overrepresented while others are underrepresented, it would be useful to set the class_weight argument in the fit method. \n",
    "\n",
    "For cases where we want instances to be given different weights, like in the case of outliers (NBA dataset), we need to use sample_weight. For sample_weight we can either pass a flat Numpy array with the same length as the input samples. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        loss  accuracy  val_loss  val_accuracy\n",
       "0   0.709397  0.768127  0.494491        0.8332\n",
       "1   0.486282  0.831073  0.438256        0.8534\n",
       "2   0.441617  0.846345  0.413920        0.8546\n",
       "3   0.415819  0.854527  0.395465        0.8660\n",
       "4   0.394034  0.861764  0.425879        0.8562\n",
       "5   0.378878  0.866709  0.378669        0.8676\n",
       "6   0.364719  0.870909  0.414707        0.8532\n",
       "7   0.353002  0.874909  0.375915        0.8648\n",
       "8   0.341537  0.878000  0.344890        0.8800\n",
       "9   0.333680  0.880564  0.344613        0.8776\n",
       "10  0.324251  0.884273  0.354516        0.8766\n",
       "11  0.316188  0.886491  0.326407        0.8850\n",
       "12  0.308687  0.889473  0.340679        0.8768\n",
       "13  0.301175  0.891073  0.331979        0.8834\n",
       "14  0.295456  0.893927  0.319652        0.8888\n",
       "15  0.289264  0.896727  0.327562        0.8850\n",
       "16  0.282639  0.897709  0.320450        0.8868\n",
       "17  0.278138  0.899655  0.308238        0.8894\n",
       "18  0.272548  0.900818  0.310224        0.8860\n",
       "19  0.266725  0.904527  0.302545        0.8922\n",
       "20  0.262571  0.904927  0.305269        0.8878\n",
       "21  0.257687  0.907509  0.321868        0.8838\n",
       "22  0.252687  0.909364  0.306685        0.8882\n",
       "23  0.248384  0.911218  0.316546        0.8852\n",
       "24  0.244049  0.912127  0.299868        0.8936\n",
       "25  0.239956  0.913709  0.315789        0.8814\n",
       "26  0.236823  0.915418  0.340254        0.8758\n",
       "27  0.232677  0.916455  0.295213        0.8944\n",
       "28  0.228823  0.917855  0.312209        0.8902\n",
       "29  0.226044  0.919018  0.300963        0.8894"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n      <th>accuracy</th>\n      <th>val_loss</th>\n      <th>val_accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.709397</td>\n      <td>0.768127</td>\n      <td>0.494491</td>\n      <td>0.8332</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.486282</td>\n      <td>0.831073</td>\n      <td>0.438256</td>\n      <td>0.8534</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.441617</td>\n      <td>0.846345</td>\n      <td>0.413920</td>\n      <td>0.8546</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.415819</td>\n      <td>0.854527</td>\n      <td>0.395465</td>\n      <td>0.8660</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.394034</td>\n      <td>0.861764</td>\n      <td>0.425879</td>\n      <td>0.8562</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.378878</td>\n      <td>0.866709</td>\n      <td>0.378669</td>\n      <td>0.8676</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.364719</td>\n      <td>0.870909</td>\n      <td>0.414707</td>\n      <td>0.8532</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.353002</td>\n      <td>0.874909</td>\n      <td>0.375915</td>\n      <td>0.8648</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.341537</td>\n      <td>0.878000</td>\n      <td>0.344890</td>\n      <td>0.8800</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.333680</td>\n      <td>0.880564</td>\n      <td>0.344613</td>\n      <td>0.8776</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.324251</td>\n      <td>0.884273</td>\n      <td>0.354516</td>\n      <td>0.8766</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.316188</td>\n      <td>0.886491</td>\n      <td>0.326407</td>\n      <td>0.8850</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.308687</td>\n      <td>0.889473</td>\n      <td>0.340679</td>\n      <td>0.8768</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.301175</td>\n      <td>0.891073</td>\n      <td>0.331979</td>\n      <td>0.8834</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.295456</td>\n      <td>0.893927</td>\n      <td>0.319652</td>\n      <td>0.8888</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.289264</td>\n      <td>0.896727</td>\n      <td>0.327562</td>\n      <td>0.8850</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.282639</td>\n      <td>0.897709</td>\n      <td>0.320450</td>\n      <td>0.8868</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.278138</td>\n      <td>0.899655</td>\n      <td>0.308238</td>\n      <td>0.8894</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.272548</td>\n      <td>0.900818</td>\n      <td>0.310224</td>\n      <td>0.8860</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.266725</td>\n      <td>0.904527</td>\n      <td>0.302545</td>\n      <td>0.8922</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.262571</td>\n      <td>0.904927</td>\n      <td>0.305269</td>\n      <td>0.8878</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.257687</td>\n      <td>0.907509</td>\n      <td>0.321868</td>\n      <td>0.8838</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.252687</td>\n      <td>0.909364</td>\n      <td>0.306685</td>\n      <td>0.8882</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.248384</td>\n      <td>0.911218</td>\n      <td>0.316546</td>\n      <td>0.8852</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.244049</td>\n      <td>0.912127</td>\n      <td>0.299868</td>\n      <td>0.8936</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.239956</td>\n      <td>0.913709</td>\n      <td>0.315789</td>\n      <td>0.8814</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.236823</td>\n      <td>0.915418</td>\n      <td>0.340254</td>\n      <td>0.8758</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.232677</td>\n      <td>0.916455</td>\n      <td>0.295213</td>\n      <td>0.8944</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.228823</td>\n      <td>0.917855</td>\n      <td>0.312209</td>\n      <td>0.8902</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.226044</td>\n      <td>0.919018</td>\n      <td>0.300963</td>\n      <td>0.8894</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data = history.history)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2442583e2e0>"
      ]
     },
     "metadata": {},
     "execution_count": 23
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 864x720 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"574.678125pt\" version=\"1.1\" viewBox=\"0 0 706.903125 574.678125\" width=\"706.903125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-04-10T11:32:54.753876</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 574.678125 \r\nL 706.903125 574.678125 \r\nL 706.903125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 30.103125 550.8 \r\nL 699.703125 550.8 \r\nL 699.703125 7.2 \r\nL 30.103125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m7df1281c5a\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"60.539489\" xlink:href=\"#m7df1281c5a\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(57.358239 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"165.492467\" xlink:href=\"#m7df1281c5a\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(162.311217 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"270.445445\" xlink:href=\"#m7df1281c5a\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(264.082945 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"375.398423\" xlink:href=\"#m7df1281c5a\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(369.035923 565.398438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"480.351401\" xlink:href=\"#m7df1281c5a\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(473.988901 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"585.304379\" xlink:href=\"#m7df1281c5a\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(578.941879 565.398438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"690.257357\" xlink:href=\"#m7df1281c5a\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 30 -->\r\n      <g transform=\"translate(683.894857 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_8\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"md318bda79f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#md318bda79f\" y=\"544.663808\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.2 -->\r\n      <g transform=\"translate(7.2 548.463026)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#md318bda79f\" y=\"473.350632\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.3 -->\r\n      <g transform=\"translate(7.2 477.149851)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#md318bda79f\" y=\"402.037457\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.4 -->\r\n      <g transform=\"translate(7.2 405.836675)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#md318bda79f\" y=\"330.724281\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.5 -->\r\n      <g transform=\"translate(7.2 334.5235)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#md318bda79f\" y=\"259.411106\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.6 -->\r\n      <g transform=\"translate(7.2 263.210325)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#md318bda79f\" y=\"188.09793\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 0.7 -->\r\n      <g transform=\"translate(7.2 191.897149)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#md318bda79f\" y=\"116.784755\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 0.8 -->\r\n      <g transform=\"translate(7.2 120.583974)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#md318bda79f\" y=\"45.47158\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 0.9 -->\r\n      <g transform=\"translate(7.2 49.270798)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.984375 1.515625 \r\nL 10.984375 10.5 \r\nQ 14.703125 8.734375 18.5 7.8125 \r\nQ 22.3125 6.890625 25.984375 6.890625 \r\nQ 35.75 6.890625 40.890625 13.453125 \r\nQ 46.046875 20.015625 46.78125 33.40625 \r\nQ 43.953125 29.203125 39.59375 26.953125 \r\nQ 35.25 24.703125 29.984375 24.703125 \r\nQ 19.046875 24.703125 12.671875 31.3125 \r\nQ 6.296875 37.9375 6.296875 49.421875 \r\nQ 6.296875 60.640625 12.9375 67.421875 \r\nQ 19.578125 74.21875 30.609375 74.21875 \r\nQ 43.265625 74.21875 49.921875 64.515625 \r\nQ 56.59375 54.828125 56.59375 36.375 \r\nQ 56.59375 19.140625 48.40625 8.859375 \r\nQ 40.234375 -1.421875 26.421875 -1.421875 \r\nQ 22.703125 -1.421875 18.890625 -0.6875 \r\nQ 15.09375 0.046875 10.984375 1.515625 \r\nz\r\nM 30.609375 32.421875 \r\nQ 37.25 32.421875 41.125 36.953125 \r\nQ 45.015625 41.5 45.015625 49.421875 \r\nQ 45.015625 57.28125 41.125 61.84375 \r\nQ 37.25 66.40625 30.609375 66.40625 \r\nQ 23.96875 66.40625 20.09375 61.84375 \r\nQ 16.21875 57.28125 16.21875 49.421875 \r\nQ 16.21875 41.5 20.09375 36.953125 \r\nQ 23.96875 32.421875 30.609375 32.421875 \r\nz\r\n\" id=\"DejaVuSans-57\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_16\">\r\n    <path clip-path=\"url(#pa4e062bd32)\" d=\"M 60.539489 181.396916 \r\nL 81.530084 340.506795 \r\nL 102.52068 372.359001 \r\nL 123.511275 390.756688 \r\nL 144.501871 406.29213 \r\nL 165.492467 417.100057 \r\nL 186.483062 427.197307 \r\nL 207.473658 435.553257 \r\nL 228.464254 443.729173 \r\nL 249.454849 449.332416 \r\nL 270.445445 456.056455 \r\nL 291.43604 461.806492 \r\nL 312.426636 467.155358 \r\nL 333.417232 472.512661 \r\nL 354.407827 476.590832 \r\nL 375.398423 481.006522 \r\nL 396.389018 485.731655 \r\nL 417.379614 488.94128 \r\nL 438.37021 492.927468 \r\nL 459.360805 497.079791 \r\nL 480.351401 500.042393 \r\nL 501.341996 503.525714 \r\nL 522.332592 507.09086 \r\nL 543.323188 510.159546 \r\nL 564.313783 513.251377 \r\nL 585.304379 516.170113 \r\nL 606.294975 518.404077 \r\nL 627.28557 521.360909 \r\nL 648.276166 524.109249 \r\nL 669.266761 526.090909 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_17\">\r\n    <path clip-path=\"url(#pa4e062bd32)\" d=\"M 60.539489 139.514216 \r\nL 81.530084 94.625792 \r\nL 102.52068 83.734319 \r\nL 123.511275 77.89961 \r\nL 144.501871 72.739131 \r\nL 165.492467 69.212368 \r\nL 186.483062 66.217228 \r\nL 207.473658 63.364695 \r\nL 228.464254 61.160463 \r\nL 249.454849 59.332282 \r\nL 270.445445 56.687178 \r\nL 291.43604 55.105361 \r\nL 312.426636 52.978915 \r\nL 333.417232 51.837885 \r\nL 354.407827 49.802232 \r\nL 375.398423 47.805472 \r\nL 396.389018 47.105313 \r\nL 417.379614 45.717919 \r\nL 438.37021 44.888117 \r\nL 459.360805 42.243056 \r\nL 480.351401 41.957799 \r\nL 501.341996 40.11661 \r\nL 522.332592 38.79408 \r\nL 543.323188 37.471549 \r\nL 564.313783 36.823248 \r\nL 585.304379 35.695182 \r\nL 606.294975 34.476366 \r\nL 627.28557 33.737315 \r\nL 648.276166 32.738935 \r\nL 669.266761 31.909091 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_18\">\r\n    <path clip-path=\"url(#pa4e062bd32)\" d=\"M 60.539489 334.652661 \r\nL 81.530084 374.7557 \r\nL 102.52068 392.110609 \r\nL 123.511275 405.271646 \r\nL 144.501871 383.582064 \r\nL 165.492467 417.24953 \r\nL 186.483062 391.549679 \r\nL 207.473658 419.212901 \r\nL 228.464254 441.338361 \r\nL 249.454849 441.535929 \r\nL 270.445445 434.473244 \r\nL 291.43604 454.519057 \r\nL 312.426636 444.341153 \r\nL 333.417232 450.545387 \r\nL 354.407827 459.336173 \r\nL 375.398423 453.695164 \r\nL 396.389018 458.76687 \r\nL 417.379614 467.476087 \r\nL 438.37021 466.059703 \r\nL 459.360805 471.535768 \r\nL 480.351401 469.593245 \r\nL 501.341996 457.75595 \r\nL 522.332592 468.583346 \r\nL 543.323188 461.551372 \r\nL 564.313783 473.445093 \r\nL 585.304379 462.090985 \r\nL 606.294975 444.644433 \r\nL 627.28557 476.76466 \r\nL 648.276166 464.643766 \r\nL 669.266761 472.66394 \r\n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_19\">\r\n    <path clip-path=\"url(#pa4e062bd32)\" d=\"M 60.539489 93.108797 \r\nL 81.530084 78.703525 \r\nL 102.52068 77.847753 \r\nL 123.511275 69.718062 \r\nL 144.501871 76.706765 \r\nL 165.492467 68.577032 \r\nL 186.483062 78.846133 \r\nL 207.473658 70.573834 \r\nL 228.464254 59.734218 \r\nL 249.454849 61.445721 \r\nL 270.445445 62.158843 \r\nL 291.43604 56.168563 \r\nL 312.426636 62.016236 \r\nL 333.417232 57.30955 \r\nL 354.407827 53.458637 \r\nL 375.398423 56.168563 \r\nL 396.389018 54.884925 \r\nL 417.379614 53.030772 \r\nL 438.37021 55.45544 \r\nL 459.360805 51.034012 \r\nL 480.351401 54.171803 \r\nL 501.341996 57.024293 \r\nL 522.332592 53.886545 \r\nL 543.323188 56.025913 \r\nL 564.313783 50.035632 \r\nL 585.304379 58.735838 \r\nL 606.294975 62.729358 \r\nL 627.28557 49.465117 \r\nL 648.276166 52.460257 \r\nL 669.266761 53.030772 \r\n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 30.103125 550.8 \r\nL 30.103125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 699.703125 550.8 \r\nL 699.703125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 30.103125 550.8 \r\nL 699.703125 550.8 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 30.103125 7.2 \r\nL 699.703125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 37.103125 545.8 \r\nL 134.046875 545.8 \r\nQ 136.046875 545.8 136.046875 543.8 \r\nL 136.046875 485.53125 \r\nQ 136.046875 483.53125 134.046875 483.53125 \r\nL 37.103125 483.53125 \r\nQ 35.103125 483.53125 35.103125 485.53125 \r\nL 35.103125 543.8 \r\nQ 35.103125 545.8 37.103125 545.8 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_20\">\r\n     <path d=\"M 39.103125 491.629688 \r\nL 59.103125 491.629688 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_21\"/>\r\n    <g id=\"text_16\">\r\n     <!-- loss -->\r\n     <g transform=\"translate(67.103125 495.129688)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n       <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_22\">\r\n     <path d=\"M 39.103125 506.307813 \r\nL 59.103125 506.307813 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_23\"/>\r\n    <g id=\"text_17\">\r\n     <!-- accuracy -->\r\n     <g transform=\"translate(67.103125 509.807813)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n       <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n       <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n       <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n       <path d=\"M 32.171875 -5.078125 \r\nQ 28.375 -14.84375 24.75 -17.8125 \r\nQ 21.140625 -20.796875 15.09375 -20.796875 \r\nL 7.90625 -20.796875 \r\nL 7.90625 -13.28125 \r\nL 13.1875 -13.28125 \r\nQ 16.890625 -13.28125 18.9375 -11.515625 \r\nQ 21 -9.765625 23.484375 -3.21875 \r\nL 25.09375 0.875 \r\nL 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 11.921875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nz\r\n\" id=\"DejaVuSans-121\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"171.240234\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"234.619141\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"275.732422\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"337.011719\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"391.992188\" xlink:href=\"#DejaVuSans-121\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_24\">\r\n     <path d=\"M 39.103125 520.985938 \r\nL 59.103125 520.985938 \r\n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_25\"/>\r\n    <g id=\"text_18\">\r\n     <!-- val_loss -->\r\n     <g transform=\"translate(67.103125 524.485938)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n       <path d=\"M 50.984375 -16.609375 \r\nL 50.984375 -23.578125 \r\nL -0.984375 -23.578125 \r\nL -0.984375 -16.609375 \r\nz\r\n\" id=\"DejaVuSans-95\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"226.025391\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"287.207031\" xlink:href=\"#DejaVuSans-115\"/>\r\n      <use x=\"339.306641\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_26\">\r\n     <path d=\"M 39.103125 535.942188 \r\nL 59.103125 535.942188 \r\n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_27\"/>\r\n    <g id=\"text_19\">\r\n     <!-- val_accuracy -->\r\n     <g transform=\"translate(67.103125 539.442188)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"59.179688\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"120.458984\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-95\"/>\r\n      <use x=\"198.242188\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"259.521484\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"314.501953\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"369.482422\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"432.861328\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"473.974609\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"535.253906\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"590.234375\" xlink:href=\"#DejaVuSans-121\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pa4e062bd32\">\r\n   <rect height=\"543.6\" width=\"669.6\" x=\"30.103125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig,ax = plt.subplots(1,1,figsize = (12,10))\n",
    "ax.plot(df.index.values.tolist(),df[\"loss\"], label = \"loss\")\n",
    "ax.plot(df.index.values.tolist(),df[\"accuracy\"], label = \"accuracy\")\n",
    "ax.plot(df.index.values.tolist(),df[\"val_loss\"], label = \"val_loss\")\n",
    "ax.plot(df.index.values.tolist(),df[\"val_accuracy\"], label = \"val_accuracy\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 72.5884 - accuracy: 0.8308\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'loss': 72.58840942382812, 'accuracy': 0.8307999968528748}"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, return_dict=True)"
   ]
  },
  {
   "source": [
    "Making predictions using a model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "#y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([9, 2, 1], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "model.predict_classes(X_new)"
   ]
  },
  {
   "source": [
    "<h2> Building a Regression MLP using the Sequential API </h2> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using a regression neural network\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "x_train_full_reg,x_test_reg,y_train_full_reg,y_test_reg = train_test_split(housing.data, housing.target)\n",
    "\n",
    "\n",
    "x_train_reg, x_valid_reg, y_train_reg, y_valid_reg = train_test_split(x_train_full_reg, y_train_full_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "#notice that we fit_transform on the train set and transform on the validation and test sets\n",
    "x_train_reg_scaled = scaler.fit_transform(x_train_reg)\n",
    "x_valid_reg_scaled = scaler.transform(x_valid_reg)\n",
    "x_test_reg_scaled = scaler.transform(x_test_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reg = keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding input layer\n",
    "model_reg.add(keras.layers.InputLayer(input_shape = x_train_reg_scaled.shape[1:]))\n",
    "model_reg.add(keras.layers.Dense(30, activation = \"relu\"))\n",
    "#the output neuron has no activation function to it\n",
    "model_reg.add(keras.layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_3 (Dense)              (None, 30)                270       \n_________________________________________________________________\ndense_4 (Dense)              (None, 1)                 31        \n=================================================================\nTotal params: 301\nTrainable params: 301\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no need to always add the input_layer. We can always user keras.layers.Dense\n",
    "model_test = keras.models.Sequential([\n",
    "keras.layers.Dense(30, activation=\"relu\", input_shape=x_train_reg_scaled.shape[1:]),\n",
    "keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "x_train_reg_scaled.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_5 (Dense)              (None, 30)                270       \n_________________________________________________________________\ndense_6 (Dense)              (None, 1)                 31        \n=================================================================\nTotal params: 301\nTrainable params: 301\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_test.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reg.compile(loss = \"mean_squared_error\", optimizer = \"sgd\", metrics=tf.keras.metrics.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 3s 4ms/step - loss: 2.2604 - mean_squared_error: 2.2604 - val_loss: 4.3936 - val_mean_squared_error: 4.3936\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 160.7994 - mean_squared_error: 160.7994 - val_loss: 1.2169 - val_mean_squared_error: 1.2169\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.9052 - mean_squared_error: 0.9052 - val_loss: 9901.3223 - val_mean_squared_error: 9901.3223\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 2s 5ms/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: nan - mean_squared_error: nan - val_loss: nan - val_mean_squared_error: nan\n"
     ]
    }
   ],
   "source": [
    "\"\"\" for epoch in range(n_epochs):\n",
    "for i in range(m):\n",
    "random_index = np.random.randint(m)\n",
    "xi = X_b[random_index:random_index+1]\n",
    "yi = y[random_index:random_index+1]\n",
    "gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "eta = learning_schedule(epoch * m + i)\n",
    "theta = theta - eta * gradients \"\"\"\n",
    "\n",
    "#The above code shows how epochs work. For each epoch, It selects batches of instances and then applies the training for a stochastic instance in each batch.\n",
    "\n",
    "history = model_reg.fit(x_train_reg_scaled, y_train_reg, epochs = 20, validation_data = (x_valid_reg_scaled, y_valid_reg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'loss': [1.2538857460021973,\n",
       "  82.60726165771484,\n",
       "  3.6981916427612305,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan],\n",
       " 'mean_squared_error': [1.2538857460021973,\n",
       "  82.60726165771484,\n",
       "  3.6981916427612305,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan],\n",
       " 'val_loss': [4.393558025360107,\n",
       "  1.216890573501587,\n",
       "  9901.322265625,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan],\n",
       " 'val_mean_squared_error': [4.393558025360107,\n",
       "  1.216890573501587,\n",
       "  9901.322265625,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan]}"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test.compile(loss = \"mean_squared_error\", optimizer = \"sgd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.4031 - val_loss: 0.9046\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.2462 - val_loss: 0.5674\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4577 - val_loss: 0.4518\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4180 - val_loss: 0.4144\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4072 - val_loss: 0.4435\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4108 - val_loss: 0.5782\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4390 - val_loss: 0.3956\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4118 - val_loss: 0.3782\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3956 - val_loss: 0.3762\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4012 - val_loss: 0.3725\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3646 - val_loss: 0.3712\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3769 - val_loss: 0.3714\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 983us/step - loss: 0.3734 - val_loss: 0.3650\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3494 - val_loss: 0.3611\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3457 - val_loss: 0.3573\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3462 - val_loss: 0.3838\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3685 - val_loss: 0.3750\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3602 - val_loss: 0.3569\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3595 - val_loss: 0.3542\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3793 - val_loss: 0.3501\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model_test.fit(x_train_reg_scaled, y_train_reg, epochs = 20, validation_data = (x_valid_reg_scaled, y_valid_reg))"
   ]
  },
  {
   "source": [
    "<h3> Building complex models using the Functional API </h3> \n",
    "\n",
    "These are non sequential neural networks. This architecture allows a neural network to learn both deep patterns (using the deep path) and simple rules (through the short path)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for functional APIS\n",
    "#new layer = newlayerconfig(previous_layer)\n",
    "#create an Input opbject\n",
    "inputlayer = keras.layers.Input(shape = x_train_full_reg.shape[1:])\n",
    "#30 neuron dense layer using ReLU activation. Once created we call it like a function and pass in the input layer as a parameter\n",
    "hidden1 = keras.layers.Dense(30, activation = \"relu\")(inputlayer)\n",
    "#second hidden layer\n",
    "hidden2 = keras.layers.Dense(30, activation = \"relu\")(hidden1)\n",
    "#this concatenates the input and the output of the second layer\n",
    "concat = keras.layers.concatenate([inputlayer,hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.models.Model(inputs = [inputlayer], outputs = [output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_2 (InputLayer)            [(None, 8)]          0                                            \n__________________________________________________________________________________________________\ndense_7 (Dense)                 (None, 30)           270         input_2[0][0]                    \n__________________________________________________________________________________________________\ndense_8 (Dense)                 (None, 30)           930         dense_7[0][0]                    \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 38)           0           input_2[0][0]                    \n                                                                 dense_8[0][0]                    \n__________________________________________________________________________________________________\ndense_9 (Dense)                 (None, 1)            39          concatenate[0][0]                \n==================================================================================================\nTotal params: 1,239\nTrainable params: 1,239\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sending a subset of features through the wide path and a different subset through the deep path\n",
    "input_A = keras.layers.Input(shape = [5])\n",
    "input_B = keras.layers.Input(shape = [6])\n",
    "hidden1 = keras.layers.Dense(30, activation = 'relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation = \"relu\")(hidden1)\n",
    "concat_2 = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1)(concat_2)\n",
    "model = keras.models.Model(inputs = [input_A,input_B], outputs = [output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 2.0716 - mean_squared_error: 2.0716 - val_loss: 0.5896 - val_mean_squared_error: 0.5896\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7421 - mean_squared_error: 0.7421 - val_loss: 5.2934 - val_mean_squared_error: 5.2934\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 2.8195 - mean_squared_error: 2.8195 - val_loss: 0.5234 - val_mean_squared_error: 0.5234\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4834 - mean_squared_error: 0.4834 - val_loss: 0.4880 - val_mean_squared_error: 0.4880\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4201 - mean_squared_error: 0.4201 - val_loss: 0.4314 - val_mean_squared_error: 0.4314\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4021 - mean_squared_error: 0.4021 - val_loss: 0.4183 - val_mean_squared_error: 0.4183\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4152 - mean_squared_error: 0.4152 - val_loss: 0.4005 - val_mean_squared_error: 0.4005\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3897 - mean_squared_error: 0.3897 - val_loss: 0.4079 - val_mean_squared_error: 0.4079\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3958 - mean_squared_error: 0.3958 - val_loss: 0.3943 - val_mean_squared_error: 0.3943\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3754 - mean_squared_error: 0.3754 - val_loss: 0.3858 - val_mean_squared_error: 0.3858\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3740 - mean_squared_error: 0.3740 - val_loss: 0.3864 - val_mean_squared_error: 0.3864\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3506 - mean_squared_error: 0.3506 - val_loss: 0.3811 - val_mean_squared_error: 0.3811\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3537 - mean_squared_error: 0.3537 - val_loss: 0.3840 - val_mean_squared_error: 0.3840\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3474 - mean_squared_error: 0.3474 - val_loss: 0.3730 - val_mean_squared_error: 0.3730\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3444 - mean_squared_error: 0.3444 - val_loss: 0.3705 - val_mean_squared_error: 0.3705\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3374 - mean_squared_error: 0.3374 - val_loss: 0.3836 - val_mean_squared_error: 0.3836\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3547 - mean_squared_error: 0.3547 - val_loss: 0.3675 - val_mean_squared_error: 0.3675\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3395 - mean_squared_error: 0.3395 - val_loss: 0.3579 - val_mean_squared_error: 0.3579\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3370 - mean_squared_error: 0.3370 - val_loss: 0.3618 - val_mean_squared_error: 0.3618\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3475 - mean_squared_error: 0.3475 - val_loss: 0.3674 - val_mean_squared_error: 0.3674\n",
      "162/162 [==============================] - 0s 657us/step - loss: 0.3641 - mean_squared_error: 0.3641\n"
     ]
    }
   ],
   "source": [
    "#instead of passing a single input matrix, we need to input 2 different matrices. \n",
    "#using a regression neural network\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "x_train_full_reg,x_test_reg,y_train_full_reg,y_test_reg = train_test_split(housing.data, housing.target)\n",
    "\n",
    "\n",
    "x_train_reg, x_valid_reg, y_train_reg, y_valid_reg = train_test_split(x_train_full_reg, y_train_full_reg)\n",
    "scaler = StandardScaler()\n",
    "#notice that we fit_transform on the train set and transform on the validation and test sets\n",
    "x_train_reg = scaler.fit_transform(x_train_reg)\n",
    "x_valid_reg = scaler.transform(x_valid_reg)\n",
    "x_test_reg= scaler.transform(x_test_reg)\n",
    "\n",
    "model.compile(loss = \"mse\", optimizer = \"sgd\", metrics= tf.keras.metrics.MeanSquaredError())\n",
    "\n",
    "X_train_A, X_train_B = x_train_reg[:,:5],x_train_reg[:,2:]\n",
    "X_valid_A, X_valid_B = x_valid_reg[:,:5],x_valid_reg[:,2:]\n",
    "X_test_A,X_test_B = x_test_reg[:,:5],x_test_reg[:,2:]\n",
    "X_new_A,X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "model.fit((X_train_A,X_train_B),y_train_reg, epochs = 20, validation_data = ((X_valid_A, X_valid_B),y_valid_reg))\n",
    "\n",
    "mse_test = model.evaluate((X_test_A,X_test_B),y_test_reg)\n",
    "\n",
    "y_pred_test = model.predict((X_test_A, X_test_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1.5185499],\n",
       "       [2.981575 ],\n",
       "       [2.023159 ],\n",
       "       ...,\n",
       "       [1.2162044],\n",
       "       [1.361015 ],\n",
       "       [1.8748822]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "y_pred_test"
   ]
  },
  {
   "source": [
    "<h3> There can be situations in which you want multiple outputs </h3> \n",
    "\n",
    "- Locating an object in a picture (regression to find coordinates and classification to classify image)\n",
    "\n",
    "- Multiple independent tasks to perform based on the same data. Train one neural network per task is possible, but this is inefficient. Also training one neural network with mutltiple outputs for the entire set of tasks allows higher accuracy too. This is because the neural network can learn data useful across the tasks. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multioutput\n",
    "input_A_multi = keras.layers.Input(shape = [5])\n",
    "input_B_mutli = keras.layers.Input(shape = [6])\n",
    "hidden1_multi = keras.layers.Dense(30, activation = 'relu')(input_B_mutli)\n",
    "hidden2_multi = keras.layers.Dense(30, activation = 'relu')(hidden1_multi)\n",
    "concat_multi = keras.layers.concatenate([input_A_multi, hidden2_multi])\n",
    "output1 = keras.layers.Dense(1)(concat_multi)\n",
    "output_aux = keras.layers.Dense(1)(hidden1_multi)\n",
    "model_multi = keras.models.Model(inputs = [input_A_multi,input_B_mutli], outputs = [output1,output_aux])"
   ]
  },
  {
   "source": [
    "For multi output neural networks, each output requires its own loss function. If one is passed, keras assumes its the same for all."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usually different outputs will be given different weights\n",
    "model_multi.compile(loss = ['mse','mse'],loss_weights = [0.9,0.1],optimizer = \"sgd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.7281 - dense_15_loss: 1.5138 - dense_16_loss: 3.6569 - val_loss: 1.3791 - val_dense_15_loss: 1.3791 - val_dense_16_loss: 1.3787\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 2.8989 - dense_15_loss: 3.0781 - dense_16_loss: 1.2863 - val_loss: 1.1918 - val_dense_15_loss: 1.1918 - val_dense_16_loss: 1.1919\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7037 - dense_15_loss: 0.6530 - dense_16_loss: 1.1602 - val_loss: 0.5870 - val_dense_15_loss: 0.5360 - val_dense_16_loss: 1.0459\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5608 - dense_15_loss: 0.5104 - dense_16_loss: 1.0143 - val_loss: 0.5135 - val_dense_15_loss: 0.4647 - val_dense_16_loss: 0.9534\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5013 - dense_15_loss: 0.4535 - dense_16_loss: 0.9321 - val_loss: 0.4857 - val_dense_15_loss: 0.4419 - val_dense_16_loss: 0.8805\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4802 - dense_15_loss: 0.4379 - dense_16_loss: 0.8612 - val_loss: 0.4582 - val_dense_15_loss: 0.4165 - val_dense_16_loss: 0.8334\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4668 - dense_15_loss: 0.4284 - dense_16_loss: 0.8131 - val_loss: 0.4482 - val_dense_15_loss: 0.4098 - val_dense_16_loss: 0.7946\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4544 - dense_15_loss: 0.4199 - dense_16_loss: 0.7652 - val_loss: 0.4449 - val_dense_15_loss: 0.4088 - val_dense_16_loss: 0.7699\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4409 - dense_15_loss: 0.4061 - dense_16_loss: 0.7546 - val_loss: 0.4461 - val_dense_15_loss: 0.4090 - val_dense_16_loss: 0.7798\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4183 - dense_15_loss: 0.3845 - dense_16_loss: 0.7220 - val_loss: 0.4319 - val_dense_15_loss: 0.3970 - val_dense_16_loss: 0.7457\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4174 - dense_15_loss: 0.3845 - dense_16_loss: 0.7141 - val_loss: 0.4228 - val_dense_15_loss: 0.3877 - val_dense_16_loss: 0.7390\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3862 - dense_15_loss: 0.3552 - dense_16_loss: 0.6652 - val_loss: 0.4892 - val_dense_15_loss: 0.4566 - val_dense_16_loss: 0.7831\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3931 - dense_15_loss: 0.3602 - dense_16_loss: 0.6897 - val_loss: 0.4174 - val_dense_15_loss: 0.3814 - val_dense_16_loss: 0.7412\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3921 - dense_15_loss: 0.3602 - dense_16_loss: 0.6790 - val_loss: 0.4796 - val_dense_15_loss: 0.4555 - val_dense_16_loss: 0.6962\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3925 - dense_15_loss: 0.3630 - dense_16_loss: 0.6580 - val_loss: 0.4065 - val_dense_15_loss: 0.3737 - val_dense_16_loss: 0.7018\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3981 - dense_15_loss: 0.3674 - dense_16_loss: 0.6748 - val_loss: 0.4044 - val_dense_15_loss: 0.3678 - val_dense_16_loss: 0.7336\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3817 - dense_15_loss: 0.3514 - dense_16_loss: 0.6543 - val_loss: 0.4016 - val_dense_15_loss: 0.3678 - val_dense_16_loss: 0.7053\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3873 - dense_15_loss: 0.3572 - dense_16_loss: 0.6583 - val_loss: 0.4094 - val_dense_15_loss: 0.3773 - val_dense_16_loss: 0.6978\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3764 - dense_15_loss: 0.3476 - dense_16_loss: 0.6359 - val_loss: 0.3949 - val_dense_15_loss: 0.3602 - val_dense_16_loss: 0.7071\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3657 - dense_15_loss: 0.3359 - dense_16_loss: 0.6341 - val_loss: 0.4290 - val_dense_15_loss: 0.3990 - val_dense_16_loss: 0.6996\n"
     ]
    }
   ],
   "source": [
    "#as we are using auxillary outputs, we will need to pass in auxillary output labels too\n",
    "history = model_multi.fit([X_train_A,X_train_B],[y_train_reg,y_train_reg], epochs = 20, validation_data= ([X_valid_A, X_valid_B],[y_valid_reg, y_valid_reg]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "162/162 [==============================] - 0s 855us/step - loss: 0.4091 - dense_15_loss: 0.3823 - dense_16_loss: 0.6503\n"
     ]
    }
   ],
   "source": [
    "#if you have more than one output, you will have more than one loss\n",
    "total_loss, main_loss, aux_loss = model_multi.evaluate([X_test_A,X_test_B],[y_test_reg,y_test_reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.38230040669441223"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "main_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_main, y_pred_aux = model_multi.predict([X_test_A,X_test_B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x24481e37910>"
      ]
     },
     "metadata": {},
     "execution_count": 53
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 576x720 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"574.678125pt\" version=\"1.1\" viewBox=\"0 0 500.565625 574.678125\" width=\"500.565625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-04-10T11:33:44.751907</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 574.678125 \r\nL 500.565625 574.678125 \r\nL 500.565625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 46.965625 550.8 \r\nL 493.365625 550.8 \r\nL 493.365625 7.2 \r\nL 46.965625 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 67.256534 550.8 \r\nL 73.40529 550.8 \r\nL 73.40529 549.433999 \r\nL 67.256534 549.433999 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 73.40529 550.8 \r\nL 79.554055 550.8 \r\nL 79.554055 550.8 \r\nL 73.40529 550.8 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 79.554055 550.8 \r\nL 85.702811 550.8 \r\nL 85.702811 550.8 \r\nL 79.554055 550.8 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 85.702811 550.8 \r\nL 91.851577 550.8 \r\nL 91.851577 550.8 \r\nL 85.702811 550.8 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_7\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 91.851577 550.8 \r\nL 98.000332 550.8 \r\nL 98.000332 550.8 \r\nL 91.851577 550.8 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_8\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 98.000332 550.8 \r\nL 104.149098 550.8 \r\nL 104.149098 550.8 \r\nL 98.000332 550.8 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_9\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 104.149098 550.8 \r\nL 110.297854 550.8 \r\nL 110.297854 550.8 \r\nL 104.149098 550.8 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_10\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 110.297854 550.8 \r\nL 116.446619 550.8 \r\nL 116.446619 550.8 \r\nL 110.297854 550.8 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_11\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 116.446619 550.8 \r\nL 122.595375 550.8 \r\nL 122.595375 549.433999 \r\nL 116.446619 549.433999 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_12\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 122.595375 550.8 \r\nL 128.744136 550.8 \r\nL 128.744136 549.433999 \r\nL 122.595375 549.433999 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_13\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 128.744136 550.8 \r\nL 134.892896 550.8 \r\nL 134.892896 550.8 \r\nL 128.744136 550.8 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_14\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 134.892896 550.8 \r\nL 141.041657 550.8 \r\nL 141.041657 548.067998 \r\nL 134.892896 548.067998 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_15\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 141.041657 550.8 \r\nL 147.190417 550.8 \r\nL 147.190417 550.8 \r\nL 141.041657 550.8 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_16\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 147.190417 550.8 \r\nL 153.339178 550.8 \r\nL 153.339178 550.8 \r\nL 147.190417 550.8 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_17\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 153.339178 550.8 \r\nL 159.487939 550.8 \r\nL 159.487939 550.8 \r\nL 153.339178 550.8 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_18\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 159.487939 550.8 \r\nL 165.636699 550.8 \r\nL 165.636699 550.8 \r\nL 159.487939 550.8 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_19\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 165.636699 550.8 \r\nL 171.78546 550.8 \r\nL 171.78546 548.067998 \r\nL 165.636699 548.067998 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_20\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 171.78546 550.8 \r\nL 177.93422 550.8 \r\nL 177.93422 550.8 \r\nL 171.78546 550.8 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_21\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 177.93422 550.8 \r\nL 184.08298 550.8 \r\nL 184.08298 549.433999 \r\nL 177.93422 549.433999 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_22\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 184.08298 550.8 \r\nL 190.231741 550.8 \r\nL 190.231741 549.433999 \r\nL 184.08298 549.433999 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_23\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 190.231741 550.8 \r\nL 196.380501 550.8 \r\nL 196.380501 546.701998 \r\nL 190.231741 546.701998 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_24\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 196.380501 550.8 \r\nL 202.529261 550.8 \r\nL 202.529261 538.505993 \r\nL 196.380501 538.505993 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_25\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 202.529261 550.8 \r\nL 208.678022 550.8 \r\nL 208.678022 524.845986 \r\nL 202.529261 524.845986 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_26\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 208.678022 550.8 \r\nL 214.826782 550.8 \r\nL 214.826782 509.819977 \r\nL 208.678022 509.819977 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_27\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 214.826782 550.8 \r\nL 220.975542 550.8 \r\nL 220.975542 468.839955 \r\nL 214.826782 468.839955 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_28\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 220.975542 550.8 \r\nL 227.124303 550.8 \r\nL 227.124303 422.395929 \r\nL 220.975542 422.395929 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_29\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 227.124303 550.8 \r\nL 233.273064 550.8 \r\nL 233.273064 325.409876 \r\nL 227.124303 325.409876 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_30\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 233.273064 550.8 \r\nL 239.421824 550.8 \r\nL 239.421824 199.737806 \r\nL 233.273064 199.737806 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_31\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 239.421824 550.8 \r\nL 245.570583 550.8 \r\nL 245.570583 150.561779 \r\nL 239.421824 150.561779 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_32\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 245.570583 550.8 \r\nL 251.719343 550.8 \r\nL 251.719343 158.757784 \r\nL 245.570583 158.757784 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_33\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 251.719343 550.8 \r\nL 257.868104 550.8 \r\nL 257.868104 91.823747 \r\nL 251.719343 91.823747 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_34\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 257.868104 550.8 \r\nL 264.016864 550.8 \r\nL 264.016864 91.823747 \r\nL 257.868104 91.823747 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_35\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 264.016864 550.8 \r\nL 270.165625 550.8 \r\nL 270.165625 61.77173 \r\nL 264.016864 61.77173 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_36\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 270.165625 550.8 \r\nL 276.314386 550.8 \r\nL 276.314386 48.111723 \r\nL 270.165625 48.111723 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_37\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 276.314386 550.8 \r\nL 282.463142 550.8 \r\nL 282.463142 183.345797 \r\nL 276.314386 183.345797 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_38\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 282.463142 550.8 \r\nL 288.611907 550.8 \r\nL 288.611907 181.979796 \r\nL 282.463142 181.979796 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_39\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 288.611907 550.8 \r\nL 294.760663 550.8 \r\nL 294.760663 209.299812 \r\nL 288.611907 209.299812 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_40\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 294.760663 550.8 \r\nL 300.909428 550.8 \r\nL 300.909428 206.56781 \r\nL 294.760663 206.56781 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_41\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 300.909428 550.8 \r\nL 307.058184 550.8 \r\nL 307.058184 239.351828 \r\nL 300.909428 239.351828 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_42\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 307.058184 550.8 \r\nL 313.206949 550.8 \r\nL 313.206949 259.841839 \r\nL 307.058184 259.841839 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_43\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 313.206949 550.8 \r\nL 319.355705 550.8 \r\nL 319.355705 284.429853 \r\nL 313.206949 284.429853 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_44\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 319.355705 550.8 \r\nL 325.504471 550.8 \r\nL 325.504471 332.239879 \r\nL 319.355705 332.239879 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_45\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 325.504471 550.8 \r\nL 331.653226 550.8 \r\nL 331.653226 325.409876 \r\nL 325.504471 325.409876 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_46\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 331.653226 550.8 \r\nL 337.801992 550.8 \r\nL 337.801992 399.173916 \r\nL 331.653226 399.173916 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_47\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 337.801992 550.8 \r\nL 343.950748 550.8 \r\nL 343.950748 442.88594 \r\nL 337.801992 442.88594 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_48\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 343.950748 550.8 \r\nL 350.099513 550.8 \r\nL 350.099513 459.277949 \r\nL 343.950748 459.277949 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_49\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 350.099513 550.8 \r\nL 356.248269 550.8 \r\nL 356.248269 489.329966 \r\nL 350.099513 489.329966 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_50\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 356.248269 550.8 \r\nL 362.397034 550.8 \r\nL 362.397034 482.499962 \r\nL 356.248269 482.499962 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_51\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 362.397034 550.8 \r\nL 368.5458 550.8 \r\nL 368.5458 497.525971 \r\nL 362.397034 497.525971 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_52\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 368.5458 550.8 \r\nL 374.694546 550.8 \r\nL 374.694546 520.747983 \r\nL 368.5458 520.747983 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_53\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 374.694546 550.8 \r\nL 380.843311 550.8 \r\nL 380.843311 518.015982 \r\nL 374.694546 518.015982 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_54\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 380.843311 550.8 \r\nL 386.992077 550.8 \r\nL 386.992077 515.28398 \r\nL 380.843311 515.28398 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_55\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 386.992077 550.8 \r\nL 393.140823 550.8 \r\nL 393.140823 541.237995 \r\nL 386.992077 541.237995 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_56\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 393.140823 550.8 \r\nL 399.289589 550.8 \r\nL 399.289589 537.139992 \r\nL 393.140823 537.139992 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_57\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 399.289589 550.8 \r\nL 405.438354 550.8 \r\nL 405.438354 539.871994 \r\nL 399.289589 539.871994 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_58\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 405.438354 550.8 \r\nL 411.587119 550.8 \r\nL 411.587119 537.139992 \r\nL 405.438354 537.139992 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_59\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 411.587119 550.8 \r\nL 417.735866 550.8 \r\nL 417.735866 534.407991 \r\nL 411.587119 534.407991 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_60\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 417.735866 550.8 \r\nL 423.884631 550.8 \r\nL 423.884631 548.067998 \r\nL 417.735866 548.067998 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_61\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 423.884631 550.8 \r\nL 430.033396 550.8 \r\nL 430.033396 546.701998 \r\nL 423.884631 546.701998 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_62\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 430.033396 550.8 \r\nL 436.182162 550.8 \r\nL 436.182162 545.335997 \r\nL 430.033396 545.335997 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_63\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 436.182162 550.8 \r\nL 442.330908 550.8 \r\nL 442.330908 548.067998 \r\nL 436.182162 548.067998 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_64\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 442.330908 550.8 \r\nL 448.479673 550.8 \r\nL 448.479673 543.969996 \r\nL 442.330908 543.969996 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_65\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 448.479673 550.8 \r\nL 454.628439 550.8 \r\nL 454.628439 543.969996 \r\nL 448.479673 543.969996 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_66\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 454.628439 550.8 \r\nL 460.777204 550.8 \r\nL 460.777204 550.8 \r\nL 454.628439 550.8 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_67\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 460.777204 550.8 \r\nL 466.925951 550.8 \r\nL 466.925951 548.067998 \r\nL 460.777204 548.067998 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_68\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 466.925951 550.8 \r\nL 473.074716 550.8 \r\nL 473.074716 549.433999 \r\nL 466.925951 549.433999 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n   </g>\r\n   <g id=\"patch_69\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 208.596453 550.8 \r\nL 214.973602 550.8 \r\nL 214.973602 543.969996 \r\nL 208.596453 543.969996 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_70\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 214.973602 550.8 \r\nL 221.35075 550.8 \r\nL 221.35075 501.623973 \r\nL 214.973602 501.623973 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_71\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 221.35075 550.8 \r\nL 227.727898 550.8 \r\nL 227.727898 283.063852 \r\nL 221.35075 283.063852 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_72\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 227.727898 550.8 \r\nL 234.105046 550.8 \r\nL 234.105046 183.345797 \r\nL 227.727898 183.345797 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_73\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 234.105046 550.8 \r\nL 240.482194 550.8 \r\nL 240.482194 79.52974 \r\nL 234.105046 79.52974 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_74\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 240.482194 550.8 \r\nL 246.859343 550.8 \r\nL 246.859343 145.097776 \r\nL 240.482194 145.097776 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_75\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 246.859343 550.8 \r\nL 253.236491 550.8 \r\nL 253.236491 110.947757 \r\nL 246.859343 110.947757 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_76\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 253.236491 550.8 \r\nL 259.613639 550.8 \r\nL 259.613639 67.235733 \r\nL 253.236491 67.235733 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_77\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 259.613639 550.8 \r\nL 265.990787 550.8 \r\nL 265.990787 48.111723 \r\nL 259.613639 48.111723 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_78\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 265.990787 550.8 \r\nL 272.367935 550.8 \r\nL 272.367935 33.085714 \r\nL 265.990787 33.085714 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_79\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 272.367935 550.8 \r\nL 278.745084 550.8 \r\nL 278.745084 110.947757 \r\nL 272.367935 110.947757 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_80\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 278.745084 550.8 \r\nL 285.122232 550.8 \r\nL 285.122232 227.057821 \r\nL 278.745084 227.057821 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_81\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 285.122232 550.8 \r\nL 291.49938 550.8 \r\nL 291.49938 183.345797 \r\nL 285.122232 183.345797 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_82\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 291.49938 550.8 \r\nL 297.876528 550.8 \r\nL 297.876528 246.181832 \r\nL 291.49938 246.181832 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_83\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 297.876528 550.8 \r\nL 304.253676 550.8 \r\nL 304.253676 304.919864 \r\nL 297.876528 304.919864 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_84\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 304.253676 550.8 \r\nL 310.630825 550.8 \r\nL 310.630825 315.84787 \r\nL 304.253676 315.84787 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_85\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 310.630825 550.8 \r\nL 317.007973 550.8 \r\nL 317.007973 345.899887 \r\nL 310.630825 345.899887 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_86\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 317.007973 550.8 \r\nL 323.385121 550.8 \r\nL 323.385121 416.931926 \r\nL 317.007973 416.931926 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_87\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 323.385121 550.8 \r\nL 329.762269 550.8 \r\nL 329.762269 427.859932 \r\nL 323.385121 427.859932 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_88\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 329.762269 550.8 \r\nL 336.139417 550.8 \r\nL 336.139417 400.539917 \r\nL 329.762269 400.539917 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_89\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 336.139417 550.8 \r\nL 342.516565 550.8 \r\nL 342.516565 392.343913 \r\nL 336.139417 392.343913 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_90\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 342.516565 550.8 \r\nL 348.893714 550.8 \r\nL 348.893714 434.689936 \r\nL 342.516565 434.689936 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_91\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 348.893714 550.8 \r\nL 355.270862 550.8 \r\nL 355.270862 482.499962 \r\nL 348.893714 482.499962 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_92\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 355.270862 550.8 \r\nL 361.64801 550.8 \r\nL 361.64801 471.571956 \r\nL 355.270862 471.571956 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_93\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 361.64801 550.8 \r\nL 368.025158 550.8 \r\nL 368.025158 492.061968 \r\nL 361.64801 492.061968 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_94\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 368.025158 550.8 \r\nL 374.402306 550.8 \r\nL 374.402306 502.989974 \r\nL 368.025158 502.989974 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_95\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 374.402306 550.8 \r\nL 380.779455 550.8 \r\nL 380.779455 500.257972 \r\nL 374.402306 500.257972 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_96\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 380.779455 550.8 \r\nL 387.156603 550.8 \r\nL 387.156603 516.649981 \r\nL 380.779455 516.649981 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_97\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 387.156603 550.8 \r\nL 393.533751 550.8 \r\nL 393.533751 519.381983 \r\nL 387.156603 519.381983 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"patch_98\">\r\n    <path clip-path=\"url(#p53c580de68)\" d=\"M 393.533751 550.8 \r\nL 399.910899 550.8 \r\nL 399.910899 187.443799 \r\nL 393.533751 187.443799 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m9a7334dea6\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"123.787701\" xlink:href=\"#m9a7334dea6\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- −2 -->\r\n      <g transform=\"translate(116.416607 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.59375 35.5 \r\nL 73.1875 35.5 \r\nL 73.1875 27.203125 \r\nL 10.59375 27.203125 \r\nz\r\n\" id=\"DejaVuSans-8722\"/>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-8722\"/>\r\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"202.679931\" xlink:href=\"#m9a7334dea6\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(199.498681 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"281.57216\" xlink:href=\"#m9a7334dea6\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 2 -->\r\n      <g transform=\"translate(278.39091 565.398438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"360.46439\" xlink:href=\"#m9a7334dea6\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 4 -->\r\n      <g transform=\"translate(357.28314 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"439.35662\" xlink:href=\"#m9a7334dea6\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 6 -->\r\n      <g transform=\"translate(436.17537 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_6\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m5795a44734\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m5795a44734\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(33.603125 554.599219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m5795a44734\" y=\"482.499962\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(27.240625 486.299181)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m5795a44734\" y=\"414.199925\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(20.878125 417.999143)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m5795a44734\" y=\"345.899887\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 150 -->\r\n      <g transform=\"translate(20.878125 349.699106)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m5795a44734\" y=\"277.599849\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(20.878125 281.399068)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m5795a44734\" y=\"209.299812\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 250 -->\r\n      <g transform=\"translate(20.878125 213.09903)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m5795a44734\" y=\"140.999774\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 300 -->\r\n      <g transform=\"translate(20.878125 144.798993)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m5795a44734\" y=\"72.699736\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 350 -->\r\n      <g transform=\"translate(20.878125 76.498955)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_14\">\r\n     <!-- Count -->\r\n     <g transform=\"translate(14.798437 293.848437)rotate(-90)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 64.40625 67.28125 \r\nL 64.40625 56.890625 \r\nQ 59.421875 61.53125 53.78125 63.8125 \r\nQ 48.140625 66.109375 41.796875 66.109375 \r\nQ 29.296875 66.109375 22.65625 58.46875 \r\nQ 16.015625 50.828125 16.015625 36.375 \r\nQ 16.015625 21.96875 22.65625 14.328125 \r\nQ 29.296875 6.6875 41.796875 6.6875 \r\nQ 48.140625 6.6875 53.78125 8.984375 \r\nQ 59.421875 11.28125 64.40625 15.921875 \r\nL 64.40625 5.609375 \r\nQ 59.234375 2.09375 53.4375 0.328125 \r\nQ 47.65625 -1.421875 41.21875 -1.421875 \r\nQ 24.65625 -1.421875 15.125 8.703125 \r\nQ 5.609375 18.84375 5.609375 36.375 \r\nQ 5.609375 53.953125 15.125 64.078125 \r\nQ 24.65625 74.21875 41.21875 74.21875 \r\nQ 47.75 74.21875 53.53125 72.484375 \r\nQ 59.328125 70.75 64.40625 67.28125 \r\nz\r\n\" id=\"DejaVuSans-67\"/>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n       <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n       <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-67\"/>\r\n      <use x=\"69.824219\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"131.005859\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"194.384766\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"257.763672\" xlink:href=\"#DejaVuSans-116\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_99\">\r\n    <path d=\"M 46.965625 550.8 \r\nL 46.965625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_100\">\r\n    <path d=\"M 493.365625 550.8 \r\nL 493.365625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_101\">\r\n    <path d=\"M 46.965625 550.8 \r\nL 493.365625 550.8 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_102\">\r\n    <path d=\"M 46.965625 7.2 \r\nL 493.365625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_103\">\r\n     <path d=\"M 419.926562 44.55625 \r\nL 486.365625 44.55625 \r\nQ 488.365625 44.55625 488.365625 42.55625 \r\nL 488.365625 14.2 \r\nQ 488.365625 12.2 486.365625 12.2 \r\nL 419.926562 12.2 \r\nQ 417.926562 12.2 417.926562 14.2 \r\nL 417.926562 42.55625 \r\nQ 417.926562 44.55625 419.926562 44.55625 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"patch_104\">\r\n     <path d=\"M 421.926562 23.798437 \r\nL 441.926562 23.798437 \r\nL 441.926562 16.798437 \r\nL 421.926562 16.798437 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.614875;\"/>\r\n    </g>\r\n    <g id=\"text_15\">\r\n     <!-- Predict -->\r\n     <g transform=\"translate(449.926562 23.798437)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 19.671875 64.796875 \r\nL 19.671875 37.40625 \r\nL 32.078125 37.40625 \r\nQ 38.96875 37.40625 42.71875 40.96875 \r\nQ 46.484375 44.53125 46.484375 51.125 \r\nQ 46.484375 57.671875 42.71875 61.234375 \r\nQ 38.96875 64.796875 32.078125 64.796875 \r\nz\r\nM 9.8125 72.90625 \r\nL 32.078125 72.90625 \r\nQ 44.34375 72.90625 50.609375 67.359375 \r\nQ 56.890625 61.8125 56.890625 51.125 \r\nQ 56.890625 40.328125 50.609375 34.8125 \r\nQ 44.34375 29.296875 32.078125 29.296875 \r\nL 19.671875 29.296875 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-80\"/>\r\n       <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n       <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n       <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n       <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n       <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-80\"/>\r\n      <use x=\"58.552734\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"97.416016\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"158.939453\" xlink:href=\"#DejaVuSans-100\"/>\r\n      <use x=\"222.416016\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"250.199219\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"305.179688\" xlink:href=\"#DejaVuSans-116\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"patch_105\">\r\n     <path d=\"M 421.926562 38.476562 \r\nL 441.926562 38.476562 \r\nL 441.926562 31.476562 \r\nL 421.926562 31.476562 \r\nz\r\n\" style=\"fill:#0000ff;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.637715;\"/>\r\n    </g>\r\n    <g id=\"text_16\">\r\n     <!-- Actual -->\r\n     <g transform=\"translate(449.926562 38.476562)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 34.1875 63.1875 \r\nL 20.796875 26.90625 \r\nL 47.609375 26.90625 \r\nz\r\nM 28.609375 72.90625 \r\nL 39.796875 72.90625 \r\nL 67.578125 0 \r\nL 57.328125 0 \r\nL 50.6875 18.703125 \r\nL 17.828125 18.703125 \r\nL 11.1875 0 \r\nL 0.78125 0 \r\nz\r\n\" id=\"DejaVuSans-65\"/>\r\n       <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n       <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"66.658203\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"121.638672\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"160.847656\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"224.226562\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"285.505859\" xlink:href=\"#DejaVuSans-108\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p53c580de68\">\r\n   <rect height=\"543.6\" width=\"446.4\" x=\"46.965625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "fig,ax_multi = plt.subplots(1,1,figsize = (8,10))\n",
    "sns.histplot(y_pred_main, ax = ax_multi, color = 'red', label = 'Predict')\n",
    "sns.histplot(y_test_reg, ax = ax_multi, color = 'blue', label = 'Actual')\n",
    "ax_multi.legend()"
   ]
  },
  {
   "source": [
    "<h2> Subclassing API </h2> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The sequential and functional APIs are easy to save, clone, its structure can be displayed and analyszed, its framework can infer shapes and check types, errors can be caught early. This is because the model is a static graph of layers. \n",
    "\n",
    "However as it is a static graph of layers, it cannot train models involving loops, varying shapes, conditional branching and other dynamic behaviors. \n",
    "\n",
    "Instead we use the Subclass API"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.models.Model):\n",
    "    def __init__(self, units = 30, activation = \"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        #only instantiates the layers\n",
    "        self.hidden1 = keras.layers.Dense(units, activation = activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation = activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output,aux_output"
   ]
  },
  {
   "source": [
    "Notice that now we have separated layer initialization from layer architecture. Hence we can call this function with the respective inputs whenever required during normal programming operations.\n",
    "\n",
    "However now your model's architecture is hidden within the call() method. Hence we cannot clone or save the model. And we cannot inspect the summary of the model. Hence Sequential API and Functional API are safer options"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving a Keras model\n",
    "model_multi.save(\"multi_output_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['model_weights', 'optimizer_weights']>"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "import h5py\n",
    "model_h5 = h5py.File('multi_output_model.h5','r')\n",
    "model_h5.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<HDF5 group \"/model_weights\" (7 members)>"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "model_h5['model_weights']"
   ]
  },
  {
   "source": [
    "There are 2 ways in which you can load a model. Either by loading the entire model or just the weights. Use load_weights to load only the weights. However here we have to create a model architecture which matches that of the weights."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\ndef create_model():\\n    .....model architecture...\\n\\nmodel = create_model()\\nmodel.load_weights(weights_path/h5 file name)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "\"\"\"\n",
    "def create_model():\n",
    "    .....model architecture...\n",
    "\n",
    "model = create_model()\n",
    "model.load_weights(weights_path/h5 file name)\n",
    "\"\"\""
   ]
  },
  {
   "source": [
    "Training a model can take several hours sometimes. It is important to ensure that you do not loose progress by not saving. Hence we have to save at checkpoints at regular intervals during training. For this we use callbacks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h3> Using Callbacks </h3> \n",
    "\n",
    "The fit() method has a callback argument which accepts a keras.callbacks object which specifies when the model should save. We can specify whether to save after each epoch, each batch or end of training. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3767 - dense_15_loss: 0.3479 - dense_16_loss: 0.6362 - val_loss: 0.3929 - val_dense_15_loss: 0.3597 - val_dense_16_loss: 0.6917\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3898 - dense_15_loss: 0.3627 - dense_16_loss: 0.6335 - val_loss: 0.3883 - val_dense_15_loss: 0.3568 - val_dense_16_loss: 0.6725\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3690 - dense_15_loss: 0.3404 - dense_16_loss: 0.6267 - val_loss: 0.3923 - val_dense_15_loss: 0.3571 - val_dense_16_loss: 0.7096\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3655 - dense_15_loss: 0.3367 - dense_16_loss: 0.6246 - val_loss: 0.3889 - val_dense_15_loss: 0.3541 - val_dense_16_loss: 0.7021\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3672 - dense_15_loss: 0.3390 - dense_16_loss: 0.6215 - val_loss: 0.3860 - val_dense_15_loss: 0.3525 - val_dense_16_loss: 0.6867\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3616 - dense_15_loss: 0.3330 - dense_16_loss: 0.6196 - val_loss: 0.3858 - val_dense_15_loss: 0.3514 - val_dense_16_loss: 0.6959\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3597 - dense_15_loss: 0.3309 - dense_16_loss: 0.6185 - val_loss: 0.3856 - val_dense_15_loss: 0.3520 - val_dense_16_loss: 0.6878\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3582 - dense_15_loss: 0.3293 - dense_16_loss: 0.6179 - val_loss: 0.3847 - val_dense_15_loss: 0.3503 - val_dense_16_loss: 0.6945\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3575 - dense_15_loss: 0.3289 - dense_16_loss: 0.6148 - val_loss: 0.3827 - val_dense_15_loss: 0.3480 - val_dense_16_loss: 0.6946\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3575 - dense_15_loss: 0.3291 - dense_16_loss: 0.6130 - val_loss: 0.3807 - val_dense_15_loss: 0.3485 - val_dense_16_loss: 0.6699\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3549 - dense_15_loss: 0.3265 - dense_16_loss: 0.6102 - val_loss: 0.3841 - val_dense_15_loss: 0.3466 - val_dense_16_loss: 0.7215\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3553 - dense_15_loss: 0.3267 - dense_16_loss: 0.6128 - val_loss: 0.3808 - val_dense_15_loss: 0.3456 - val_dense_16_loss: 0.6977\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3551 - dense_15_loss: 0.3269 - dense_16_loss: 0.6093 - val_loss: 0.3955 - val_dense_15_loss: 0.3616 - val_dense_16_loss: 0.7002\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3527 - dense_15_loss: 0.3245 - dense_16_loss: 0.6066 - val_loss: 0.3796 - val_dense_15_loss: 0.3440 - val_dense_16_loss: 0.7003\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3503 - dense_15_loss: 0.3219 - dense_16_loss: 0.6060 - val_loss: 0.3889 - val_dense_15_loss: 0.3558 - val_dense_16_loss: 0.6867\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3496 - dense_15_loss: 0.3214 - dense_16_loss: 0.6038 - val_loss: 0.4070 - val_dense_15_loss: 0.3782 - val_dense_16_loss: 0.6661\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3499 - dense_15_loss: 0.3213 - dense_16_loss: 0.6069 - val_loss: 0.3765 - val_dense_15_loss: 0.3435 - val_dense_16_loss: 0.6741\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3481 - dense_15_loss: 0.3199 - dense_16_loss: 0.6021 - val_loss: 0.3929 - val_dense_15_loss: 0.3633 - val_dense_16_loss: 0.6593\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3476 - dense_15_loss: 0.3192 - dense_16_loss: 0.6032 - val_loss: 0.3713 - val_dense_15_loss: 0.3372 - val_dense_16_loss: 0.6782\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3464 - dense_15_loss: 0.3181 - dense_16_loss: 0.6013 - val_loss: 0.3730 - val_dense_15_loss: 0.3404 - val_dense_16_loss: 0.6658\n"
     ]
    }
   ],
   "source": [
    "#saves at end of each epoch. \n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('multi_output_model_cb.h5')\n",
    "history = model_multi.fit([X_train_A,X_train_B],[y_train_reg,y_train_reg], epochs = 20, validation_data= ([X_valid_A, X_valid_B],[y_valid_reg, y_valid_reg]), callbacks=checkpoint_cb)"
   ]
  },
  {
   "source": [
    "Saving at checkpoints allows us to only save a model with the best score on the validation set. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        loss  dense_15_loss  dense_16_loss  val_loss  val_dense_15_loss  \\\n",
       "0   0.376727       0.347896       0.636205  0.392882           0.359682   \n",
       "1   0.389769       0.362689       0.633492  0.388331           0.356761   \n",
       "2   0.368995       0.340363       0.626683  0.392331           0.357077   \n",
       "3   0.365452       0.336652       0.624650  0.388931           0.354135   \n",
       "4   0.367221       0.338966       0.621518  0.385963           0.352543   \n",
       "5   0.361638       0.332971       0.619635  0.385837           0.351380   \n",
       "6   0.359667       0.330906       0.618520  0.385583           0.352002   \n",
       "7   0.358198       0.329343       0.617889  0.384700           0.350277   \n",
       "8   0.357464       0.328868       0.614825  0.382680           0.348019   \n",
       "9   0.357469       0.329076       0.613003  0.380656           0.348522   \n",
       "10  0.354868       0.326497       0.610215  0.384097           0.346608   \n",
       "11  0.355336       0.326733       0.612760  0.380827           0.345623   \n",
       "12  0.355147       0.326910       0.609284  0.395499           0.361643   \n",
       "13  0.352671       0.324452       0.606644  0.379649           0.344019   \n",
       "14  0.350287       0.321875       0.605996  0.388930           0.355846   \n",
       "15  0.349623       0.321386       0.603758  0.406964           0.378173   \n",
       "16  0.349883       0.321326       0.606895  0.376527           0.343468   \n",
       "17  0.348124       0.319908       0.602072  0.392873           0.363273   \n",
       "18  0.347574       0.319171       0.603202  0.371339           0.337248   \n",
       "19  0.346417       0.318092       0.601339  0.372967           0.340432   \n",
       "\n",
       "    val_dense_16_loss  \n",
       "0            0.691682  \n",
       "1            0.672460  \n",
       "2            0.709615  \n",
       "3            0.702091  \n",
       "4            0.686740  \n",
       "5            0.695948  \n",
       "6            0.687807  \n",
       "7            0.694508  \n",
       "8            0.694636  \n",
       "9            0.669862  \n",
       "10           0.721503  \n",
       "11           0.697661  \n",
       "12           0.700198  \n",
       "13           0.700322  \n",
       "14           0.686678  \n",
       "15           0.666089  \n",
       "16           0.674054  \n",
       "17           0.659273  \n",
       "18           0.678156  \n",
       "19           0.665777  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n      <th>dense_15_loss</th>\n      <th>dense_16_loss</th>\n      <th>val_loss</th>\n      <th>val_dense_15_loss</th>\n      <th>val_dense_16_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.376727</td>\n      <td>0.347896</td>\n      <td>0.636205</td>\n      <td>0.392882</td>\n      <td>0.359682</td>\n      <td>0.691682</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.389769</td>\n      <td>0.362689</td>\n      <td>0.633492</td>\n      <td>0.388331</td>\n      <td>0.356761</td>\n      <td>0.672460</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.368995</td>\n      <td>0.340363</td>\n      <td>0.626683</td>\n      <td>0.392331</td>\n      <td>0.357077</td>\n      <td>0.709615</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.365452</td>\n      <td>0.336652</td>\n      <td>0.624650</td>\n      <td>0.388931</td>\n      <td>0.354135</td>\n      <td>0.702091</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.367221</td>\n      <td>0.338966</td>\n      <td>0.621518</td>\n      <td>0.385963</td>\n      <td>0.352543</td>\n      <td>0.686740</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.361638</td>\n      <td>0.332971</td>\n      <td>0.619635</td>\n      <td>0.385837</td>\n      <td>0.351380</td>\n      <td>0.695948</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.359667</td>\n      <td>0.330906</td>\n      <td>0.618520</td>\n      <td>0.385583</td>\n      <td>0.352002</td>\n      <td>0.687807</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.358198</td>\n      <td>0.329343</td>\n      <td>0.617889</td>\n      <td>0.384700</td>\n      <td>0.350277</td>\n      <td>0.694508</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.357464</td>\n      <td>0.328868</td>\n      <td>0.614825</td>\n      <td>0.382680</td>\n      <td>0.348019</td>\n      <td>0.694636</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.357469</td>\n      <td>0.329076</td>\n      <td>0.613003</td>\n      <td>0.380656</td>\n      <td>0.348522</td>\n      <td>0.669862</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.354868</td>\n      <td>0.326497</td>\n      <td>0.610215</td>\n      <td>0.384097</td>\n      <td>0.346608</td>\n      <td>0.721503</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.355336</td>\n      <td>0.326733</td>\n      <td>0.612760</td>\n      <td>0.380827</td>\n      <td>0.345623</td>\n      <td>0.697661</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.355147</td>\n      <td>0.326910</td>\n      <td>0.609284</td>\n      <td>0.395499</td>\n      <td>0.361643</td>\n      <td>0.700198</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.352671</td>\n      <td>0.324452</td>\n      <td>0.606644</td>\n      <td>0.379649</td>\n      <td>0.344019</td>\n      <td>0.700322</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.350287</td>\n      <td>0.321875</td>\n      <td>0.605996</td>\n      <td>0.388930</td>\n      <td>0.355846</td>\n      <td>0.686678</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.349623</td>\n      <td>0.321386</td>\n      <td>0.603758</td>\n      <td>0.406964</td>\n      <td>0.378173</td>\n      <td>0.666089</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.349883</td>\n      <td>0.321326</td>\n      <td>0.606895</td>\n      <td>0.376527</td>\n      <td>0.343468</td>\n      <td>0.674054</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.348124</td>\n      <td>0.319908</td>\n      <td>0.602072</td>\n      <td>0.392873</td>\n      <td>0.363273</td>\n      <td>0.659273</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.347574</td>\n      <td>0.319171</td>\n      <td>0.603202</td>\n      <td>0.371339</td>\n      <td>0.337248</td>\n      <td>0.678156</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.346417</td>\n      <td>0.318092</td>\n      <td>0.601339</td>\n      <td>0.372967</td>\n      <td>0.340432</td>\n      <td>0.665777</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "#the two outputs have different weights for their losses. \n",
    "trainingdf = pd.DataFrame(data = history.history)\n",
    "trainingdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 6)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 30)           210         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 30)           930         dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 35)           0           input_5[0][0]                    \n",
      "                                                                 dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1)            36          concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 1)            31          dense_13[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,207\n",
      "Trainable params: 1,207\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_multi.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        loss  dense_15_loss  dense_16_loss  val_loss  val_dense_15_loss  \\\n",
       "18  0.347574       0.319171       0.603202  0.371339           0.337248   \n",
       "\n",
       "    val_dense_16_loss  \n",
       "18           0.678156  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n      <th>dense_15_loss</th>\n      <th>dense_16_loss</th>\n      <th>val_loss</th>\n      <th>val_dense_15_loss</th>\n      <th>val_dense_16_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>18</th>\n      <td>0.347574</td>\n      <td>0.319171</td>\n      <td>0.603202</td>\n      <td>0.371339</td>\n      <td>0.337248</td>\n      <td>0.678156</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "trainingdf.loc[trainingdf[\"val_loss\"] == trainingdf[\"val_loss\"].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3449 - dense_15_loss: 0.3168 - dense_16_loss: 0.5976 - val_loss: 0.3968 - val_dense_15_loss: 0.3668 - val_dense_16_loss: 0.6665\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3448 - dense_15_loss: 0.3168 - dense_16_loss: 0.5963 - val_loss: 0.3695 - val_dense_15_loss: 0.3378 - val_dense_16_loss: 0.6542\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3443 - dense_15_loss: 0.3164 - dense_16_loss: 0.5958 - val_loss: 0.3768 - val_dense_15_loss: 0.3438 - val_dense_16_loss: 0.6738\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3437 - dense_15_loss: 0.3160 - dense_16_loss: 0.5934 - val_loss: 0.3674 - val_dense_15_loss: 0.3350 - val_dense_16_loss: 0.6591\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3413 - dense_15_loss: 0.3136 - dense_16_loss: 0.5910 - val_loss: 0.3780 - val_dense_15_loss: 0.3481 - val_dense_16_loss: 0.6467\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3419 - dense_15_loss: 0.3142 - dense_16_loss: 0.5913 - val_loss: 0.3645 - val_dense_15_loss: 0.3322 - val_dense_16_loss: 0.6552\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3408 - dense_15_loss: 0.3132 - dense_16_loss: 0.5893 - val_loss: 0.3708 - val_dense_15_loss: 0.3401 - val_dense_16_loss: 0.6472\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3403 - dense_15_loss: 0.3126 - dense_16_loss: 0.5897 - val_loss: 0.3817 - val_dense_15_loss: 0.3509 - val_dense_16_loss: 0.6589\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3395 - dense_15_loss: 0.3120 - dense_16_loss: 0.5870 - val_loss: 0.3629 - val_dense_15_loss: 0.3314 - val_dense_16_loss: 0.6468\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3390 - dense_15_loss: 0.3116 - dense_16_loss: 0.5854 - val_loss: 0.3648 - val_dense_15_loss: 0.3336 - val_dense_16_loss: 0.6453\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3385 - dense_15_loss: 0.3112 - dense_16_loss: 0.5846 - val_loss: 0.3637 - val_dense_15_loss: 0.3315 - val_dense_16_loss: 0.6532\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3380 - dense_15_loss: 0.3108 - dense_16_loss: 0.5833 - val_loss: 0.3590 - val_dense_15_loss: 0.3283 - val_dense_16_loss: 0.6348\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3357 - dense_15_loss: 0.3083 - dense_16_loss: 0.5820 - val_loss: 0.3612 - val_dense_15_loss: 0.3300 - val_dense_16_loss: 0.6417\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3355 - dense_15_loss: 0.3083 - dense_16_loss: 0.5805 - val_loss: 0.3682 - val_dense_15_loss: 0.3353 - val_dense_16_loss: 0.6637\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3359 - dense_15_loss: 0.3089 - dense_16_loss: 0.5797 - val_loss: 0.3602 - val_dense_15_loss: 0.3282 - val_dense_16_loss: 0.6488\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3348 - dense_15_loss: 0.3078 - dense_16_loss: 0.5783 - val_loss: 0.3606 - val_dense_15_loss: 0.3289 - val_dense_16_loss: 0.6456\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3363 - dense_15_loss: 0.3096 - dense_16_loss: 0.5768 - val_loss: 0.3790 - val_dense_15_loss: 0.3509 - val_dense_16_loss: 0.6317\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3331 - dense_15_loss: 0.3059 - dense_16_loss: 0.5784 - val_loss: 0.3624 - val_dense_15_loss: 0.3282 - val_dense_16_loss: 0.6701\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3328 - dense_15_loss: 0.3058 - dense_16_loss: 0.5758 - val_loss: 0.3694 - val_dense_15_loss: 0.3392 - val_dense_16_loss: 0.6405\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3319 - dense_15_loss: 0.3049 - dense_16_loss: 0.5744 - val_loss: 0.3644 - val_dense_15_loss: 0.3336 - val_dense_16_loss: 0.6420\n"
     ]
    }
   ],
   "source": [
    "#saving the one with best score on the validation set\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint('multi_output_model_cb.h5', save_best_only=True)\n",
    "history = model_multi.fit([X_train_A,X_train_B],[y_train_reg,y_train_reg], epochs = 20, validation_data= ([X_valid_A, X_valid_B],[y_valid_reg, y_valid_reg]), callbacks=checkpoint_cb)"
   ]
  },
  {
   "source": [
    "Another way to implement saving the iteration with the best score on the validation score is to use early stopping\n",
    "\n",
    "Using the EarlyStopping callback\n",
    "It will interrupt training when it measures no progress on the validation set for a number of epochs and also roll back to the best model.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3332 - dense_15_loss: 0.3065 - dense_16_loss: 0.5741 - val_loss: 0.3868 - val_dense_15_loss: 0.3601 - val_dense_16_loss: 0.6272\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3333 - dense_15_loss: 0.3067 - dense_16_loss: 0.5728 - val_loss: 0.3546 - val_dense_15_loss: 0.3234 - val_dense_16_loss: 0.6354\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3332 - dense_15_loss: 0.3064 - dense_16_loss: 0.5747 - val_loss: 0.3732 - val_dense_15_loss: 0.3428 - val_dense_16_loss: 0.6473\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3338 - dense_15_loss: 0.3074 - dense_16_loss: 0.5719 - val_loss: 0.3673 - val_dense_15_loss: 0.3360 - val_dense_16_loss: 0.6490\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3304 - dense_15_loss: 0.3037 - dense_16_loss: 0.5707 - val_loss: 0.3534 - val_dense_15_loss: 0.3224 - val_dense_16_loss: 0.6330\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3304 - dense_15_loss: 0.3039 - dense_16_loss: 0.5694 - val_loss: 0.3582 - val_dense_15_loss: 0.3278 - val_dense_16_loss: 0.6320\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3292 - dense_15_loss: 0.3026 - dense_16_loss: 0.5687 - val_loss: 0.3728 - val_dense_15_loss: 0.3443 - val_dense_16_loss: 0.6300\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3320 - dense_15_loss: 0.3058 - dense_16_loss: 0.5681 - val_loss: 0.3564 - val_dense_15_loss: 0.3249 - val_dense_16_loss: 0.6398\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3283 - dense_15_loss: 0.3018 - dense_16_loss: 0.5672 - val_loss: 0.3608 - val_dense_15_loss: 0.3263 - val_dense_16_loss: 0.6715\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3283 - dense_15_loss: 0.3019 - dense_16_loss: 0.5658 - val_loss: 0.3542 - val_dense_15_loss: 0.3248 - val_dense_16_loss: 0.6189\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3282 - dense_15_loss: 0.3019 - dense_16_loss: 0.5648 - val_loss: 0.3721 - val_dense_15_loss: 0.3436 - val_dense_16_loss: 0.6285\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3274 - dense_15_loss: 0.3012 - dense_16_loss: 0.5634 - val_loss: 0.3558 - val_dense_15_loss: 0.3272 - val_dense_16_loss: 0.6130\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3294 - dense_15_loss: 0.3034 - dense_16_loss: 0.5634 - val_loss: 0.3525 - val_dense_15_loss: 0.3232 - val_dense_16_loss: 0.6165\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3264 - dense_15_loss: 0.3003 - dense_16_loss: 0.5616 - val_loss: 0.3554 - val_dense_15_loss: 0.3240 - val_dense_16_loss: 0.6380\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3270 - dense_15_loss: 0.3009 - dense_16_loss: 0.5615 - val_loss: 0.3498 - val_dense_15_loss: 0.3193 - val_dense_16_loss: 0.6244\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3258 - dense_15_loss: 0.2998 - dense_16_loss: 0.5597 - val_loss: 0.3507 - val_dense_15_loss: 0.3199 - val_dense_16_loss: 0.6278\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3268 - dense_15_loss: 0.3009 - dense_16_loss: 0.5599 - val_loss: 0.3503 - val_dense_15_loss: 0.3207 - val_dense_16_loss: 0.6166\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3253 - dense_15_loss: 0.2993 - dense_16_loss: 0.5593 - val_loss: 0.3512 - val_dense_15_loss: 0.3219 - val_dense_16_loss: 0.6148\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3253 - dense_15_loss: 0.2993 - dense_16_loss: 0.5589 - val_loss: 0.3583 - val_dense_15_loss: 0.3304 - val_dense_16_loss: 0.6092\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3239 - dense_15_loss: 0.2982 - dense_16_loss: 0.5553 - val_loss: 0.3540 - val_dense_15_loss: 0.3238 - val_dense_16_loss: 0.6263\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience = 10, restore_best_weights=True)\n",
    "\n",
    "#while checkpoint_cb saves the model(at end of each epoch by default) and saves only the best validation score if save_best_only = True\n",
    "#early_stoppin checks if the validation score improves over 10 iterations after each previous \n",
    "history = model_multi.fit([X_train_A,X_train_B],[y_train_reg,y_train_reg], epochs = 20, validation_data= ([X_valid_A, X_valid_B],[y_valid_reg, y_valid_reg]), callbacks=[checkpoint_cb,early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can also write our own call backs\n",
    "class PrintValTrainRatio(keras.callbacks.Callback):\n",
    "    #on_epoch_end is a function belonging to Callback which has been inherited\n",
    "    def on_epoch_end(self,epoch,logs):\n",
    "        print(\"\\n Val/Train : {}\".format(logs[\"val_loss\"]/logs[\"loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3264 - dense_15_loss: 0.3008 - dense_16_loss: 0.5572 - val_loss: 0.3468 - val_dense_15_loss: 0.3173 - val_dense_16_loss: 0.6117\n",
      "\n",
      " Val/Train : 1.062324600243543\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3242 - dense_15_loss: 0.2983 - dense_16_loss: 0.5565 - val_loss: 0.3517 - val_dense_15_loss: 0.3227 - val_dense_16_loss: 0.6125\n",
      "\n",
      " Val/Train : 1.0848875896415826\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3258 - dense_15_loss: 0.3004 - dense_16_loss: 0.5544 - val_loss: 0.3500 - val_dense_15_loss: 0.3220 - val_dense_16_loss: 0.6022\n",
      "\n",
      " Val/Train : 1.0742648595030677\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3251 - dense_15_loss: 0.2995 - dense_16_loss: 0.5554 - val_loss: 0.3474 - val_dense_15_loss: 0.3180 - val_dense_16_loss: 0.6121\n",
      "\n",
      " Val/Train : 1.0686655022815708\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3240 - dense_15_loss: 0.2986 - dense_16_loss: 0.5526 - val_loss: 0.3450 - val_dense_15_loss: 0.3158 - val_dense_16_loss: 0.6078\n",
      "\n",
      " Val/Train : 1.0649030386627514\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3226 - dense_15_loss: 0.2971 - dense_16_loss: 0.5520 - val_loss: 0.3461 - val_dense_15_loss: 0.3171 - val_dense_16_loss: 0.6074\n",
      "\n",
      " Val/Train : 1.0729693584204174\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3223 - dense_15_loss: 0.2968 - dense_16_loss: 0.5518 - val_loss: 0.3486 - val_dense_15_loss: 0.3201 - val_dense_16_loss: 0.6052\n",
      "\n",
      " Val/Train : 1.0816627612571041\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3212 - dense_15_loss: 0.2958 - dense_16_loss: 0.5500 - val_loss: 0.3447 - val_dense_15_loss: 0.3151 - val_dense_16_loss: 0.6103\n",
      "\n",
      " Val/Train : 1.073083524253061\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3231 - dense_15_loss: 0.2979 - dense_16_loss: 0.5499 - val_loss: 0.3490 - val_dense_15_loss: 0.3198 - val_dense_16_loss: 0.6115\n",
      "\n",
      " Val/Train : 1.080146666306295\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3203 - dense_15_loss: 0.2949 - dense_16_loss: 0.5488 - val_loss: 0.3667 - val_dense_15_loss: 0.3403 - val_dense_16_loss: 0.6045\n",
      "\n",
      " Val/Train : 1.144907496795595\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3214 - dense_15_loss: 0.2963 - dense_16_loss: 0.5478 - val_loss: 0.3468 - val_dense_15_loss: 0.3165 - val_dense_16_loss: 0.6189\n",
      "\n",
      " Val/Train : 1.0788459901578464\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3210 - dense_15_loss: 0.2958 - dense_16_loss: 0.5473 - val_loss: 0.3465 - val_dense_15_loss: 0.3166 - val_dense_16_loss: 0.6159\n",
      "\n",
      " Val/Train : 1.079606856885645\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3220 - dense_15_loss: 0.2972 - dense_16_loss: 0.5453 - val_loss: 0.3407 - val_dense_15_loss: 0.3127 - val_dense_16_loss: 0.5927\n",
      "\n",
      " Val/Train : 1.0580470434009377\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3201 - dense_15_loss: 0.2949 - dense_16_loss: 0.5472 - val_loss: 0.3558 - val_dense_15_loss: 0.3289 - val_dense_16_loss: 0.5979\n",
      "\n",
      " Val/Train : 1.1113914510519667\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3219 - dense_15_loss: 0.2971 - dense_16_loss: 0.5452 - val_loss: 0.3427 - val_dense_15_loss: 0.3141 - val_dense_16_loss: 0.6004\n",
      "\n",
      " Val/Train : 1.064701809298342\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3193 - dense_15_loss: 0.2944 - dense_16_loss: 0.5438 - val_loss: 0.3475 - val_dense_15_loss: 0.3191 - val_dense_16_loss: 0.6027\n",
      "\n",
      " Val/Train : 1.0881601259910365\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3185 - dense_15_loss: 0.2936 - dense_16_loss: 0.5425 - val_loss: 0.3412 - val_dense_15_loss: 0.3132 - val_dense_16_loss: 0.5940\n",
      "\n",
      " Val/Train : 1.0714206244382718\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3182 - dense_15_loss: 0.2934 - dense_16_loss: 0.5410 - val_loss: 0.3575 - val_dense_15_loss: 0.3306 - val_dense_16_loss: 0.5997\n",
      "\n",
      " Val/Train : 1.1236132935006973\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3196 - dense_15_loss: 0.2948 - dense_16_loss: 0.5419 - val_loss: 0.3445 - val_dense_15_loss: 0.3156 - val_dense_16_loss: 0.6045\n",
      "\n",
      " Val/Train : 1.0779513405898509\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3185 - dense_15_loss: 0.2939 - dense_16_loss: 0.5403 - val_loss: 0.3449 - val_dense_15_loss: 0.3181 - val_dense_16_loss: 0.5866\n",
      "\n",
      " Val/Train : 1.0828396808463459\n"
     ]
    }
   ],
   "source": [
    "printvaltrain = PrintValTrainRatio()\n",
    "history = model_multi.fit([X_train_A,X_train_B],[y_train_reg,y_train_reg], epochs = 20, validation_data= ([X_valid_A, X_valid_B],[y_valid_reg, y_valid_reg]), callbacks=[checkpoint_cb,early_stopping_cb,printvaltrain])"
   ]
  },
  {
   "source": [
    "<h3> Tensorboard </h3> \n",
    "\n",
    "To use Tensorboard, we must modify the program such that it outputs the data we want to visualize to special binary log files called **event files**. Each binary record is called a summary. \n",
    "\n",
    "The Tensorboard server will monitor the log directory, and it will automatically pick up the changes and update the visualizations. \n",
    "\n",
    "Usually we want the TensorBoard server to point to a root log directory and configure the program to write to a different sub directory everytime it runs. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_logdir = os.path.join(os.curdir,\"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    #time.strftime is used when we are getting the data from a function like time.localtime()\n",
    "    #time.strptime is used to convert an existing string of time to a format we want\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\", time.localtime())\n",
    "    return os.path.join(root_logdir,run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "time.struct_time(tm_year=2021, tm_mon=4, tm_mday=10, tm_hour=11, tm_min=34, tm_sec=24, tm_wday=5, tm_yday=100, tm_isdst=0)"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "time.localtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    " run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\", time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'run_2021_04_10-11_34_24'"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 3ms/step - loss: 0.3171 - dense_15_loss: 0.2925 - dense_16_loss: 0.5387 - val_loss: 0.3513 - val_dense_15_loss: 0.3244 - val_dense_16_loss: 0.5935\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3172 - dense_15_loss: 0.2926 - dense_16_loss: 0.5382 - val_loss: 0.3435 - val_dense_15_loss: 0.3155 - val_dense_16_loss: 0.5962\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3161 - dense_15_loss: 0.2917 - dense_16_loss: 0.5361 - val_loss: 0.3510 - val_dense_15_loss: 0.3246 - val_dense_16_loss: 0.5888\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3167 - dense_15_loss: 0.2923 - dense_16_loss: 0.5355 - val_loss: 0.3483 - val_dense_15_loss: 0.3199 - val_dense_16_loss: 0.6038\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3160 - dense_15_loss: 0.2917 - dense_16_loss: 0.5355 - val_loss: 0.3458 - val_dense_15_loss: 0.3183 - val_dense_16_loss: 0.5930\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3154 - dense_15_loss: 0.2912 - dense_16_loss: 0.5332 - val_loss: 0.3398 - val_dense_15_loss: 0.3125 - val_dense_16_loss: 0.5847\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3143 - dense_15_loss: 0.2900 - dense_16_loss: 0.5337 - val_loss: 0.3525 - val_dense_15_loss: 0.3270 - val_dense_16_loss: 0.5825\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3155 - dense_15_loss: 0.2915 - dense_16_loss: 0.5319 - val_loss: 0.3380 - val_dense_15_loss: 0.3106 - val_dense_16_loss: 0.5847\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3140 - dense_15_loss: 0.2899 - dense_16_loss: 0.5310 - val_loss: 0.3457 - val_dense_15_loss: 0.3197 - val_dense_16_loss: 0.5794\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3132 - dense_15_loss: 0.2891 - dense_16_loss: 0.5301 - val_loss: 0.3474 - val_dense_15_loss: 0.3200 - val_dense_16_loss: 0.5941\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3129 - dense_15_loss: 0.2888 - dense_16_loss: 0.5293 - val_loss: 0.3519 - val_dense_15_loss: 0.3266 - val_dense_16_loss: 0.5801\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3151 - dense_15_loss: 0.2915 - dense_16_loss: 0.5278 - val_loss: 0.3389 - val_dense_15_loss: 0.3124 - val_dense_16_loss: 0.5771\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3142 - dense_15_loss: 0.2905 - dense_16_loss: 0.5276 - val_loss: 0.3463 - val_dense_15_loss: 0.3194 - val_dense_16_loss: 0.5881\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3202 - dense_15_loss: 0.2971 - dense_16_loss: 0.5284 - val_loss: 0.3336 - val_dense_15_loss: 0.3070 - val_dense_16_loss: 0.5733\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3126 - dense_15_loss: 0.2890 - dense_16_loss: 0.5258 - val_loss: 0.3396 - val_dense_15_loss: 0.3133 - val_dense_16_loss: 0.5761\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3126 - dense_15_loss: 0.2891 - dense_16_loss: 0.5248 - val_loss: 0.3406 - val_dense_15_loss: 0.3141 - val_dense_16_loss: 0.5785\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3117 - dense_15_loss: 0.2880 - dense_16_loss: 0.5251 - val_loss: 0.3425 - val_dense_15_loss: 0.3171 - val_dense_16_loss: 0.5709\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3130 - dense_15_loss: 0.2897 - dense_16_loss: 0.5230 - val_loss: 0.3492 - val_dense_15_loss: 0.3246 - val_dense_16_loss: 0.5712\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3120 - dense_15_loss: 0.2887 - dense_16_loss: 0.5215 - val_loss: 0.3392 - val_dense_15_loss: 0.3132 - val_dense_16_loss: 0.5723\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3109 - dense_15_loss: 0.2876 - dense_16_loss: 0.5210 - val_loss: 0.3382 - val_dense_15_loss: 0.3125 - val_dense_16_loss: 0.5692\n"
     ]
    }
   ],
   "source": [
    "run_logdir = get_run_logdir()\n",
    "#this allows to create many subdirectories at the end of each epoch under the main directory \"my_logs\"\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model_multi.fit([X_train_A,X_train_B],[y_train_reg,y_train_reg], epochs = 20, validation_data= ([X_valid_A, X_valid_B],[y_valid_reg, y_valid_reg]), callbacks=[tensorboard_cb])"
   ]
  },
  {
   "source": [
    "<h2> Hyperparameter Tuning of Neural Networks</h2> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "One option is to use GridSearchCV or RandomizedSearchCV"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wrap our Keras models in objects that mimic regular Scikit-Learn regressors. Create a function that will build and compile a Keras model, given a set of hyperparameters\n",
    "def build_model(n_hidden = 1, n_neurons = 30, learning_rate = 3e-3, input_shape = [8]):\n",
    "    model = keras.models.Sequential()\n",
    "    options = {\"input_shape\":input_shape}\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation = \"relu\",**options))\n",
    "        options = {}\n",
    "    model.add(keras.layers.Dense(1 , **options))\n",
    "    #we add the optimizer like this so that we can specify the learning rate\n",
    "    optimizer = keras.optimizers.SGD(learning_rate)\n",
    "    model.compile(loss=\"mse\",optimizer = optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Functional API method with ability to specify varying neuron numbers\n",
    "def build_model_2(n_hidden = 1, n_neurons = 30, learning_rate = 3e-3, input_shape = [8]):\n",
    "    input_layer = keras.layers.Input(shape = input_shape)\n",
    "    hidden_layers = list()\n",
    "    type_of_n_neuron_entry = type(n_neurons)\n",
    "    prev_layer = input_layer\n",
    "    if type_of_n_neuron_entry == int:\n",
    "        for layer in range(n_hidden):\n",
    "            hidden_layers.append(keras.layers.Dense(n_neurons, activation = \"relu\")(prev_layer))\n",
    "            prev_layer = hidden_layers[-1]\n",
    "    elif type_of_n_neuron_entry == list:\n",
    "        for i in range(n_hidden):\n",
    "            hidden_layers.append(keras.layers.Dense(n_neurons[i],activation = \"relu\")(prev_layer))\n",
    "            prev_layer = hidden_layers[-1]\n",
    "    else:\n",
    "        raise TypeError('n_neuron should be int or list type')\n",
    "    output = keras.layers.Dense(1)(hidden_layers[-1])\n",
    "    model = keras.models.Model(inputs = input_layer, outputs = [output])\n",
    "    #we add the optimizer like this so that we can specify the learning rate\n",
    "    optimizer = keras.optimizers.SGD(learning_rate)\n",
    "    model.compile(loss=\"mse\",optimizer = optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a KerasRegressor based on the build_model function\n",
    "\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "source": [
    "KerasRegressor object is a thin wrapper around the Keras model built using build_model(). Since we did not specify any hyperparameter when creating it, it will just use the default hyperparameters we defined in build_model(). \n",
    "\n",
    "As we have specified build_model with certain parameters, now we can use **keras_reg** like a regular sklearn regressor.\n",
    "\n",
    "Any parameter we pass to the fit() method of keras_reg, will be passed to the underlying Keras model. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "param_distribs = {\n",
    "    \"n_hidden\":[0,1,2,3],\n",
    "    \"n_neurons\":np.arange(1,100),\n",
    "    \"learning_rate\":reciprocal(3e-4,3e-2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "========================] - 0s 1ms/step - loss: 0.3431 - val_loss: 0.3894\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3490 - val_loss: 0.3813\n",
      "121/121 [==============================] - 0s 576us/step - loss: 0.3868\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 3.4492 - val_loss: 1.3279\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 0s 997us/step - loss: 1.1171 - val_loss: 2.8102\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 0s 983us/step - loss: 1.1355 - val_loss: 8.1979\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 10.9039 - val_loss: 26.7846\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 12.5224 - val_loss: 89.8200\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 0s 999us/step - loss: 25.8562 - val_loss: 304.0906\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 124.2003 - val_loss: 1021.7225\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1452.3542 - val_loss: 3455.5234\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 9615.4518 - val_loss: 11674.8750\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 2161.4202 - val_loss: 40045.8867\n",
      "121/121 [==============================] - 0s 664us/step - loss: 375.8216\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 4.2421 - val_loss: 0.7749\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6974 - val_loss: 0.7015\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6604 - val_loss: 1.0469\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8530 - val_loss: 2.2520\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 9.7299 - val_loss: 6.8250\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 12.3804 - val_loss: 21.7468\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 25.5140 - val_loss: 74.8309\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 4.3885 - val_loss: 262.3543\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 354.0389 - val_loss: 898.5156\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2880.4385 - val_loss: 3098.0610\n",
      "121/121 [==============================] - 0s 577us/step - loss: 46.9909\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.8798 - val_loss: 0.8509\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7813 - val_loss: 0.6786\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6673 - val_loss: 0.6418\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6096 - val_loss: 0.6212\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5755 - val_loss: 0.6161\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5755 - val_loss: 0.6201\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5479 - val_loss: 0.6319\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5377 - val_loss: 0.6422\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5454 - val_loss: 0.6835\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5213 - val_loss: 0.6832\n",
      "121/121 [==============================] - 0s 777us/step - loss: 0.8347\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.7924 - val_loss: 0.9642\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.9079 - val_loss: 0.9173\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.8637 - val_loss: 0.9660\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.0240 - val_loss: 0.7752\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7772 - val_loss: 0.7306\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7402 - val_loss: 0.7016\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6734 - val_loss: 0.6791\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6873 - val_loss: 0.6580\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6354 - val_loss: 0.6396\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.6220 - val_loss: 0.6256\n",
      "121/121 [==============================] - 0s 667us/step - loss: 0.6109\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.5978 - val_loss: 0.8077\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7255 - val_loss: 0.6460\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6265 - val_loss: 0.5988\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5777 - val_loss: 0.5706\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5797 - val_loss: 0.5569\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5564 - val_loss: 0.5362\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5537 - val_loss: 0.5218\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5136 - val_loss: 0.5097\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5143 - val_loss: 0.4977\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4869 - val_loss: 0.4874\n",
      "121/121 [==============================] - 0s 599us/step - loss: 0.4808\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 3.0189 - val_loss: 1.3857\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0921 - val_loss: 1.1281\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8640 - val_loss: 0.9935\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7719 - val_loss: 0.8665\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7007 - val_loss: 0.8144\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6683 - val_loss: 0.7651\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6108 - val_loss: 0.7287\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5738 - val_loss: 0.7102\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5813 - val_loss: 0.6758\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5686 - val_loss: 0.6581\n",
      "121/121 [==============================] - 0s 582us/step - loss: 0.8639\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.8428 - val_loss: 0.8465\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7116 - val_loss: 0.6557\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6249 - val_loss: 0.5735\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5517 - val_loss: 0.5201\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5145 - val_loss: 0.4894\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4638 - val_loss: 0.4629\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4403 - val_loss: 0.4439\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4288 - val_loss: 0.4304\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4225 - val_loss: 0.4193\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4036 - val_loss: 0.4108\n",
      "121/121 [==============================] - 0s 632us/step - loss: 0.4103\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.4004 - val_loss: 1.3332\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9900 - val_loss: 0.6546\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6322 - val_loss: 0.5954\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5473 - val_loss: 0.5587\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5256 - val_loss: 0.5278\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4986 - val_loss: 0.5052\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4939 - val_loss: 0.4873\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4823 - val_loss: 0.4758\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4659 - val_loss: 0.4634\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4477 - val_loss: 0.4580\n",
      "121/121 [==============================] - 0s 589us/step - loss: 0.4475\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.3687 - val_loss: 0.7797\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6851 - val_loss: 0.6616\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6246 - val_loss: 0.5936\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5529 - val_loss: 0.5491\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5378 - val_loss: 0.5159\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5008 - val_loss: 0.4965\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4503 - val_loss: 0.4917\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4617 - val_loss: 0.4751\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4302 - val_loss: 0.5162\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4185 - val_loss: 0.4630\n",
      "121/121 [==============================] - 0s 611us/step - loss: 0.4844\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.9681 - val_loss: 0.9076\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8571 - val_loss: 1.0588\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8773 - val_loss: 1.6452\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.6186 - val_loss: 0.8120\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8249 - val_loss: 0.6043\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5930 - val_loss: 0.5774\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5402 - val_loss: 0.5439\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5338 - val_loss: 0.5287\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5066 - val_loss: 0.5194\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5003 - val_loss: 0.5049\n",
      "121/121 [==============================] - 0s 578us/step - loss: 0.4918\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.3908 - val_loss: 1.5333\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9269 - val_loss: 3.6683\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7245 - val_loss: 4.7033\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3804 - val_loss: 0.6316\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5956 - val_loss: 0.5702\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5209 - val_loss: 0.5260\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4992 - val_loss: 0.4944\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4802 - val_loss: 0.4718\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4163 - val_loss: 0.4565\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4246 - val_loss: 0.4425\n",
      "121/121 [==============================] - 0s 583us/step - loss: 0.4040\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.6520 - val_loss: 0.6429\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5898 - val_loss: 0.5894\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5443 - val_loss: 0.5635\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5105 - val_loss: 0.5498\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5082 - val_loss: 0.5443\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5066 - val_loss: 0.5479\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4808 - val_loss: 0.5473\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4537 - val_loss: 0.5499\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4508 - val_loss: 0.5543\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4511 - val_loss: 0.5600\n",
      "121/121 [==============================] - 0s 576us/step - loss: 0.6300\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 2.2707 - val_loss: 1.7895\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 619us/step - loss: nan\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0718 - val_loss: 0.5884\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 582us/step - loss: nan\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 1s 2ms/step - loss: 1.0225 - val_loss: 0.5177\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4691 - val_loss: 0.4604\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4203 - val_loss: 0.4307\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4215 - val_loss: 0.4559\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4041 - val_loss: 0.4032\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3633 - val_loss: 0.3826\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3558 - val_loss: 0.3923\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3671 - val_loss: 0.4234\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3335 - val_loss: 0.3922\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3476 - val_loss: 0.3566\n",
      "121/121 [==============================] - 0s 582us/step - loss: 0.3563\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 3.5593 - val_loss: 1.1223\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8891 - val_loss: 2.8797\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 0s 939us/step - loss: 1.3438 - val_loss: 13.9779\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 0s 978us/step - loss: 3.5794 - val_loss: 86.7346\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 0s 987us/step - loss: 73.0192 - val_loss: 540.9372\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 0s 972us/step - loss: 62.2099 - val_loss: 3465.7139\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 0s 979us/step - loss: 548.1820 - val_loss: 21808.1914\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 0s 970us/step - loss: 4963.8983 - val_loss: 137786.3438\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 0s 977us/step - loss: 51951.1503 - val_loss: 865371.0000\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 954315.1169 - val_loss: 5434105.5000\n",
      "121/121 [==============================] - 0s 529us/step - loss: 33441.0547\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 3.6649 - val_loss: 2.6747\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 0s 973us/step - loss: 5.2824 - val_loss: 14.2835\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 8.9241 - val_loss: 90.1587\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 0s 993us/step - loss: 15.6319 - val_loss: 584.9991\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 98.4809 - val_loss: 3777.8682\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 0s 971us/step - loss: 10699.3597 - val_loss: 24457.0762\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 0s 940us/step - loss: 14135.1312 - val_loss: 159509.7812\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 35386.1421 - val_loss: 1041143.9375\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 677644.5533 - val_loss: 6714706.5000\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 0s 990us/step - loss: 3699252.2689 - val_loss: 43573348.0000\n",
      "121/121 [==============================] - 0s 582us/step - loss: 673222.6250\n",
      "Epoch 1/10\n",
      "242/242 [==============================] - 1s 1ms/step - loss: 3.8049 - val_loss: 1.5464\n",
      "Epoch 2/10\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.6041 - val_loss: 1.4918\n",
      "Epoch 3/10\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.5704 - val_loss: 1.5125\n",
      "Epoch 4/10\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.5355 - val_loss: 1.5417\n",
      "Epoch 5/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5130 - val_loss: 1.6023\n",
      "Epoch 6/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5306 - val_loss: 1.6011\n",
      "Epoch 7/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5219 - val_loss: 1.6423\n",
      "Epoch 8/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4959 - val_loss: 1.6843\n",
      "Epoch 9/10\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.5090 - val_loss: 1.7059\n",
      "Epoch 10/10\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4961 - val_loss: 1.8738\n",
      "121/121 [==============================] - 0s 549us/step - loss: 3.2537\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [            nan -4.41996664e-01 -7.31956283e-01             nan\n",
      " -1.41215732e+02 -6.51857217e-01 -4.47373897e-01 -5.08588264e-01\n",
      "             nan -2.35555644e+05]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x00000244820A4A00>, as the constructor either does not set or modifies parameter learning_rate",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-a67985e9e532>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mrnd_search_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_reg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_distribs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#All the parameters in the fit method get relayed to the underlying Keras Mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m rnd_search_cv.fit(x_train_reg,y_train_reg, epochs = 10, validation_data = (x_valid_reg,y_valid_reg),\n\u001b[0m\u001b[0;32m      4\u001b[0m callbacks = [keras.callbacks.EarlyStopping(patience = 10)])\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    874\u001b[0m             \u001b[1;31m# we clone again after setting params in case some\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m             \u001b[1;31m# of the params are estimators as well.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 876\u001b[1;33m             self.best_estimator_ = clone(clone(base_estimator).set_params(\n\u001b[0m\u001b[0;32m    877\u001b[0m                 **self.best_params_))\n\u001b[0;32m    878\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mparam2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparam1\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mparam2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m             raise RuntimeError('Cannot clone object %s, as the constructor '\n\u001b[0m\u001b[0;32m     86\u001b[0m                                \u001b[1;34m'either does not set or modifies parameter %s'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                                (estimator, name))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x00000244820A4A00>, as the constructor either does not set or modifies parameter learning_rate"
     ]
    }
   ],
   "source": [
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter = 10, cv = 3)\n",
    "#All the parameters in the fit method get relayed to the underlying Keras Mode\n",
    "rnd_search_cv.fit(x_train_reg,y_train_reg, epochs = 10, validation_data = (x_valid_reg,y_valid_reg),\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience = 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'learning_rate': 0.011133075366404921, 'n_hidden': 3, 'n_neurons': 33}"
      ]
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-0.37642330924669903"
      ]
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "source": [
    "The unified scoring API always maximizes the score, so scores which need to be minimized are negated in order for the unified scoring API to work correctly. While scores which need to be maximised are kept positive. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = build_model(n_hidden = 3, n_neurons = 33, learning_rate = 0.011133075366404921, input_shape = [8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 1.2311 - val_loss: 0.5658\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5200 - val_loss: 0.4642\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4517 - val_loss: 0.4235\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4341 - val_loss: 0.4215\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4030 - val_loss: 0.4156\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3981 - val_loss: 0.3989\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3873 - val_loss: 0.4101\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3669 - val_loss: 0.3809\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3520 - val_loss: 0.3772\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3541 - val_loss: 0.3623\n"
     ]
    }
   ],
   "source": [
    "history_best_model = best_model.fit(x_train_reg,y_train_reg, epochs = 10, validation_data = (x_valid_reg,y_valid_reg),\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience = 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x24484c3e2b0>"
      ]
     },
     "metadata": {},
     "execution_count": 80
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 576x720 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"574.678125pt\" version=\"1.1\" viewBox=\"0 0 500.565625 574.678125\" width=\"500.565625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-04-10T11:37:19.046739</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 574.678125 \r\nL 500.565625 574.678125 \r\nL 500.565625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 46.965625 550.8 \r\nL 493.365625 550.8 \r\nL 493.365625 7.2 \r\nL 46.965625 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 67.256534 550.8 \r\nL 77.580894 550.8 \r\nL 77.580894 543.965488 \r\nL 67.256534 543.965488 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 77.580894 550.8 \r\nL 87.905253 550.8 \r\nL 87.905253 391.897595 \r\nL 77.580894 391.897595 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 87.905253 550.8 \r\nL 98.229613 550.8 \r\nL 98.229613 232.995191 \r\nL 87.905253 232.995191 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 98.229613 550.8 \r\nL 108.553972 550.8 \r\nL 108.553972 181.736351 \r\nL 98.229613 181.736351 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_7\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 108.553972 550.8 \r\nL 118.878331 550.8 \r\nL 118.878331 103.139463 \r\nL 108.553972 103.139463 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_8\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 118.878331 550.8 \r\nL 129.202691 550.8 \r\nL 129.202691 116.808487 \r\nL 118.878331 116.808487 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_9\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 129.202691 550.8 \r\nL 139.52705 550.8 \r\nL 139.52705 41.628854 \r\nL 129.202691 41.628854 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_10\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 139.52705 550.8 \r\nL 149.85141 550.8 \r\nL 149.85141 33.085714 \r\nL 139.52705 33.085714 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_11\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 149.85141 550.8 \r\nL 160.175769 550.8 \r\nL 160.175769 86.053182 \r\nL 149.85141 86.053182 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_12\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 160.175769 550.8 \r\nL 170.500129 550.8 \r\nL 170.500129 99.722207 \r\nL 160.175769 99.722207 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_13\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 170.500129 550.8 \r\nL 180.824488 550.8 \r\nL 180.824488 215.908911 \r\nL 170.500129 215.908911 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_14\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 180.824488 550.8 \r\nL 191.148848 550.8 \r\nL 191.148848 127.060255 \r\nL 180.824488 127.060255 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_15\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 191.148848 550.8 \r\nL 201.473207 550.8 \r\nL 201.473207 234.703819 \r\nL 191.148848 234.703819 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_16\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 201.473207 550.8 \r\nL 211.797567 550.8 \r\nL 211.797567 306.466195 \r\nL 201.473207 306.466195 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_17\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 211.797567 550.8 \r\nL 222.121926 550.8 \r\nL 222.121926 308.174823 \r\nL 211.797567 308.174823 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_18\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 222.121926 550.8 \r\nL 232.446286 550.8 \r\nL 232.446286 378.228571 \r\nL 222.121926 378.228571 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_19\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 232.446286 550.8 \r\nL 242.770645 550.8 \r\nL 242.770645 410.692504 \r\nL 232.446286 410.692504 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_20\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 242.770645 550.8 \r\nL 253.095005 550.8 \r\nL 253.095005 402.149364 \r\nL 242.770645 402.149364 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_21\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 253.095005 550.8 \r\nL 263.419364 550.8 \r\nL 263.419364 415.818388 \r\nL 253.095005 415.818388 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_22\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 263.419364 550.8 \r\nL 273.743724 550.8 \r\nL 273.743724 443.156436 \r\nL 263.419364 443.156436 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_23\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 273.743724 550.8 \r\nL 284.068083 550.8 \r\nL 284.068083 487.580764 \r\nL 273.743724 487.580764 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_24\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 284.068083 550.8 \r\nL 294.392443 550.8 \r\nL 294.392443 468.785856 \r\nL 284.068083 468.785856 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_25\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 294.392443 550.8 \r\nL 304.716802 550.8 \r\nL 304.716802 485.872136 \r\nL 294.392443 485.872136 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_26\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 304.716802 550.8 \r\nL 315.041161 550.8 \r\nL 315.041161 496.123904 \r\nL 304.716802 496.123904 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_27\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 315.041161 550.8 \r\nL 325.365521 550.8 \r\nL 325.365521 502.958416 \r\nL 315.041161 502.958416 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_28\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 325.365521 550.8 \r\nL 335.68988 550.8 \r\nL 335.68988 520.044696 \r\nL 325.365521 520.044696 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_29\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 335.68988 550.8 \r\nL 346.01424 550.8 \r\nL 346.01424 224.452051 \r\nL 335.68988 224.452051 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n   </g>\r\n   <g id=\"patch_30\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 88.933941 550.8 \r\nL 98.537461 550.8 \r\nL 98.537461 484.163508 \r\nL 88.933941 484.163508 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_31\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 98.537461 550.8 \r\nL 108.140981 550.8 \r\nL 108.140981 238.121075 \r\nL 98.537461 238.121075 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_32\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 108.140981 550.8 \r\nL 117.744498 550.8 \r\nL 117.744498 98.013579 \r\nL 108.140981 98.013579 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_33\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 117.744498 550.8 \r\nL 127.348018 550.8 \r\nL 127.348018 41.628854 \r\nL 117.744498 41.628854 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_34\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 127.348018 550.8 \r\nL 136.951538 550.8 \r\nL 136.951538 92.887694 \r\nL 127.348018 92.887694 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_35\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 136.951538 550.8 \r\nL 146.555058 550.8 \r\nL 146.555058 103.139463 \r\nL 136.951538 103.139463 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_36\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 146.555058 550.8 \r\nL 156.158578 550.8 \r\nL 156.158578 150.981047 \r\nL 146.555058 150.981047 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_37\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 156.158578 550.8 \r\nL 165.762098 550.8 \r\nL 165.762098 111.682603 \r\nL 156.158578 111.682603 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_38\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 165.762098 550.8 \r\nL 175.365619 550.8 \r\nL 175.365619 115.099859 \r\nL 165.762098 115.099859 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_39\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 175.365619 550.8 \r\nL 184.969139 550.8 \r\nL 184.969139 137.312023 \r\nL 175.365619 137.312023 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_40\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 184.969139 550.8 \r\nL 194.572659 550.8 \r\nL 194.572659 176.610467 \r\nL 184.969139 176.610467 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_41\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 194.572659 550.8 \r\nL 204.176179 550.8 \r\nL 204.176179 190.279491 \r\nL 194.572659 190.279491 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_42\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 204.176179 550.8 \r\nL 213.779699 550.8 \r\nL 213.779699 162.941443 \r\nL 204.176179 162.941443 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_43\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 213.779699 550.8 \r\nL 223.383205 550.8 \r\nL 223.383205 262.041867 \r\nL 213.779699 262.041867 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_44\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 223.383205 550.8 \r\nL 232.986726 550.8 \r\nL 232.986726 284.254031 \r\nL 223.383205 284.254031 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_45\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 232.986726 550.8 \r\nL 242.590246 550.8 \r\nL 242.590246 350.890523 \r\nL 232.986726 350.890523 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_46\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 242.590246 550.8 \r\nL 252.193766 550.8 \r\nL 252.193766 407.275248 \r\nL 242.590246 407.275248 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_47\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 252.193766 550.8 \r\nL 261.797286 550.8 \r\nL 261.797286 405.56662 \r\nL 252.193766 405.56662 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_48\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 261.797286 550.8 \r\nL 271.400806 550.8 \r\nL 271.400806 456.82546 \r\nL 261.797286 456.82546 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_49\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 271.400806 550.8 \r\nL 281.004326 550.8 \r\nL 281.004326 456.82546 \r\nL 271.400806 456.82546 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_50\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 281.004326 550.8 \r\nL 290.607861 550.8 \r\nL 290.607861 467.077228 \r\nL 281.004326 467.077228 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_51\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 290.607861 550.8 \r\nL 300.211381 550.8 \r\nL 300.211381 502.958416 \r\nL 290.607861 502.958416 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_52\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 300.211381 550.8 \r\nL 309.814873 550.8 \r\nL 309.814873 501.249788 \r\nL 300.211381 501.249788 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_53\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 309.814873 550.8 \r\nL 319.418393 550.8 \r\nL 319.418393 509.792928 \r\nL 309.814873 509.792928 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_54\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 319.418393 550.8 \r\nL 329.021913 550.8 \r\nL 329.021913 508.0843 \r\nL 319.418393 508.0843 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_55\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 329.021913 550.8 \r\nL 338.625434 550.8 \r\nL 338.625434 523.461952 \r\nL 329.021913 523.461952 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_56\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 338.625434 550.8 \r\nL 348.228954 550.8 \r\nL 348.228954 535.422348 \r\nL 338.625434 535.422348 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_57\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 348.228954 550.8 \r\nL 357.832474 550.8 \r\nL 357.832474 540.548232 \r\nL 348.228954 540.548232 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_58\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 357.832474 550.8 \r\nL 367.435994 550.8 \r\nL 367.435994 550.8 \r\nL 357.832474 550.8 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_59\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 367.435994 550.8 \r\nL 377.039514 550.8 \r\nL 377.039514 549.091372 \r\nL 367.435994 549.091372 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_60\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 377.039514 550.8 \r\nL 386.643034 550.8 \r\nL 386.643034 550.8 \r\nL 377.039514 550.8 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_61\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 386.643034 550.8 \r\nL 396.246555 550.8 \r\nL 396.246555 550.8 \r\nL 386.643034 550.8 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_62\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 396.246555 550.8 \r\nL 405.850075 550.8 \r\nL 405.850075 550.8 \r\nL 396.246555 550.8 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_63\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 405.850075 550.8 \r\nL 415.453595 550.8 \r\nL 415.453595 550.8 \r\nL 405.850075 550.8 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_64\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 415.453595 550.8 \r\nL 425.057115 550.8 \r\nL 425.057115 550.8 \r\nL 415.453595 550.8 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_65\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 425.057115 550.8 \r\nL 434.660635 550.8 \r\nL 434.660635 549.091372 \r\nL 425.057115 549.091372 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_66\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 434.660635 550.8 \r\nL 444.264155 550.8 \r\nL 444.264155 550.8 \r\nL 434.660635 550.8 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_67\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 444.264155 550.8 \r\nL 453.867676 550.8 \r\nL 453.867676 550.8 \r\nL 444.264155 550.8 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_68\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 453.867676 550.8 \r\nL 463.471196 550.8 \r\nL 463.471196 550.8 \r\nL 453.867676 550.8 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"patch_69\">\r\n    <path clip-path=\"url(#p23249bce59)\" d=\"M 463.471196 550.8 \r\nL 473.074716 550.8 \r\nL 473.074716 549.091372 \r\nL 463.471196 549.091372 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m7692ef5a3c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"54.121382\" xlink:href=\"#m7692ef5a3c\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(50.940132 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"112.499837\" xlink:href=\"#m7692ef5a3c\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 1 -->\r\n      <g transform=\"translate(109.318587 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"170.878291\" xlink:href=\"#m7692ef5a3c\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 2 -->\r\n      <g transform=\"translate(167.697041 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"229.256746\" xlink:href=\"#m7692ef5a3c\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 3 -->\r\n      <g transform=\"translate(226.075496 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"287.635201\" xlink:href=\"#m7692ef5a3c\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 4 -->\r\n      <g transform=\"translate(284.453951 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"346.013656\" xlink:href=\"#m7692ef5a3c\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(342.832406 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"404.392111\" xlink:href=\"#m7692ef5a3c\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 6 -->\r\n      <g transform=\"translate(401.210861 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_8\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"462.770566\" xlink:href=\"#m7692ef5a3c\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 7 -->\r\n      <g transform=\"translate(459.589316 565.398438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-55\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_9\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m3cb55764e8\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m3cb55764e8\" y=\"550.8\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(33.603125 554.599219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m3cb55764e8\" y=\"465.3686\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(27.240625 469.167818)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m3cb55764e8\" y=\"379.937199\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(20.878125 383.736418)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m3cb55764e8\" y=\"294.505799\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 150 -->\r\n      <g transform=\"translate(20.878125 298.305018)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m3cb55764e8\" y=\"209.074399\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(20.878125 212.873618)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m3cb55764e8\" y=\"123.642999\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 250 -->\r\n      <g transform=\"translate(20.878125 127.442217)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"46.965625\" xlink:href=\"#m3cb55764e8\" y=\"38.211598\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 300 -->\r\n      <g transform=\"translate(20.878125 42.010817)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_16\">\r\n     <!-- Count -->\r\n     <g transform=\"translate(14.798437 293.848437)rotate(-90)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 64.40625 67.28125 \r\nL 64.40625 56.890625 \r\nQ 59.421875 61.53125 53.78125 63.8125 \r\nQ 48.140625 66.109375 41.796875 66.109375 \r\nQ 29.296875 66.109375 22.65625 58.46875 \r\nQ 16.015625 50.828125 16.015625 36.375 \r\nQ 16.015625 21.96875 22.65625 14.328125 \r\nQ 29.296875 6.6875 41.796875 6.6875 \r\nQ 48.140625 6.6875 53.78125 8.984375 \r\nQ 59.421875 11.28125 64.40625 15.921875 \r\nL 64.40625 5.609375 \r\nQ 59.234375 2.09375 53.4375 0.328125 \r\nQ 47.65625 -1.421875 41.21875 -1.421875 \r\nQ 24.65625 -1.421875 15.125 8.703125 \r\nQ 5.609375 18.84375 5.609375 36.375 \r\nQ 5.609375 53.953125 15.125 64.078125 \r\nQ 24.65625 74.21875 41.21875 74.21875 \r\nQ 47.75 74.21875 53.53125 72.484375 \r\nQ 59.328125 70.75 64.40625 67.28125 \r\nz\r\n\" id=\"DejaVuSans-67\"/>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n       <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n       <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-67\"/>\r\n      <use x=\"69.824219\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"131.005859\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"194.384766\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"257.763672\" xlink:href=\"#DejaVuSans-116\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_70\">\r\n    <path d=\"M 46.965625 550.8 \r\nL 46.965625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_71\">\r\n    <path d=\"M 493.365625 550.8 \r\nL 493.365625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_72\">\r\n    <path d=\"M 46.965625 550.8 \r\nL 493.365625 550.8 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_73\">\r\n    <path d=\"M 46.965625 7.2 \r\nL 493.365625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_74\">\r\n     <path d=\"M 407.425 44.55625 \r\nL 486.365625 44.55625 \r\nQ 488.365625 44.55625 488.365625 42.55625 \r\nL 488.365625 14.2 \r\nQ 488.365625 12.2 486.365625 12.2 \r\nL 407.425 12.2 \r\nQ 405.425 12.2 405.425 14.2 \r\nL 405.425 42.55625 \r\nQ 405.425 44.55625 407.425 44.55625 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"patch_75\">\r\n     <path d=\"M 409.425 23.798437 \r\nL 429.425 23.798437 \r\nL 429.425 16.798437 \r\nL 409.425 16.798437 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"text_17\">\r\n     <!-- Actual -->\r\n     <g transform=\"translate(437.425 23.798437)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 34.1875 63.1875 \r\nL 20.796875 26.90625 \r\nL 47.609375 26.90625 \r\nz\r\nM 28.609375 72.90625 \r\nL 39.796875 72.90625 \r\nL 67.578125 0 \r\nL 57.328125 0 \r\nL 50.6875 18.703125 \r\nL 17.828125 18.703125 \r\nL 11.1875 0 \r\nL 0.78125 0 \r\nz\r\n\" id=\"DejaVuSans-65\"/>\r\n       <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n       <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n       <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"66.658203\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"121.638672\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"160.847656\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"224.226562\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"285.505859\" xlink:href=\"#DejaVuSans-108\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"patch_76\">\r\n     <path d=\"M 409.425 38.476562 \r\nL 429.425 38.476562 \r\nL 429.425 31.476562 \r\nL 409.425 31.476562 \r\nz\r\n\" style=\"fill:#1f77b4;fill-opacity:0.75;stroke:#000000;stroke-linejoin:miter;stroke-width:0.960349;\"/>\r\n    </g>\r\n    <g id=\"text_18\">\r\n     <!-- Predicted -->\r\n     <g transform=\"translate(437.425 38.476562)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 19.671875 64.796875 \r\nL 19.671875 37.40625 \r\nL 32.078125 37.40625 \r\nQ 38.96875 37.40625 42.71875 40.96875 \r\nQ 46.484375 44.53125 46.484375 51.125 \r\nQ 46.484375 57.671875 42.71875 61.234375 \r\nQ 38.96875 64.796875 32.078125 64.796875 \r\nz\r\nM 9.8125 72.90625 \r\nL 32.078125 72.90625 \r\nQ 44.34375 72.90625 50.609375 67.359375 \r\nQ 56.890625 61.8125 56.890625 51.125 \r\nQ 56.890625 40.328125 50.609375 34.8125 \r\nQ 44.34375 29.296875 32.078125 29.296875 \r\nL 19.671875 29.296875 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-80\"/>\r\n       <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n       <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n       <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n       <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-80\"/>\r\n      <use x=\"58.552734\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"97.416016\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"158.939453\" xlink:href=\"#DejaVuSans-100\"/>\r\n      <use x=\"222.416016\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"250.199219\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"305.179688\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"344.388672\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"405.912109\" xlink:href=\"#DejaVuSans-100\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p23249bce59\">\r\n   <rect height=\"543.6\" width=\"446.4\" x=\"46.965625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfQAAAI/CAYAAACBEStgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAmLElEQVR4nO3df5TVdb3v8debHzJc9STCwIWZ4TDXQxRylIg8ghwX4ErINKLSke5VKxVcV9Syzs28mlrW6rbKXJpX4UhXuhmiJPkjtUxnDrGIFIgrChIaKDMQQ0gqhQwz875/zJdpgGH2nh/fvfe89/Ox1l6zv5/92d95f2nla76f7+f7+Zq7CwAA9G598l0AAADoPgIdAIAACHQAAAIg0AEACIBABwAgAAIdAIAA+uW7gO4YMmSIjxo1Kt9lAACQM2vXrv2zu5ce2d6rA33UqFFas2ZNvssAACBnzOyN9toZcgcAIAACHQCAAAh0AAAC6NXX0AEAhefgwYOqra3Ve++9l+9SerWSkhKVl5erf//+WfUn0AEAPaq2tlYnnniiRo0aJTPLdzm9krtrz549qq2tVWVlZVbfYcgdANCj3nvvPQ0ePJgw7wYz0+DBgzs1ykGgAwB6HGHefZ39NyTQAQAh/fznP5eZ6dVXX+2w35133qm//e1vXf49DzzwgObPn9/l7/cUAh0AkKqyipEysx57lVWMzOr3LlmyRFOmTNGSJUs67NfdQC8UTIoDAKRqR+12VS1Y1WP7WzpvcsY++/bt08qVK1VdXa0LLrhAt912m5qamvTVr35VzzzzjPr06aMrr7xS7q4dO3Zo2rRpGjJkiKqrq3XCCSdo3759kqRly5bpySef1AMPPKAnnnhCt99+uxoaGjR48GA9+OCDGjZsWI8dV3cR6ACAcB577DHNnDlT73//+zV48GCtXbtWL7zwgrZt26b169erX79+euutt3TyySfrjjvuUHV1tYYMGdLhPqdMmaLVq1fLzHT//ffru9/9rr7//e/n6IgyI9ABAOEsWbJE1113nSTp4osv1pIlS7R161ZdddVV6tevJfpOPvnkTu2ztrZWVVVV2rlzpxoaGrK+nSxXCHQAQChvvfWWnn/+eW3YsEFmpqamJpmZPvKRj2T1/bazy9veNnbNNdfo+uuv1yc+8QnV1NTo1ltv7enSu4VJcQCAUJYtW6ZLLrlEb7zxhrZt26bt27ersrJSp59+uhYsWKDGxkZJLcEvSSeeeKLefffd1u8PGzZMmzZtUnNzs5YvX97a/vbbb6usrEyStHjx4hweUXYIdABAKEuWLNHs2bMPa/v0pz+tnTt3auTIkTrttNN0+umn66c//akkae7cuZo5c6amTZsmSfrOd76j888/X5MnT9bw4cNb93Hrrbfqwgsv1Ic//OGM19vzwdw93zV02cSJE53noQNAYdm0aZM++MEPtm6XVYzUjtrtPbb/EeUVqtv+Zo/tr5Ad+W8pSWa21t0nHtmXa+gAgFQVS/jmG0PuAAAEQKADABAAgQ4AQAAEOgAAARDoAAAEQKADAMLp27evxo8fr3HjxunCCy/s1tPUPve5z2nZsmWSpCuuuEIbN248Zt+amhqtWtX5B9GMGjVKf/7zn7tco0SgAwBSVnnKaA0oGdhjr8pTRmf8nQMHDtT69ev18ssv67jjjtN999132OeHVovrrPvvv19jx4495uddDfSewH3o6JRsF4gopoUfAHRsR12tZt/1fI/tb/m10zvV/1//9V/10ksvqaamRjfffLMGDRqkV199VZs2bdINN9ygmpoaHThwQFdffbXmzZsnd9c111yjZ599VhUVFTruuONa9zV16lR973vf08SJE/XMM8/oxhtvVFNTk4YMGaJFixbpvvvuU9++ffWTn/xEd999tz7wgQ/oqquu0ptvtvz38M4779RZZ52lPXv2aM6cOaqrq9OkSZPUE4u8pRboZlYiaYWkAcnvWebut5hZpaSHJA2WtFbSJe7eYGYDJP1Y0ocl7ZFU5e7b0qoPXZPtc42zeV4xAKStsbFRTz/9tGbOnClJWrdunV5++WVVVlZq4cKFet/73qcXX3xRBw4c0FlnnaVzzz1Xv//977V582Zt3LhRu3bt0tixY/WFL3zhsP3u3r1bV155pVasWKHKysrWR7FeddVVOuGEE/SVr3xFkvTZz35WX/rSlzRlyhS9+eabmjFjhjZt2qTbbrtNU6ZM0de//nX94he/0KJFi7p9rGmeoR+QNN3d95lZf0krzexpSddL+oG7P2Rm90m6XNK9yc+97v5PZnaxpP8lqSrF+gAAQe3fv1/jx4+X1HKGfvnll2vVqlU644wzWh97+qtf/UovvfRS6/Xxt99+W1u2bNGKFSs0Z84c9e3bVyNGjND06UePCKxevVpnn312676O9SjWX//614ddc3/nnXe0b98+rVixQo8++qgk6eMf/7gGDRrU7WNOLdC9ZfxgX7LZP3m5pOmSPpu0L5Z0q1oCfVbyXpKWSfqhmZn35sXmAQB5cega+pGOP/741vfurrvvvlszZsw4rM9TTz3VY3U0Nzdr9erVKikp6bF9Hkuqk+LMrK+ZrZdUL+lZSa9L+ou7H5qNUCupLHlfJmm7JCWfv62WYXkAAHrcjBkzdO+99+rgwYOSpD/84Q/661//qrPPPltLly5VU1OTdu7cqerq6qO+e+aZZ2rFihXaunWrpGM/ivXcc8/V3Xff3bp96I+Ms88+u/Vpb08//bT27t3b7eNJNdDdvcndx0sql3SGpA90d59mNtfM1pjZmt27d3d3dwCAInXFFVdo7NixmjBhgsaNG6d58+apsbFRs2fP1ujRozV27FhdeumlmjRp0lHfLS0t1cKFC/WpT31Kp59+uqqqWq4QX3DBBVq+fLnGjx+v3/zmN7rrrru0Zs0anXbaaRo7dmzrbPtbbrlFK1as0KmnnqpHH31UI0eO7Pbx5OzxqWb2dUn7JX1V0n9290YzmyTpVnefYWa/TN7/1sz6SfqTpNKOhtx5fGrumVnWk+K4WgIUpyMf+Vl5ymjtqKvtsf2PKCvX1te39Nj+CllBPD7VzEolHXT3v5jZQEkfVctEt2pJn1HLTPfLJD2WfOXxZPu3yefPc/0cAHq/YgnffEtzlvtwSYvNrK9ahvYfdvcnzWyjpIfM7HZJv5d0aK7+Ikn/18xek/SWpItTrA0AgFDSnOX+kqQPtdP+R7VcTz+y/T1JF6ZVT7HJdohrRFm5GhoOZLVYDACgcLFSXFDZrsy0/NrpajjwXlbXxSUWjAGQHXeXmeW7jF6ts1edWcsdANCjSkpKtGfPHibGdoO7a8+ePZ26f50zdABAjyovL1dtba24tbh7SkpKVF5ennV/Ah0A0KP69+/fuiQqcochdwAAAiDQAQAIgEAHACAAAh0AgAAIdAAAAiDQAQAIgEAHACAAAh0AgAAIdAAAAiDQAQAIgEAHACAAAh0AgAAIdAAAAiDQAQAIgEAHACAAAh0AgAAIdAAAAiDQAQAIgEAHACAAAh0AgAAIdAAAAiDQAQAIgEAHACAAAh0AgAAIdAAAAiDQAQAIgEAHACAAAh0AgAAIdAAAAiDQAQAIgEAHACAAAh0AgAAIdAAAAiDQAQAIgEAHACAAAh0AgAAIdAAAAiDQAQAIgEAHACAAAh3p6NNPZpbxVVYxMt+VAkAI/fJdAIJqblTVglUZuy2dNzkHxQBAfJyhAwAQAIEOAEAABDoAAAEQ6AAABECgAwAQALPci9zBpmbJ+uiR+dOO2WfgoKE6/5tLc1gVAKCzCPQi502NGlZ1u4aOmXDMPq98e1YOKwIAdAVD7gAABECgAwAQAIEOAEAAXEPvZSpPGa0ddbUZ+x08eDAH1QAACgWB3svsqKvV7Luez9hv6VVTclANAKBQMOSO/MryqWw8mQ0AOsYZOvIry6eySTyZDQA6whk6AAABcIbeC61a9Vs1HDiQVb/Jkyd1+/d5c/PfV5LrYFU5VpQDgPwh0HuhhgMHOlzZTZJ2Jf16gjc3adxNT0iS6jevO+bvZkU5AMgfhtwBAAiAQAcAIAACHQCAAAh0AAACINABAAiAQAcAIAACHQCAALgPHTn15M1V2r+3/u8Nx1iohkVqAKBzCHTk1P699Tr1xsdat4+1UA2L1ABA5zDkDgBAAJyhoyAdtn78IdZHA0oGHtY0oqxcW1/fksPKAKAwEegoSG3Xjz+kfvM6TZ029bC25ddOz11RAFDACPTIzFRTXZPvKgAAOUCgR+ae1VPZAAC9H5PiAAAIILVAN7MKM6s2s41m9oqZXZe032pmdWa2Pnmd1+Y7XzOz18xss5nNSKs2AACiSXPIvVHSl919nZmdKGmtmT2bfPYDd/9e285mNlbSxZJOlTRC0q/N7P3u3pRijQAAhJDaGbq773T3dcn7dyVtklTWwVdmSXrI3Q+4+1ZJr0k6I636AACIJCfX0M1slKQPSfpd0jTfzF4ysx+Z2aCkrUzS9jZfq1XHfwAAAIBE6oFuZidI+pmkL7r7O5LulXSKpPGSdkr6fif3N9fM1pjZmt27d/d0uQAA9EqpBrqZ9VdLmD/o7o9Kkrvvcvcmd2+W9O/6+7B6naSKNl8vT9oO4+4L3X2iu08sLS1Ns3wAAHqNNGe5m6RFkja5+x1t2oe36TZb0svJ+8clXWxmA8ysUtJoSS+kVR8AAJGkOcv9LEmXSNpgZuuTthslzTGz8ZJc0jZJ8yTJ3V8xs4clbVTLDPmrmeGOriirGKkdtdsz9htRXqG67W/moCIASF9qge7uKyVZOx891cF3viXpW2nVhOKwo3a7qhasythv6bzJOagGAHKDleIAAAiAQAcAIAACHQCAAAh0AAACINABAAiAQAcAIAACHQCAAAh0AAACSHOlOCB1B5uaNaBk4OGN1kePzJ/Wujlw0FCd/82lOa4MAHKLQEev5k2Nmn3PysPaaqprNHTMhNbtV749K9dlAUDOMeQOAEAABDoAAAEQ6AAABECgAwAQAIEOAEAABDoAAAEQ6AAABECgAwAQAIEOAEAABDoAAAEQ6AAABECgAwAQAA9nQY/x5ua/P+XsiCeeHdLc1JjjqgCgOBDo6DHe3KRxNz0hSarfvO6wJ54dsuEb5+W6LAAoCgy5AwAQAIEOAEAABDoAAAEQ6AAABECgAwAQAIEOAEAABDoAAAEQ6AAABECgAwAQAIEOAEAALP0KdELlKaO1o642Y78RZeXa+vqWHFQEAC0IdKATdtTVavZdz2fst/za6TmoBgD+jiF3AAACINABAAiAQAcAIAACHQCAAAh0AAACINABAAiAQAcAIAACHQCAAAh0AAACINABAAiAQAcAIAACHQCAAHg4C8Lz5mY9Mn/a0R9YHw0oGdi6yRPSAPRmBDrC8+YmjbvpiaPa6zev09RpU1u3eUIagN6MIXcAAAIg0AEACIBABwAgAAK9QJRVjJSZZXw1NDTku1QAQAFiUlyB2FG7XVULVmXs1+5sbQBA0eMMHQCAAAh0AAACINABAAiAQAcAIAACHQCAAAh0AAAC4LY1IHGwqfmwh7W02+fgwRxVAwCdQ6ADCW9q1Ox7VnbYZ+lVU3JUDQB0DkPuAAAEQKADABAAgQ4AQAAEOgAAARDoAAAEQKADABAAgQ4AQAAEOgAAARDoAAAEQKADABAAgQ4AQAAEOgAAARDoAAAEkFqgm1mFmVWb2UYze8XMrkvaTzazZ81sS/JzUNJuZnaXmb1mZi+Z2YS0agMAIJo0z9AbJX3Z3cdKOlPS1WY2VtINkp5z99GSnku2JeljkkYnr7mS7k2xNgAAQkkt0N19p7uvS96/K2mTpDJJsyQtTrotlvTJ5P0sST/2FqslnWRmw9OqDwCASHJyDd3MRkn6kKTfSRrm7juTj/4kaVjyvkzS9jZfq03aAABABqkHupmdIOlnkr7o7u+0/czdXZJ3cn9zzWyNma3ZvXt3D1YKAEDvlWqgm1l/tYT5g+7+aNK869BQevKzPmmvk1TR5uvlSdth3H2hu09094mlpaXpFQ8AQC+S5ix3k7RI0iZ3v6PNR49Luix5f5mkx9q0X5rMdj9T0ttthuYBAEAH+qW477MkXSJpg5mtT9pulPQdSQ+b2eWS3pB0UfLZU5LOk/SapL9J+nyKtQEAEEpqge7uKyXZMT4+p53+LunqtOoBACAyVooDACAAAh0AgADSvIYO9Cwz1VTXHNXcXhsAFBsCHb2Hu4aOOXyJ/13SUW31m9flsCgAKAwMuQMAEACBDgBAAAQ6AAABEOgAAARAoAMAEACBDgBAAAQ6AHRRWcVImVnGV1nFyHyXiiLAfegA0EU7arerasGqjP2Wzpucg2pQ7DhDBwAgAAIdAIAACHQAAAIg0AEACIBABwAgAAIdAIAACHQAAAIg0FOW7cITAAB0BwvLpIyFJwAAuUCgo3iZqaa65rCmI7cPOW7AAE2ePCn9mgCgiwh0FC93DR0zoXVzl3TYdlv1m9flqCgA6BquoQMAEABn6AXkyZurtH9vfYd9mpsac1QNAKA3IdALyP699Tr1xsc67LPhG+flqBoAQG/CkDsAAAEQ6AAABECgAwAQAIEOAEAABDoAAAEQ6AAABECgAwAQAIEOAEAABDoAAAEQ6AAABECgAwAQAIEOAEAABDoAAAEQ6AAABECgAwAQAIEOAEAABDoAAAEQ6AAABECgAwAQAIEOAEAABDoAAAEQ6AAABNAv3wUAER1sataAkoEZ+40oK9fW17fkoCIA0RHoQAq8qVGz71mZsd/ya6fnoBoAxYAhdwAAAiDQAQAIIKtAN7OzsmkDAAD5ke0Z+t1ZtgEAgDzocFKcmU2SNFlSqZld3+ajf5DUN83CAABA9jLNcj9O0glJvxPbtL8j6TNpFQUAADqnw0B39/+Q9B9m9oC7v5GjmgAAQCdlex/6ADNbKGlU2++4OzfRAgBQALIN9Eck3SfpfklN6ZUDAAC6IttAb3T3e1OtBAAAdFm2t609YWb/3cyGm9nJh16pVgYAALKW7Rn6ZcnPf2vT5pL+S8+WAwAAuiKrQHf3yrQLAQAAXZdVoJvZpe21u/uPe7YcAADQFdkOuX+kzfsSSedIWieJQAcAoABkO+R+TdttMztJ0kNpFAQAADqvq49P/askrqsDAFAgsr2G/oRaZrVLLQ9l+aCkh9MqCgAAdE6219C/1+Z9o6Q33L02hXoAAEAXZDXknjyk5VW1PHFtkKSGNIsCAACdk1Wgm9lFkl6QdKGkiyT9zsx4fCrQTQebmjWgZGCHr8pTRue7TAC9QLZD7v9T0kfcvV6SzKxU0q8lLUurMKAYeFOjZt+zssM+y6/loYYAMst2lnufQ2Ge2NOJ7wIAgJRle4b+jJn9UtKSZLtK0lPplAQAADqrw0A3s3+SNMzd/83MPiVpSvLRbyU9mHZxAAAgO5nO0O+U9DVJcvdHJT0qSWb2z8lnF6RYGwAAyFKm6+DD3H3DkY1J26hUKgIAAJ2WKdBP6uCzgR190cx+ZGb1ZvZym7ZbzazOzNYnr/PafPY1M3vNzDab2YysqgcAAJIyB/oaM7vyyEYzu0LS2gzffUDSzHbaf+Du45PXU8n+xkq6WNKpyXf+t5n1zVQ8AABokeka+hclLTez/6q/B/hEScdJmt3RF919hZmNyrKOWZIecvcDkraa2WuSzlDL5DsAAJBBh4Hu7rskTTazaZLGJc2/cPfnu/E755vZpZLWSPqyu++VVCZpdZs+tUkbAADIQrZruVe7+93Jqzthfq+kUySNl7RT0vc7uwMzm2tma8xsze7du7tRCgAAceR0tTd33+XuTe7eLOnf1TKsLkl1kiradC1P2trbx0J3n+juE0tLS9MtGACAXiKngW5mw9tszpZ0aAb845IuNrMBZlYpabRaHgYDAACykO3Sr51mZkskTZU0xMxqJd0iaaqZjZfkkrZJmidJ7v6KmT0saaNanrd+tbs3pVUbAADRpBbo7j6nneZFHfT/lqRvpVUPAACR8cQ0AAACINABAAiAQAcAIAACHQCAAAh0AAACINABAAggtdvWgFDMVFNdI0mtP9tz3IABmjx5Um5qAoA2CHQgG+4aOmaCdkkaOmbCMbvVb16Xu5oAoA0CHehJnMkDyBMCHehJnMkDyBMmxQEAEACBDgBAAAQ6AAABEOgAAARAoAMAEACBDgBAAAQ6AAABEOgAAARAoAMAEACBDgBAAAQ6AAABEOgAAARAoAMAEACBDgBAAAQ6AAABEOgAAARAoAMAEACBDgBAAAQ6AAABEOgAAARAoAMAEACBDgBAAAQ6AAABEOgAAARAoAMAEACBDgBAAAQ6AAABEOgAAARAoAMAEACBDgBAAAQ6AAABEOgAAARAoAMAEACBDgBAAAQ6AAABEOgAAARAoAMAEACBDgBAAAQ6AAABEOgAAARAoAMAEEC/fBcAoGMHm5o1oGRgxn4jysq19fUtOagIQCEi0IEC502Nmn3Pyoz9ll87PQfVAChUDLkDABAAgQ4AQAAEOgAAARDoAAAEQKADABAAgQ4AQAAEOgAAARDoAAAEQKADABAAgQ4AQAAEOgAAARDoAAAEQKADABAAgQ4AQAAEOgAAARDoAAAEQKADABAAgQ4AQAAEOgAAARDoAAAEQKADABAAgQ4AQAAEOgAAAaQW6Gb2IzOrN7OX27SdbGbPmtmW5OegpN3M7C4ze83MXjKzCWnVBQBARGmeoT8gaeYRbTdIes7dR0t6LtmWpI9JGp285kq6N8W6AAAIp19aO3b3FWY26ojmWZKmJu8XS6qR9NWk/cfu7pJWm9lJZjbc3XemVR+QV2aqqa6RpNafANAdqQX6MQxrE9J/kjQseV8maXubfrVJG4GOmNw1dMwE7ZI0dEzHV5h25aYiAL1c3ibFJWfj3tnvmdlcM1tjZmt2796dQmUAAPQ+uQ70XWY2XJKSn/VJe52kijb9ypO2o7j7Qnef6O4TS0tLUy0WAIDeIteB/riky5L3l0l6rE37pcls9zMlvc31cwAAspfaNXQzW6KWCXBDzKxW0i2SviPpYTO7XNIbki5Kuj8l6TxJr0n6m6TPp1UXAAARpTnLfc4xPjqnnb4u6eq0agEAIDpWigMAIAACHQCAAAh0AAACINABAAiAQAcAIAACHQCAAAh0AAACINC7qKxipMws4wsAgFzI9dPWwthRu11VC1Zl7Ld03uQcVAMAKHacoQMAEACBDgBAAAQ6AAABEOgAAARAoAMAEACBDgBAAAQ6AAABEOgAAARAoANBHDx4MKvVC8sqRua7VAApYKU4IAh3Z/VCoIhxhg4AQAAEOgAAATDkDhSRJ2+ukqyPBpQM7LDfiLJybX19S46qAtATCHQgij599cj8aR12aW5q1LCq2zV12tQO+y2/dnrP1QUgJwh0IIrmJp160xMddtnwjfNyVAyAXOMaOgAAARDoAAAEwJA70AvUVNfkuwQABY5AB3qBoWMmZOyzKwd1AChcDLkDABAAgQ4AQAAEOgAAAXANHSg2Zhkn2TU0NMjMNKK8QnXb38xNXQC6hUAHio17xkl2u/v114U/rObJbEAvwpA7AAABEOgAAARAoAMAEACBDgBAAAQ6AAABEOgAAARAoAMAEACBDgBAAAQ6AAABEOgAAARAoAMAEACBDgBAAAQ6AAABEOgAAARAoAMAEACBDgBAAAQ6AAABEOgAAARAoAMAEACBDgBAAAQ6AAABEOgAAARAoAMAEACBDgBAAAQ6AAABEOgAjq1PP5lZxldZxch8VwoUvX75LgBAAWtuVNWCVRm7LZ03OQfFAOgIZ+gAAARAoAMAEACBDgBAAAQ6AAABEOgAAARAoAMAEACBDgBAAAQ6AAABEOgAAARAoAMAEACBDgBAAAQ6AAAB8HAWAEfx5mY9Mn+aZH1afh6LmeQuWR8NKBnYbpcRZeXa+vqWlCoFcAiBDuAo3tykcTc9ofrN6zR0zIRj9tvwjfP0z19/SvWb12nqtKnt9ll+7fR0igRwGIbcAQAIgEAHACCAvAy5m9k2Se9KapLU6O4TzexkSUsljZK0TdJF7r43H/UBANDb5PMMfZq7j3f3icn2DZKec/fRkp5LtgEAQBYKach9lqTFyfvFkj6Zv1IAAOhd8hXoLulXZrbWzOYmbcPcfWfy/k+ShuWnNAAAep983bY2xd3rzGyopGfN7NW2H7q7m5m398XkD4C5kjRy5Mj0KwUAoBfIyxm6u9clP+slLZd0hqRdZjZckpKf9cf47kJ3n+juE0tLS3NVMgAABS3ngW5mx5vZiYfeSzpX0suSHpd0WdLtMkmP5bo2AD3vYFOzBpQMzPiqPGV0vksFerV8DLkPk7TczA79/p+6+zNm9qKkh83scklvSLooD7UB6GHe1KjZ96zM2I8V5YDuyXmgu/sfJZ3eTvseSefkuh4AubNq1W/VcOBAu581NDQo+UNfkjSivEJ129/MVWlAr8da7gBypuHAgWOuDb+7X39d+MPq1u2l8ybnqiwghEK6Dx0AAHQRgQ4AQAAEOgAAARDoAAAEQKADABAAgQ4AQAAEOgAAARDoAAAEQKADABAAgQ4AQAAEOgAAARDoAAAEQKADABAAT1vLgSdvrtL+vfUdd7I+am5qzE1BAIBwCPQc2L+3Xqfe+FiHfeo3r9OupTflqCIAQDQMuQMoTH36ycwyvsoqRua7UqAgcIYOoDA1N6pqwaqM3ZbOm5yDYoDCR6AD6D4z1VTXHPPjjj4D0DMIdADd566hYya0+9EuqfWz+s3rclgUUFy4hg4AQAAEOgAAARDoAAAEQKADABAAgQ4AQAAEOgAAARDoAHo3VpQDJHEfOoDejhXlAEmcoQMAEAKBDgBAAAQ6AAABcA0dQK/x5M1V2r+3/vBG66NH5k87rGngoKE6/5tLc1gZkH8EOoBeY//eep1642OHtdVvXnfUg2Fe+fasXJYFFAQCHUBB8Obmw8+02znzbm5q7PovSG5vy8aI8grVbX+z678LyAMCHUBB8OYmjbvpidbt9s68N3zjvK7/gixvb5O4xQ29E5PiAAAIgEAHACAAAh0AgAC4hg6gaLV7G5wkWR8NKBkoSRpRVq6tr2/JcWVA5xHoAIpWe7fBSS0T8qZOmypJWn7t9NwWBXQRgQ4gnKNugZN6/jY4oMAQ6ADCOfIWOCmF2+CAAsOkOAAAAiDQAQAIgEAHACAAAh0AgAAIdAAAAiDQAQAIgEAHACAAAh0AgABYWAYAjmSmmuoaSVJDQ4PMLL/1AFkg0AHgSO6tq8rt7tdfF/6wut1uS+dNzmVVQIcYcgcAIADO0AGgA+0+6OWQNg98GThoqM7/5tIcVgYcjkAHgA6096CXQ9o+8OWVb8/KZVnAURhyBwAgAAIdAIAACHQAAAIg0AEACIBABwAgAAIdAIAACHQAAAIg0AEACIBABwAgAAIdAIAAWPoVAHpApjXfB5QM1Iiycm19fUtuC0PRINABoAdkWvN96rSpWn7t9NwWhaLCkDsAAAEQ6AAABECgAwAQAIEOAEAATIoDgBw52NSsASUDM/ZjNjy6gkAHgBzxpkbNvmdlxn7MhkdXMOQOAEAABHobZRUjZWZZvQAAKCQMubexo3a7qhasyqrv0nmTU64GAIDscYYOAEAABRfoZjbTzDab2WtmdkO+6wEAoDcoqEA3s76S7pH0MUljJc0xs7H5rQoAgMJXaNfQz5D0mrv/UZLM7CFJsyRtzGtVAFCAKk8ZrR11tR326e33tGdzjFJ+jrPQ/v0LLdDLJG1vs10r6V/yVAsAFLQddbWafdfzHfbp7fe0Z3OMUn6Os9D+/c3dc/bLMjGzz0ia6e5XJNuXSPoXd5/fps9cSXOTzTGSNh9jd0Mk/TnFcgtJsRwrxxlPsRxrsRynVDzHms/j/Ed3Lz2ysdDO0OskVbTZLk/aWrn7QkkLM+3IzNa4+8SeLa8wFcuxcpzxFMuxFstxSsVzrIV4nAU1KU7Si5JGm1mlmR0n6WJJj+e5JgAACl5BnaG7e6OZzZf0S0l9Jf3I3V/Jc1kAABS8ggp0SXL3pyQ91QO7yjgsH0ixHCvHGU+xHGuxHKdUPMdacMdZUJPiAABA1xTaNXQAANAFIQO9WJaPNbMfmVm9mb2c71rSZGYVZlZtZhvN7BUzuy7fNaXBzErM7AUz+3/Jcd6W75rSZGZ9zez3ZvZkvmtJk5ltM7MNZrbezNbku560mNlJZrbMzF41s01mNinfNaXBzMYk/1seer1jZl/Md11SwCH3ZPnYP0j6qFoWpnlR0hx3D7fanJmdLWmfpB+7+7h815MWMxsuabi7rzOzEyWtlfTJaP+bWstzeY93931m1l/SSknXufvqPJeWCjO7XtJESf/g7ufnu560mNk2SRPdPfS92Wa2WNJv3P3+5C6l/+Tuf8lzWalK8qZOLeulvJHveiKeobcuH+vuDZIOLR8bjruvkPRWvutIm7vvdPd1yft3JW1Sy6qCoXiLfclm/+QV6y/uhJmVS/q4pPvzXQu6z8zeJ+lsSYskyd0bood54hxJrxdCmEsxA7295WPD/ce/WJnZKEkfkvS7PJeSimQYer2keknPunvI45R0p6T/Iak5z3Xkgkv6lZmtTVa6jKhS0m5J/ye5jHK/mR2f76Jy4GJJS/JdxCERAx1BmdkJkn4m6Yvu/k6+60mDuze5+3i1rJJ4hpmFu5RiZudLqnf3tfmuJUemuPsEtTxF8urkUlk0/SRNkHSvu39I0l8lhZ2/JEnJZYVPSHok37UcEjHQMy4fi94nuab8M0kPuvuj+a4nbclwZbWkmXkuJQ1nSfpEcm35IUnTzewn+S0pPe5el/ysl7RcLZcFo6mVVNtmRGmZWgI+so9JWufuu/JdyCERA53lY4NJJostkrTJ3e/Idz1pMbNSMzspeT9QLRM7X81rUSlw96+5e7m7j1LL/z+fd/f/lueyUmFmxycTOZUMQZ8rKdxdKe7+J0nbzWxM0nSO4j/2eo4KaLhdKsCV4rqrmJaPNbMlkqZKGmJmtZJucfdF+a0qFWdJukTShuT6siTdmKwqGMlwSYuTmbN9JD3s7qFv6SoCwyQtb/mbVP0k/dTdn8lvSam5RtKDyYnUHyV9Ps/1pCb54+yjkublu5a2wt22BgBAMYo45A4AQNEh0AEACIBABwAgAAIdAIAACHQAAAIg0AEACIBABwAgAAIdAIAA/j92x51AViEa5wAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "y_pred = best_model.predict(x_valid_reg)\n",
    "fig,ax_best = plt.subplots(1,1,figsize = (8,10))\n",
    "sns.histplot(y_valid_reg, ax = ax_best, label = 'Actual')\n",
    "sns.histplot(y_pred, ax = ax_best, label = \"Predicted\")\n",
    "ax_best.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       loss  val_loss\n",
       "0  0.749101  0.565843\n",
       "1  0.516577  0.464196\n",
       "2  0.439561  0.423527\n",
       "3  0.416545  0.421457\n",
       "4  0.405481  0.415570\n",
       "5  0.398528  0.398936\n",
       "6  0.384862  0.410111\n",
       "7  0.367804  0.380877\n",
       "8  0.360016  0.377216\n",
       "9  0.350841  0.362288"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n      <th>val_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.749101</td>\n      <td>0.565843</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.516577</td>\n      <td>0.464196</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.439561</td>\n      <td>0.423527</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.416545</td>\n      <td>0.421457</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.405481</td>\n      <td>0.415570</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.398528</td>\n      <td>0.398936</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.384862</td>\n      <td>0.410111</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.367804</td>\n      <td>0.380877</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.360016</td>\n      <td>0.377216</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.350841</td>\n      <td>0.362288</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "df_best_model = pd.DataFrame(data = history_best_model.history)\n",
    "df_best_model"
   ]
  },
  {
   "source": [
    "With Randomized Search CV, when there is a very large parameter space, this approach will only explore a tiny portion of the hyperparameter space.      \n",
    "\n",
    "To overcome this, first run a quick random search using wide ranges of hyperparameter values, then run another search using smaller range of values centered on the best ones found during the first run. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<h2>General Guidelines When Hyperparameter Tuning</h2>\n",
    "\n",
    "\n",
    "- Number of Hidden Layers\n",
    "\n",
    "Deep neural networks have a much higher parameter efficiency than shallow nets, allowing them to model complex functions using exponentially fewer neurons than shallow nets. \n",
    "\n",
    "Lower hidden layers tend to model low-level structures, intermediate hidden layers combine these low level strutures to model intermediate level structures and the higher hidden layers and the output layer combine these intermediate structures to model high level structures. \n",
    "\n",
    "For example, if you have already trained a model to recognize faces in pictures, and you now want to train a new neural network to recognize hairstyles, then you can kickstart training by reusing the lower layers of the first network.\n",
    "\n",
    "Using the lower layers of a previously built neural network, and attaching new layers to it, so that we can make the network to learn higher level structures using the deeper newer layers. This is called transfer learning. \n",
    "\n",
    "\n",
    "<h2> Learning Rate </h2> \n",
    "\n",
    "- The most important hyperparameter. The optimal learning rate is about half the maximum learning rate(rate at which the algorithm diverges). A simple approach is to start with a large value which makes the algorithm diverge. Divide this by 3 and try again. Repeat until algorithm stops diverging. \n",
    "\n",
    "- Choose a better optimizer.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}