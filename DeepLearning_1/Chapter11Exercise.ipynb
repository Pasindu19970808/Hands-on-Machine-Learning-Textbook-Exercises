{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0965ba1120b0c101b3f715b6e258a73742ec1cf86f2c8b04492724c87d9f112c3",
   "display_name": "Python 3.8.8 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full,y_train_full),(X_test,y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1492d7f31f0>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-05-29T14:37:50.132190</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 251.565 248.518125 \r\nL 251.565 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 26.925 224.64 \r\nL 244.365 224.64 \r\nL 244.365 7.2 \r\nL 26.925 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g clip-path=\"url(#p43612d2adb)\">\r\n    <image height=\"218\" id=\"image2966232fef\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\r\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\" y=\"-6.64\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"md80545a2eb\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#md80545a2eb\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#md80545a2eb\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#md80545a2eb\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#md80545a2eb\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#md80545a2eb\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#md80545a2eb\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mc738aaa6d7\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mc738aaa6d7\" y=\"11.082857\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mc738aaa6d7\" y=\"49.911429\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mc738aaa6d7\" y=\"88.74\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mc738aaa6d7\" y=\"127.568571\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mc738aaa6d7\" y=\"166.397143\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mc738aaa6d7\" y=\"205.225714\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 26.925 224.64 \r\nL 26.925 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 244.365 224.64 \r\nL 244.365 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 26.925 224.64 \r\nL 244.365 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 26.925 7.2 \r\nL 244.365 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p43612d2adb\">\r\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.imshow(X_train_full[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "source": [
    "<h3> DNN Summary </h3> \n",
    "\n",
    "A DNN trained with 5 hiddens layers, 1 Flatten input layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dnn = keras.models.Sequential()\n",
    "model_dnn.add(keras.layers.Flatten(input_shape = [28,28]))\n",
    "model_dnn.add(keras.layers.Dense(100,activation = \"elu\",kernel_initializer=\"he_normal\"))\n",
    "model_dnn.add(keras.layers.Dense(100,activation = \"elu\",kernel_initializer=\"he_normal\"))\n",
    "model_dnn.add(keras.layers.Dense(100,activation = \"elu\",kernel_initializer=\"he_normal\"))\n",
    "model_dnn.add(keras.layers.Dense(100,activation = \"elu\",kernel_initializer=\"he_normal\"))\n",
    "model_dnn.add(keras.layers.Dense(100,activation = \"elu\",kernel_initializer=\"he_normal\"))\n",
    "model_dnn.add(keras.layers.Dense(5, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten (Flatten)            (None, 784)               0         \n_________________________________________________________________\ndense (Dense)                (None, 100)               78500     \n_________________________________________________________________\ndense_1 (Dense)              (None, 100)               10100     \n_________________________________________________________________\ndense_2 (Dense)              (None, 100)               10100     \n_________________________________________________________________\ndense_3 (Dense)              (None, 100)               10100     \n_________________________________________________________________\ndense_4 (Dense)              (None, 100)               10100     \n_________________________________________________________________\ndense_5 (Dense)              (None, 5)                 505       \n=================================================================\nTotal params: 119,405\nTrainable params: 119,405\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_dnn.summary()"
   ]
  },
  {
   "source": [
    "Preparing the MNIST Data. \n",
    "\n",
    "First we shall only train on 0 to 4 digit labels, as we will use the rest for transfer learning."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prepdata0to4(X,y):\n",
    "        y_data = y[np.where((y == 0) | (y == 1) | (y == 2)|(y == 3)| (y == 4))[0]]\n",
    "        X_data = X[np.where((y == 0) | (y == 1) | (y == 2)|(y == 3)| (y == 4))[0]]\n",
    "        return (X_data,y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_data_0to4,y_data_0to4_sparse) = Prepdata0to4(X_train_full,y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_0to4 = keras.utils.to_categorical(y_data_0to4_sparse)\n",
    "X_data_train0to4 = X_data_0to4[:25000]\n",
    "y_data_train0to4 = y_data_0to4[:25000]\n",
    "X_data_valid0to4 = X_data_0to4[25000:]\n",
    "y_data_valid0to4 = y_data_0to4[25000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "y_data_train0to4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(5596, 28, 28)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "X_data_valid0to4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(X_data_test0to4,y_data_test0to4_sparse) = Prepdata0to4(X_test,y_test)\n",
    "y_data_test0to4 = keras.utils.to_categorical(y_data_test0to4_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([2, 1, 0, ..., 2, 3, 4], dtype=uint8)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "y_data_test0to4_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#beta_1 is the momentum for moving mean of gradient\n",
    "#beta_2 is the momentum for moving mean of squared gradient\n",
    "adamoptim = keras.optimizers.Adam(lr = 0.001,beta_1=0.9,beta_2=0.999)\n",
    "#callbacks to use\n",
    "#checkpoints\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"zerotofour.h5\", save_best_only=True)\n",
    "#early stopping callback\n",
    "earlystopping_cb = keras.callbacks.EarlyStopping(patience = 10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics\n",
    "accuracy = keras.metrics.CategoricalAccuracy(name = \"categorical_accuracy\")\n",
    "precision = keras.metrics.Precision()\n",
    "recall = keras.metrics.Recall()\n",
    "aucscore = keras.metrics.AUC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dnn.compile(loss = \"categorical_crossentropy\",\n",
    "                    optimizer = adamoptim, \n",
    "                    metrics = [accuracy,precision,recall,aucscore])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(25000, 28, 28)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "X_data_train0to4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(25000, 5)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "y_data_train0to4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/40\n",
      "782/782 [==============================] - 5s 4ms/step - loss: 5.2510 - categorical_accuracy: 0.8817 - precision: 0.8817 - recall: 0.8816 - auc: 0.9368 - val_loss: 0.3594 - val_categorical_accuracy: 0.9580 - val_precision: 0.9585 - val_recall: 0.9580 - val_auc: 0.9856\n",
      "Epoch 2/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.2667 - categorical_accuracy: 0.9639 - precision: 0.9643 - recall: 0.9637 - auc: 0.9891 - val_loss: 0.2442 - val_categorical_accuracy: 0.9627 - val_precision: 0.9635 - val_recall: 0.9621 - val_auc: 0.9905\n",
      "Epoch 3/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.1690 - categorical_accuracy: 0.9724 - precision: 0.9727 - recall: 0.9721 - auc: 0.9928 - val_loss: 0.1865 - val_categorical_accuracy: 0.9664 - val_precision: 0.9671 - val_recall: 0.9652 - val_auc: 0.9920\n",
      "Epoch 4/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.1328 - categorical_accuracy: 0.9729 - precision: 0.9735 - recall: 0.9724 - auc: 0.9945 - val_loss: 0.1064 - val_categorical_accuracy: 0.9766 - val_precision: 0.9773 - val_recall: 0.9766 - val_auc: 0.9964\n",
      "Epoch 5/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0784 - categorical_accuracy: 0.9817 - precision: 0.9820 - recall: 0.9812 - auc: 0.9972 - val_loss: 0.0936 - val_categorical_accuracy: 0.9761 - val_precision: 0.9788 - val_recall: 0.9744 - val_auc: 0.9967\n",
      "Epoch 6/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0658 - categorical_accuracy: 0.9843 - precision: 0.9852 - recall: 0.9835 - auc: 0.9978 - val_loss: 0.0806 - val_categorical_accuracy: 0.9807 - val_precision: 0.9814 - val_recall: 0.9800 - val_auc: 0.9970\n",
      "Epoch 7/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0650 - categorical_accuracy: 0.9829 - precision: 0.9840 - recall: 0.9822 - auc: 0.9979 - val_loss: 0.0750 - val_categorical_accuracy: 0.9855 - val_precision: 0.9861 - val_recall: 0.9853 - val_auc: 0.9972\n",
      "Epoch 8/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0524 - categorical_accuracy: 0.9869 - precision: 0.9877 - recall: 0.9864 - auc: 0.9985 - val_loss: 0.0711 - val_categorical_accuracy: 0.9812 - val_precision: 0.9832 - val_recall: 0.9809 - val_auc: 0.9978\n",
      "Epoch 9/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0516 - categorical_accuracy: 0.9858 - precision: 0.9870 - recall: 0.9851 - auc: 0.9985 - val_loss: 0.0894 - val_categorical_accuracy: 0.9771 - val_precision: 0.9799 - val_recall: 0.9752 - val_auc: 0.9968\n",
      "Epoch 10/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0478 - categorical_accuracy: 0.9872 - precision: 0.9881 - recall: 0.9868 - auc: 0.9986 - val_loss: 0.0628 - val_categorical_accuracy: 0.9850 - val_precision: 0.9864 - val_recall: 0.9845 - val_auc: 0.9973\n",
      "Epoch 11/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0387 - categorical_accuracy: 0.9898 - precision: 0.9910 - recall: 0.9893 - auc: 0.9986 - val_loss: 0.0615 - val_categorical_accuracy: 0.9837 - val_precision: 0.9858 - val_recall: 0.9830 - val_auc: 0.9979\n",
      "Epoch 12/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0329 - categorical_accuracy: 0.9900 - precision: 0.9913 - recall: 0.9885 - auc: 0.9993 - val_loss: 0.0585 - val_categorical_accuracy: 0.9873 - val_precision: 0.9882 - val_recall: 0.9861 - val_auc: 0.9979\n",
      "Epoch 13/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0266 - categorical_accuracy: 0.9925 - precision: 0.9931 - recall: 0.9914 - auc: 0.9995 - val_loss: 0.0621 - val_categorical_accuracy: 0.9848 - val_precision: 0.9878 - val_recall: 0.9834 - val_auc: 0.9980\n",
      "Epoch 14/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0380 - categorical_accuracy: 0.9895 - precision: 0.9906 - recall: 0.9884 - auc: 0.9991 - val_loss: 0.0708 - val_categorical_accuracy: 0.9852 - val_precision: 0.9876 - val_recall: 0.9825 - val_auc: 0.9974\n",
      "Epoch 15/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0308 - categorical_accuracy: 0.9919 - precision: 0.9929 - recall: 0.9913 - auc: 0.9992 - val_loss: 0.0668 - val_categorical_accuracy: 0.9868 - val_precision: 0.9875 - val_recall: 0.9864 - val_auc: 0.9974\n",
      "Epoch 16/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0239 - categorical_accuracy: 0.9942 - precision: 0.9953 - recall: 0.9934 - auc: 0.9995 - val_loss: 0.0610 - val_categorical_accuracy: 0.9843 - val_precision: 0.9862 - val_recall: 0.9825 - val_auc: 0.9977\n",
      "Epoch 17/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0180 - categorical_accuracy: 0.9945 - precision: 0.9953 - recall: 0.9938 - auc: 0.9996 - val_loss: 0.0562 - val_categorical_accuracy: 0.9878 - val_precision: 0.9885 - val_recall: 0.9866 - val_auc: 0.9981\n",
      "Epoch 18/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0158 - categorical_accuracy: 0.9958 - precision: 0.9967 - recall: 0.9952 - auc: 0.9996 - val_loss: 0.0767 - val_categorical_accuracy: 0.9832 - val_precision: 0.9856 - val_recall: 0.9818 - val_auc: 0.9971\n",
      "Epoch 19/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0167 - categorical_accuracy: 0.9953 - precision: 0.9962 - recall: 0.9948 - auc: 0.9996 - val_loss: 0.0883 - val_categorical_accuracy: 0.9862 - val_precision: 0.9873 - val_recall: 0.9857 - val_auc: 0.9962\n",
      "Epoch 20/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0177 - categorical_accuracy: 0.9953 - precision: 0.9959 - recall: 0.9948 - auc: 0.9996 - val_loss: 0.0784 - val_categorical_accuracy: 0.9871 - val_precision: 0.9878 - val_recall: 0.9857 - val_auc: 0.9969\n",
      "Epoch 21/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0293 - categorical_accuracy: 0.9942 - precision: 0.9950 - recall: 0.9938 - auc: 0.9997 - val_loss: 0.0794 - val_categorical_accuracy: 0.9882 - val_precision: 0.9893 - val_recall: 0.9870 - val_auc: 0.9969\n",
      "Epoch 22/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0247 - categorical_accuracy: 0.9945 - precision: 0.9954 - recall: 0.9938 - auc: 0.9995 - val_loss: 0.0593 - val_categorical_accuracy: 0.9866 - val_precision: 0.9884 - val_recall: 0.9859 - val_auc: 0.9982\n",
      "Epoch 23/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0132 - categorical_accuracy: 0.9967 - precision: 0.9974 - recall: 0.9963 - auc: 0.9997 - val_loss: 0.0548 - val_categorical_accuracy: 0.9900 - val_precision: 0.9905 - val_recall: 0.9893 - val_auc: 0.9981\n",
      "Epoch 24/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0108 - categorical_accuracy: 0.9974 - precision: 0.9978 - recall: 0.9971 - auc: 0.9997 - val_loss: 0.0640 - val_categorical_accuracy: 0.9905 - val_precision: 0.9914 - val_recall: 0.9902 - val_auc: 0.9974\n",
      "Epoch 25/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0187 - categorical_accuracy: 0.9961 - precision: 0.9968 - recall: 0.9954 - auc: 0.9995 - val_loss: 0.0677 - val_categorical_accuracy: 0.9886 - val_precision: 0.9905 - val_recall: 0.9878 - val_auc: 0.9973\n",
      "Epoch 26/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0132 - categorical_accuracy: 0.9967 - precision: 0.9970 - recall: 0.9964 - auc: 0.9996 - val_loss: 0.0495 - val_categorical_accuracy: 0.9902 - val_precision: 0.9910 - val_recall: 0.9891 - val_auc: 0.9978\n",
      "Epoch 27/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0099 - categorical_accuracy: 0.9977 - precision: 0.9979 - recall: 0.9976 - auc: 0.9998 - val_loss: 0.0537 - val_categorical_accuracy: 0.9896 - val_precision: 0.9900 - val_recall: 0.9886 - val_auc: 0.9976\n",
      "Epoch 28/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0091 - categorical_accuracy: 0.9976 - precision: 0.9978 - recall: 0.9974 - auc: 0.9998 - val_loss: 0.0503 - val_categorical_accuracy: 0.9889 - val_precision: 0.9889 - val_recall: 0.9887 - val_auc: 0.9985\n",
      "Epoch 29/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0058 - categorical_accuracy: 0.9984 - precision: 0.9984 - recall: 0.9983 - auc: 0.9999 - val_loss: 0.0562 - val_categorical_accuracy: 0.9875 - val_precision: 0.9887 - val_recall: 0.9857 - val_auc: 0.9980\n",
      "Epoch 30/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0154 - categorical_accuracy: 0.9968 - precision: 0.9971 - recall: 0.9963 - auc: 0.9996 - val_loss: 0.0598 - val_categorical_accuracy: 0.9880 - val_precision: 0.9896 - val_recall: 0.9871 - val_auc: 0.9976\n",
      "Epoch 31/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0079 - categorical_accuracy: 0.9979 - precision: 0.9984 - recall: 0.9978 - auc: 0.9998 - val_loss: 0.0856 - val_categorical_accuracy: 0.9889 - val_precision: 0.9896 - val_recall: 0.9886 - val_auc: 0.9970\n",
      "Epoch 32/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0115 - categorical_accuracy: 0.9975 - precision: 0.9980 - recall: 0.9972 - auc: 0.9996 - val_loss: 0.0889 - val_categorical_accuracy: 0.9864 - val_precision: 0.9871 - val_recall: 0.9857 - val_auc: 0.9966\n",
      "Epoch 33/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0171 - categorical_accuracy: 0.9966 - precision: 0.9970 - recall: 0.9959 - auc: 0.9995 - val_loss: 0.1184 - val_categorical_accuracy: 0.9880 - val_precision: 0.9882 - val_recall: 0.9875 - val_auc: 0.9960\n",
      "Epoch 34/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0110 - categorical_accuracy: 0.9966 - precision: 0.9973 - recall: 0.9961 - auc: 0.9998 - val_loss: 0.1093 - val_categorical_accuracy: 0.9911 - val_precision: 0.9914 - val_recall: 0.9905 - val_auc: 0.9970\n",
      "Epoch 35/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0077 - categorical_accuracy: 0.9985 - precision: 0.9987 - recall: 0.9985 - auc: 0.9998 - val_loss: 0.1007 - val_categorical_accuracy: 0.9889 - val_precision: 0.9898 - val_recall: 0.9873 - val_auc: 0.9971\n",
      "Epoch 36/40\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.0163 - categorical_accuracy: 0.9966 - precision: 0.9971 - recall: 0.9960 - auc: 0.9995 - val_loss: 0.1011 - val_categorical_accuracy: 0.9900 - val_precision: 0.9907 - val_recall: 0.9895 - val_auc: 0.9971\n"
     ]
    }
   ],
   "source": [
    "model_dnn_history1 = model_dnn.fit(X_data_train0to4,y_data_train0to4, epochs = 40, validation_data=(X_data_valid0to4,y_data_valid0to4), callbacks = [checkpoint_cb,earlystopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting on validation data, test data and making confusion matrices\n",
    "y_pred_valid = np.argmax(model_dnn.predict(X_data_valid0to4),axis = 1)\n",
    "y_pred_test = np.argmax(model_dnn.predict(X_data_test0to4),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1097,    0,    3,    0,    0],\n",
       "       [   0, 1167,    1,    2,    5],\n",
       "       [   4,    3, 1077,    6,    5],\n",
       "       [   2,    0,   15, 1132,    2],\n",
       "       [   1,    3,    2,    1, 1068]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score,auc\n",
    "confusion_matrix(y_data_0to4_sparse[25000:],y_pred_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9901803407018425"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    " precision_score(y_data_0to4_sparse[25000:],y_pred_valid, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9901715511079342"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "recall_score(y_data_0to4_sparse[25000:],y_pred_valid, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 977,    0,    0,    3,    0],\n",
       "       [   1, 1126,    3,    2,    3],\n",
       "       [   5,    2, 1017,    2,    6],\n",
       "       [   0,    0,    4, 1005,    1],\n",
       "       [   2,    0,    3,    1,  976]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "#on test data\n",
    "confusion_matrix(y_data_test0to4_sparse,y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9926140829447524"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    " precision_score(y_data_test0to4_sparse,y_pred_test, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9926055652850749"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "recall_score(y_data_test0to4_sparse,y_pred_test, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        loss  categorical_accuracy  precision   recall       auc  val_loss  \\\n",
       "0   1.714488               0.92904   0.929172  0.92880  0.966800  0.359379   \n",
       "1   0.245397               0.96488   0.965296  0.96464  0.990201  0.244235   \n",
       "2   0.164445               0.97244   0.972858  0.97208  0.993009  0.186471   \n",
       "3   0.119872               0.97516   0.975969  0.97472  0.995196  0.106354   \n",
       "4   0.080945               0.98212   0.982660  0.98152  0.997088  0.093580   \n",
       "5   0.070238               0.98284   0.983895  0.98236  0.997618  0.080642   \n",
       "6   0.065008               0.98368   0.984811  0.98296  0.998049  0.075044   \n",
       "7   0.061378               0.98516   0.985902  0.98464  0.998044  0.071085   \n",
       "8   0.055906               0.98592   0.987133  0.98508  0.998349  0.089357   \n",
       "9   0.040297               0.98884   0.989903  0.98828  0.998952  0.062814   \n",
       "10  0.045433               0.98824   0.989577  0.98736  0.998640  0.061472   \n",
       "11  0.037133               0.98984   0.991139  0.98880  0.999011  0.058456   \n",
       "12  0.030726               0.99072   0.991705  0.98996  0.999360  0.062088   \n",
       "13  0.032808               0.99116   0.992026  0.99028  0.999236  0.070773   \n",
       "14  0.032001               0.99100   0.992225  0.99032  0.999175  0.066760   \n",
       "15  0.023101               0.99380   0.994871  0.99320  0.999500  0.060971   \n",
       "16  0.023547               0.99356   0.994511  0.99280  0.999451  0.056217   \n",
       "17  0.020578               0.99448   0.995234  0.99388  0.999466  0.076729   \n",
       "18  0.018975               0.99472   0.995394  0.99416  0.999521  0.088349   \n",
       "19  0.016599               0.99572   0.996356  0.99524  0.999628  0.078398   \n",
       "20  0.025348               0.99416   0.995113  0.99368  0.999473  0.079391   \n",
       "21  0.020145               0.99524   0.995834  0.99440  0.999506  0.059317   \n",
       "22  0.013054               0.99656   0.997117  0.99608  0.999720  0.054803   \n",
       "23  0.015067               0.99632   0.996797  0.99576  0.999618  0.063984   \n",
       "24  0.019434               0.99552   0.996354  0.99484  0.999521  0.067679   \n",
       "25  0.015903               0.99636   0.996877  0.99596  0.999552  0.049540   \n",
       "26  0.012721               0.99716   0.997439  0.99692  0.999707  0.053667   \n",
       "27  0.011124               0.99724   0.997638  0.99700  0.999684  0.050312   \n",
       "28  0.011625               0.99748   0.997639  0.99736  0.999711  0.056202   \n",
       "29  0.016975               0.99636   0.996678  0.99604  0.999521  0.059756   \n",
       "30  0.010204               0.99748   0.997839  0.99732  0.999652  0.085624   \n",
       "31  0.015530               0.99688   0.997278  0.99652  0.999587  0.088935   \n",
       "32  0.011116               0.99732   0.997638  0.99688  0.999742  0.118386   \n",
       "33  0.010371               0.99716   0.997678  0.99676  0.999732  0.109305   \n",
       "34  0.010745               0.99804   0.998240  0.99796  0.999664  0.100684   \n",
       "35  0.016762               0.99656   0.996997  0.99604  0.999487  0.101104   \n",
       "\n",
       "    val_categorical_accuracy  val_precision  val_recall   val_auc  \n",
       "0                   0.958006       0.958520    0.958006  0.985630  \n",
       "1                   0.962652       0.963493    0.962116  0.990539  \n",
       "2                   0.966405       0.967055    0.965154  0.991950  \n",
       "3                   0.976590       0.977289    0.976590  0.996429  \n",
       "4                   0.976054       0.978819    0.974446  0.996731  \n",
       "5                   0.980700       0.981389    0.979986  0.996982  \n",
       "6                   0.985525       0.986051    0.985347  0.997186  \n",
       "7                   0.981237       0.983163    0.980879  0.997844  \n",
       "8                   0.977127       0.979889    0.975161  0.996801  \n",
       "9                   0.984989       0.986392    0.984453  0.997251  \n",
       "10                  0.983738       0.985842    0.983024  0.997942  \n",
       "11                  0.987312       0.988181    0.986061  0.997863  \n",
       "12                  0.984811       0.987794    0.983381  0.997977  \n",
       "13                  0.985168       0.987606    0.982487  0.997360  \n",
       "14                  0.986776       0.987478    0.986419  0.997406  \n",
       "15                  0.984275       0.986188    0.982487  0.997682  \n",
       "16                  0.987848       0.988541    0.986598  0.998129  \n",
       "17                  0.983202       0.985648    0.981773  0.997126  \n",
       "18                  0.986240       0.987292    0.985704  0.996209  \n",
       "19                  0.987134       0.987822    0.985704  0.996927  \n",
       "20                  0.988206       0.989253    0.986955  0.996855  \n",
       "21                  0.986598       0.988355    0.985883  0.998225  \n",
       "22                  0.989993       0.990517    0.989278  0.998110  \n",
       "23                  0.990529       0.991412    0.990172  0.997401  \n",
       "24                  0.988563       0.990503    0.987848  0.997306  \n",
       "25                  0.990172       0.991047    0.989099  0.997845  \n",
       "26                  0.989635       0.989979    0.988563  0.997574  \n",
       "27                  0.988921       0.988919    0.988742  0.998500  \n",
       "28                  0.987491       0.988708    0.985704  0.997962  \n",
       "29                  0.988027       0.989609    0.987134  0.997628  \n",
       "30                  0.988921       0.989624    0.988563  0.997004  \n",
       "31                  0.986419       0.987115    0.985704  0.996620  \n",
       "32                  0.988027       0.988197    0.987491  0.996037  \n",
       "33                  0.991065       0.991415    0.990529  0.996963  \n",
       "34                  0.988921       0.989789    0.987312  0.997110  \n",
       "35                  0.989993       0.990696    0.989457  0.997077  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n      <th>categorical_accuracy</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>auc</th>\n      <th>val_loss</th>\n      <th>val_categorical_accuracy</th>\n      <th>val_precision</th>\n      <th>val_recall</th>\n      <th>val_auc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.714488</td>\n      <td>0.92904</td>\n      <td>0.929172</td>\n      <td>0.92880</td>\n      <td>0.966800</td>\n      <td>0.359379</td>\n      <td>0.958006</td>\n      <td>0.958520</td>\n      <td>0.958006</td>\n      <td>0.985630</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.245397</td>\n      <td>0.96488</td>\n      <td>0.965296</td>\n      <td>0.96464</td>\n      <td>0.990201</td>\n      <td>0.244235</td>\n      <td>0.962652</td>\n      <td>0.963493</td>\n      <td>0.962116</td>\n      <td>0.990539</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.164445</td>\n      <td>0.97244</td>\n      <td>0.972858</td>\n      <td>0.97208</td>\n      <td>0.993009</td>\n      <td>0.186471</td>\n      <td>0.966405</td>\n      <td>0.967055</td>\n      <td>0.965154</td>\n      <td>0.991950</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.119872</td>\n      <td>0.97516</td>\n      <td>0.975969</td>\n      <td>0.97472</td>\n      <td>0.995196</td>\n      <td>0.106354</td>\n      <td>0.976590</td>\n      <td>0.977289</td>\n      <td>0.976590</td>\n      <td>0.996429</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.080945</td>\n      <td>0.98212</td>\n      <td>0.982660</td>\n      <td>0.98152</td>\n      <td>0.997088</td>\n      <td>0.093580</td>\n      <td>0.976054</td>\n      <td>0.978819</td>\n      <td>0.974446</td>\n      <td>0.996731</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.070238</td>\n      <td>0.98284</td>\n      <td>0.983895</td>\n      <td>0.98236</td>\n      <td>0.997618</td>\n      <td>0.080642</td>\n      <td>0.980700</td>\n      <td>0.981389</td>\n      <td>0.979986</td>\n      <td>0.996982</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.065008</td>\n      <td>0.98368</td>\n      <td>0.984811</td>\n      <td>0.98296</td>\n      <td>0.998049</td>\n      <td>0.075044</td>\n      <td>0.985525</td>\n      <td>0.986051</td>\n      <td>0.985347</td>\n      <td>0.997186</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.061378</td>\n      <td>0.98516</td>\n      <td>0.985902</td>\n      <td>0.98464</td>\n      <td>0.998044</td>\n      <td>0.071085</td>\n      <td>0.981237</td>\n      <td>0.983163</td>\n      <td>0.980879</td>\n      <td>0.997844</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.055906</td>\n      <td>0.98592</td>\n      <td>0.987133</td>\n      <td>0.98508</td>\n      <td>0.998349</td>\n      <td>0.089357</td>\n      <td>0.977127</td>\n      <td>0.979889</td>\n      <td>0.975161</td>\n      <td>0.996801</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.040297</td>\n      <td>0.98884</td>\n      <td>0.989903</td>\n      <td>0.98828</td>\n      <td>0.998952</td>\n      <td>0.062814</td>\n      <td>0.984989</td>\n      <td>0.986392</td>\n      <td>0.984453</td>\n      <td>0.997251</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.045433</td>\n      <td>0.98824</td>\n      <td>0.989577</td>\n      <td>0.98736</td>\n      <td>0.998640</td>\n      <td>0.061472</td>\n      <td>0.983738</td>\n      <td>0.985842</td>\n      <td>0.983024</td>\n      <td>0.997942</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.037133</td>\n      <td>0.98984</td>\n      <td>0.991139</td>\n      <td>0.98880</td>\n      <td>0.999011</td>\n      <td>0.058456</td>\n      <td>0.987312</td>\n      <td>0.988181</td>\n      <td>0.986061</td>\n      <td>0.997863</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.030726</td>\n      <td>0.99072</td>\n      <td>0.991705</td>\n      <td>0.98996</td>\n      <td>0.999360</td>\n      <td>0.062088</td>\n      <td>0.984811</td>\n      <td>0.987794</td>\n      <td>0.983381</td>\n      <td>0.997977</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.032808</td>\n      <td>0.99116</td>\n      <td>0.992026</td>\n      <td>0.99028</td>\n      <td>0.999236</td>\n      <td>0.070773</td>\n      <td>0.985168</td>\n      <td>0.987606</td>\n      <td>0.982487</td>\n      <td>0.997360</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.032001</td>\n      <td>0.99100</td>\n      <td>0.992225</td>\n      <td>0.99032</td>\n      <td>0.999175</td>\n      <td>0.066760</td>\n      <td>0.986776</td>\n      <td>0.987478</td>\n      <td>0.986419</td>\n      <td>0.997406</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.023101</td>\n      <td>0.99380</td>\n      <td>0.994871</td>\n      <td>0.99320</td>\n      <td>0.999500</td>\n      <td>0.060971</td>\n      <td>0.984275</td>\n      <td>0.986188</td>\n      <td>0.982487</td>\n      <td>0.997682</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.023547</td>\n      <td>0.99356</td>\n      <td>0.994511</td>\n      <td>0.99280</td>\n      <td>0.999451</td>\n      <td>0.056217</td>\n      <td>0.987848</td>\n      <td>0.988541</td>\n      <td>0.986598</td>\n      <td>0.998129</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.020578</td>\n      <td>0.99448</td>\n      <td>0.995234</td>\n      <td>0.99388</td>\n      <td>0.999466</td>\n      <td>0.076729</td>\n      <td>0.983202</td>\n      <td>0.985648</td>\n      <td>0.981773</td>\n      <td>0.997126</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.018975</td>\n      <td>0.99472</td>\n      <td>0.995394</td>\n      <td>0.99416</td>\n      <td>0.999521</td>\n      <td>0.088349</td>\n      <td>0.986240</td>\n      <td>0.987292</td>\n      <td>0.985704</td>\n      <td>0.996209</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.016599</td>\n      <td>0.99572</td>\n      <td>0.996356</td>\n      <td>0.99524</td>\n      <td>0.999628</td>\n      <td>0.078398</td>\n      <td>0.987134</td>\n      <td>0.987822</td>\n      <td>0.985704</td>\n      <td>0.996927</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.025348</td>\n      <td>0.99416</td>\n      <td>0.995113</td>\n      <td>0.99368</td>\n      <td>0.999473</td>\n      <td>0.079391</td>\n      <td>0.988206</td>\n      <td>0.989253</td>\n      <td>0.986955</td>\n      <td>0.996855</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.020145</td>\n      <td>0.99524</td>\n      <td>0.995834</td>\n      <td>0.99440</td>\n      <td>0.999506</td>\n      <td>0.059317</td>\n      <td>0.986598</td>\n      <td>0.988355</td>\n      <td>0.985883</td>\n      <td>0.998225</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.013054</td>\n      <td>0.99656</td>\n      <td>0.997117</td>\n      <td>0.99608</td>\n      <td>0.999720</td>\n      <td>0.054803</td>\n      <td>0.989993</td>\n      <td>0.990517</td>\n      <td>0.989278</td>\n      <td>0.998110</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.015067</td>\n      <td>0.99632</td>\n      <td>0.996797</td>\n      <td>0.99576</td>\n      <td>0.999618</td>\n      <td>0.063984</td>\n      <td>0.990529</td>\n      <td>0.991412</td>\n      <td>0.990172</td>\n      <td>0.997401</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.019434</td>\n      <td>0.99552</td>\n      <td>0.996354</td>\n      <td>0.99484</td>\n      <td>0.999521</td>\n      <td>0.067679</td>\n      <td>0.988563</td>\n      <td>0.990503</td>\n      <td>0.987848</td>\n      <td>0.997306</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.015903</td>\n      <td>0.99636</td>\n      <td>0.996877</td>\n      <td>0.99596</td>\n      <td>0.999552</td>\n      <td>0.049540</td>\n      <td>0.990172</td>\n      <td>0.991047</td>\n      <td>0.989099</td>\n      <td>0.997845</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.012721</td>\n      <td>0.99716</td>\n      <td>0.997439</td>\n      <td>0.99692</td>\n      <td>0.999707</td>\n      <td>0.053667</td>\n      <td>0.989635</td>\n      <td>0.989979</td>\n      <td>0.988563</td>\n      <td>0.997574</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.011124</td>\n      <td>0.99724</td>\n      <td>0.997638</td>\n      <td>0.99700</td>\n      <td>0.999684</td>\n      <td>0.050312</td>\n      <td>0.988921</td>\n      <td>0.988919</td>\n      <td>0.988742</td>\n      <td>0.998500</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.011625</td>\n      <td>0.99748</td>\n      <td>0.997639</td>\n      <td>0.99736</td>\n      <td>0.999711</td>\n      <td>0.056202</td>\n      <td>0.987491</td>\n      <td>0.988708</td>\n      <td>0.985704</td>\n      <td>0.997962</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.016975</td>\n      <td>0.99636</td>\n      <td>0.996678</td>\n      <td>0.99604</td>\n      <td>0.999521</td>\n      <td>0.059756</td>\n      <td>0.988027</td>\n      <td>0.989609</td>\n      <td>0.987134</td>\n      <td>0.997628</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.010204</td>\n      <td>0.99748</td>\n      <td>0.997839</td>\n      <td>0.99732</td>\n      <td>0.999652</td>\n      <td>0.085624</td>\n      <td>0.988921</td>\n      <td>0.989624</td>\n      <td>0.988563</td>\n      <td>0.997004</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.015530</td>\n      <td>0.99688</td>\n      <td>0.997278</td>\n      <td>0.99652</td>\n      <td>0.999587</td>\n      <td>0.088935</td>\n      <td>0.986419</td>\n      <td>0.987115</td>\n      <td>0.985704</td>\n      <td>0.996620</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>0.011116</td>\n      <td>0.99732</td>\n      <td>0.997638</td>\n      <td>0.99688</td>\n      <td>0.999742</td>\n      <td>0.118386</td>\n      <td>0.988027</td>\n      <td>0.988197</td>\n      <td>0.987491</td>\n      <td>0.996037</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.010371</td>\n      <td>0.99716</td>\n      <td>0.997678</td>\n      <td>0.99676</td>\n      <td>0.999732</td>\n      <td>0.109305</td>\n      <td>0.991065</td>\n      <td>0.991415</td>\n      <td>0.990529</td>\n      <td>0.996963</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.010745</td>\n      <td>0.99804</td>\n      <td>0.998240</td>\n      <td>0.99796</td>\n      <td>0.999664</td>\n      <td>0.100684</td>\n      <td>0.988921</td>\n      <td>0.989789</td>\n      <td>0.987312</td>\n      <td>0.997110</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>0.016762</td>\n      <td>0.99656</td>\n      <td>0.996997</td>\n      <td>0.99604</td>\n      <td>0.999487</td>\n      <td>0.101104</td>\n      <td>0.989993</td>\n      <td>0.990696</td>\n      <td>0.989457</td>\n      <td>0.997077</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "#learning curves for lr = 0.01, beta1 = 0.9, beta2 = 0.9994\n",
    "import pandas as pd\n",
    "model_dnn_history1_df = pd.DataFrame(data = model_dnn_history1.history)\n",
    "model_dnn_history1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of layers is fixed, only thing being changed is learning rate, beta1, beta2\n",
    "def validating_dnn_model_0to4(lr = 0.001, beta1 = 0.9, beta2 = 0.999):\n",
    "    #set for only 5 hidden layers\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape = [28,28]))\n",
    "    for i in range(5):\n",
    "        model.add(keras.layers.Dense(100,activation = \"elu\",kernel_initializer=\"he_normal\"))\n",
    "    model.add(keras.layers.Dense(5, activation=\"softmax\"))\n",
    "    optim = keras.optimizers.Adam(lr = lr,beta_1=beta1,beta_2=beta2)\n",
    "    accuracy = keras.metrics.CategoricalAccuracy(name = \"categorical_accuracy\")\n",
    "    precision = keras.metrics.Precision()\n",
    "    recall = keras.metrics.Recall()\n",
    "    aucscore = keras.metrics.AUC()\n",
    "    model.compile(loss = \"categorical_crossentropy\",\n",
    "                    optimizer = optim, metrics = [accuracy,precision,recall,aucscore])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_dnn_model_0to4 = keras.wrappers.scikit_learn.KerasRegressor(validating_dnn_model_0to4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint(\"zerotofour.h5\", save_best_only=True)\n",
    "earlystopping = keras.callbacks.EarlyStopping(patience = 10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" scoring_dnn = {\n",
    "    \"Precision\" : make_scorer(precision),\n",
    "    \"Accuracy\" : make_scorer(accuracy), \n",
    "    \"Recall\" : make_scorer(recall), \n",
    "    \"AUC\" : make_scorer(aucscore)\n",
    "} \"\"\"\n",
    "\n",
    "param_grid_dnn_0to4 = {\n",
    "    'lr' : (0.0005,0.001, 0.005, 0.01, 0.05, 0.1),\n",
    "    'beta1' : (0.4,0.5,0.6,0.7,0.8),\n",
    "    'beta2' : (0.6,0.7,0.8,0.9,0.999)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_dnn = RandomizedSearchCV(estimator = keras_dnn_model_0to4, param_distributions = param_grid_dnn_0to4, verbose = 2, return_train_score = True, n_iter = 10, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.0000e+00 - val_recall_48: 0.0000e+00 - val_auc_48: 0.4961\n",
      "Epoch 2/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.2740 - categorical_accuracy: 0.2055 - precision_48: 0.2045 - recall_48: 0.1036 - auc_48: 0.5041 - val_loss: 3.8227 - val_categorical_accuracy: 0.2057 - val_precision_48: 0.2057 - val_recall_48: 0.2057 - val_auc_48: 0.5052\n",
      "Epoch 3/40\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 2.3292 - categorical_accuracy: 0.1993 - precision_48: 0.1999 - recall_48: 0.1145 - auc_48: 0.4995 - val_loss: 2.3477 - val_categorical_accuracy: 0.1957 - val_precision_48: 0.0000e+00 - val_recall_48: 0.0000e+00 - val_auc_48: 0.4995\n",
      "Epoch 4/40\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 2.3127 - categorical_accuracy: 0.2024 - precision_48: 0.2005 - recall_48: 0.1134 - auc_48: 0.4971 - val_loss: 2.6131 - val_categorical_accuracy: 0.1957 - val_precision_48: 0.1957 - val_recall_48: 0.1957 - val_auc_48: 0.4939\n",
      "Epoch 5/40\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 2.3449 - categorical_accuracy: 0.2030 - precision_48: 0.2050 - recall_48: 0.1204 - auc_48: 0.5058 - val_loss: 2.7936 - val_categorical_accuracy: 0.2100 - val_precision_48: 0.0000e+00 - val_recall_48: 0.0000e+00 - val_auc_48: 0.5094\n",
      "Epoch 6/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3392 - categorical_accuracy: 0.2016 - precision_48: 0.2048 - recall_48: 0.1088 - auc_48: 0.5052 - val_loss: 2.8840 - val_categorical_accuracy: 0.1921 - val_precision_48: 0.1921 - val_recall_48: 0.1921 - val_auc_48: 0.4888\n",
      "Epoch 7/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3294 - categorical_accuracy: 0.1962 - precision_48: 0.1956 - recall_48: 0.1102 - auc_48: 0.4998 - val_loss: 3.3962 - val_categorical_accuracy: 0.2100 - val_precision_48: 0.2100 - val_recall_48: 0.2100 - val_auc_48: 0.5101\n",
      "Epoch 8/40\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 2.3634 - categorical_accuracy: 0.1960 - precision_48: 0.2005 - recall_48: 0.1096 - auc_48: 0.4950 - val_loss: 1.9188 - val_categorical_accuracy: 0.1921 - val_precision_48: 0.0000e+00 - val_recall_48: 0.0000e+00 - val_auc_48: 0.5003\n",
      "Epoch 9/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3188 - categorical_accuracy: 0.1966 - precision_48: 0.1967 - recall_48: 0.1113 - auc_48: 0.4982 - val_loss: 2.0841 - val_categorical_accuracy: 0.1966 - val_precision_48: 0.0000e+00 - val_recall_48: 0.0000e+00 - val_auc_48: 0.4987\n",
      "Epoch 10/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3392 - categorical_accuracy: 0.1962 - precision_48: 0.1930 - recall_48: 0.1094 - auc_48: 0.4957 - val_loss: 1.7002 - val_categorical_accuracy: 0.2057 - val_precision_48: 0.0000e+00 - val_recall_48: 0.0000e+00 - val_auc_48: 0.5039\n",
      "Epoch 11/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.2779 - categorical_accuracy: 0.1999 - precision_48: 0.1924 - recall_48: 0.0981 - auc_48: 0.4986 - val_loss: 2.4814 - val_categorical_accuracy: 0.2100 - val_precision_48: 0.0000e+00 - val_recall_48: 0.0000e+00 - val_auc_48: 0.5033\n",
      "Epoch 12/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3449 - categorical_accuracy: 0.2003 - precision_48: 0.2036 - recall_48: 0.1050 - auc_48: 0.5026 - val_loss: 2.1279 - val_categorical_accuracy: 0.2100 - val_precision_48: 0.2100 - val_recall_48: 0.2100 - val_auc_48: 0.5067\n",
      "Epoch 13/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3172 - categorical_accuracy: 0.1969 - precision_48: 0.2014 - recall_48: 0.1064 - auc_48: 0.4946 - val_loss: 1.9360 - val_categorical_accuracy: 0.2100 - val_precision_48: 0.2100 - val_recall_48: 0.2100 - val_auc_48: 0.5033\n",
      "Epoch 14/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3844 - categorical_accuracy: 0.2010 - precision_48: 0.2038 - recall_48: 0.1295 - auc_48: 0.5016 - val_loss: 1.9219 - val_categorical_accuracy: 0.1966 - val_precision_48: 0.0000e+00 - val_recall_48: 0.0000e+00 - val_auc_48: 0.4971\n",
      "Epoch 15/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.2744 - categorical_accuracy: 0.2085 - precision_48: 0.2137 - recall_48: 0.1121 - auc_48: 0.5075 - val_loss: 2.0324 - val_categorical_accuracy: 0.1957 - val_precision_48: 0.0000e+00 - val_recall_48: 0.0000e+00 - val_auc_48: 0.5029\n",
      "Epoch 16/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3724 - categorical_accuracy: 0.1990 - precision_48: 0.1921 - recall_48: 0.1076 - auc_48: 0.5005 - val_loss: 2.3582 - val_categorical_accuracy: 0.1921 - val_precision_48: 0.0000e+00 - val_recall_48: 0.0000e+00 - val_auc_48: 0.4913\n",
      "Epoch 17/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3297 - categorical_accuracy: 0.1954 - precision_48: 0.1938 - recall_48: 0.1114 - auc_48: 0.4941 - val_loss: 1.7577 - val_categorical_accuracy: 0.2100 - val_precision_48: 0.0000e+00 - val_recall_48: 0.0000e+00 - val_auc_48: 0.5010\n",
      "Epoch 18/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3618 - categorical_accuracy: 0.2005 - precision_48: 0.1965 - recall_48: 0.1153 - auc_48: 0.4980 - val_loss: 3.1196 - val_categorical_accuracy: 0.1966 - val_precision_48: 0.1966 - val_recall_48: 0.1966 - val_auc_48: 0.4994\n",
      "Epoch 19/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3257 - categorical_accuracy: 0.2069 - precision_48: 0.2115 - recall_48: 0.1188 - auc_48: 0.5042 - val_loss: 1.9288 - val_categorical_accuracy: 0.1921 - val_precision_48: 0.0000e+00 - val_recall_48: 0.0000e+00 - val_auc_48: 0.4890\n",
      "Epoch 20/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3126 - categorical_accuracy: 0.2075 - precision_48: 0.2004 - recall_48: 0.1106 - auc_48: 0.5043 - val_loss: 1.9155 - val_categorical_accuracy: 0.2100 - val_precision_48: 0.0000e+00 - val_recall_48: 0.0000e+00 - val_auc_48: 0.5101\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 1.7058 - categorical_accuracy: 0.1962 - precision_48: 0.0000e+00 - recall_48: 0.0000e+00 - auc_48: 0.4978\n",
      "625/625 [==============================] - 2s 2ms/step - loss: 1.6993 - categorical_accuracy: 0.1999 - precision_48: 0.0000e+00 - recall_48: 0.0000e+00 - auc_48: 0.5028\n",
      "[CV] END ......................beta1=0.6, beta2=0.9, lr=0.05; total time=  43.5s\n",
      "Epoch 1/40\n",
      "625/625 [==============================] - 4s 5ms/step - loss: 330.0901 - categorical_accuracy: 0.2778 - precision_49: 0.3070 - recall_49: 0.1964 - auc_49: 0.5807 - val_loss: 2.2977 - val_categorical_accuracy: 0.1919 - val_precision_49: 0.0000e+00 - val_recall_49: 0.0000e+00 - val_auc_49: 0.4933\n",
      "Epoch 2/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3563 - categorical_accuracy: 0.1948 - precision_49: 0.1990 - recall_49: 0.1107 - auc_49: 0.4966 - val_loss: 2.3628 - val_categorical_accuracy: 0.2100 - val_precision_49: 0.2100 - val_recall_49: 0.2100 - val_auc_49: 0.5077\n",
      "Epoch 3/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3805 - categorical_accuracy: 0.1989 - precision_49: 0.1961 - recall_49: 0.1082 - auc_49: 0.4981 - val_loss: 1.7447 - val_categorical_accuracy: 0.1966 - val_precision_49: 0.0000e+00 - val_recall_49: 0.0000e+00 - val_auc_49: 0.5033\n",
      "Epoch 4/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3020 - categorical_accuracy: 0.2050 - precision_49: 0.1989 - recall_49: 0.1074 - auc_49: 0.5021 - val_loss: 2.2160 - val_categorical_accuracy: 0.1966 - val_precision_49: 0.1966 - val_recall_49: 0.1966 - val_auc_49: 0.5033\n",
      "Epoch 5/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.6180 - categorical_accuracy: 0.1987 - precision_49: 0.1975 - recall_49: 0.1108 - auc_49: 0.5015 - val_loss: 2.7640 - val_categorical_accuracy: 0.1957 - val_precision_49: 0.1957 - val_recall_49: 0.1957 - val_auc_49: 0.4984\n",
      "Epoch 6/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3150 - categorical_accuracy: 0.1992 - precision_49: 0.2009 - recall_49: 0.1138 - auc_49: 0.4974 - val_loss: 1.9837 - val_categorical_accuracy: 0.1966 - val_precision_49: 0.0000e+00 - val_recall_49: 0.0000e+00 - val_auc_49: 0.5012\n",
      "Epoch 7/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3920 - categorical_accuracy: 0.1995 - precision_49: 0.1972 - recall_49: 0.1214 - auc_49: 0.5037 - val_loss: 2.9857 - val_categorical_accuracy: 0.2057 - val_precision_49: 0.2057 - val_recall_49: 0.2057 - val_auc_49: 0.4970\n",
      "Epoch 8/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3287 - categorical_accuracy: 0.2042 - precision_49: 0.2104 - recall_49: 0.1287 - auc_49: 0.5061 - val_loss: 2.7909 - val_categorical_accuracy: 0.2057 - val_precision_49: 0.0000e+00 - val_recall_49: 0.0000e+00 - val_auc_49: 0.4987\n",
      "Epoch 9/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3284 - categorical_accuracy: 0.1985 - precision_49: 0.2001 - recall_49: 0.1050 - auc_49: 0.4978 - val_loss: 3.0159 - val_categorical_accuracy: 0.1957 - val_precision_49: 0.1957 - val_recall_49: 0.1957 - val_auc_49: 0.4985\n",
      "Epoch 10/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3671 - categorical_accuracy: 0.1939 - precision_49: 0.1935 - recall_49: 0.1166 - auc_49: 0.4925 - val_loss: 2.1664 - val_categorical_accuracy: 0.1966 - val_precision_49: 0.1966 - val_recall_49: 0.1966 - val_auc_49: 0.4942\n",
      "Epoch 11/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3848 - categorical_accuracy: 0.2007 - precision_49: 0.1923 - recall_49: 0.1091 - auc_49: 0.5021 - val_loss: 2.8715 - val_categorical_accuracy: 0.1921 - val_precision_49: 0.1921 - val_recall_49: 0.1921 - val_auc_49: 0.4968\n",
      "Epoch 12/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3733 - categorical_accuracy: 0.1925 - precision_49: 0.1911 - recall_49: 0.1155 - auc_49: 0.5013 - val_loss: 2.3631 - val_categorical_accuracy: 0.1966 - val_precision_49: 0.1966 - val_recall_49: 0.1966 - val_auc_49: 0.4990\n",
      "Epoch 13/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3393 - categorical_accuracy: 0.1932 - precision_49: 0.1917 - recall_49: 0.1134 - auc_49: 0.5006 - val_loss: 1.7799 - val_categorical_accuracy: 0.2100 - val_precision_49: 0.0000e+00 - val_recall_49: 0.0000e+00 - val_auc_49: 0.5083\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 1.7392 - categorical_accuracy: 0.1898 - precision_49: 0.0000e+00 - recall_49: 0.0000e+00 - auc_49: 0.5044\n",
      "625/625 [==============================] - 2s 2ms/step - loss: 1.7353 - categorical_accuracy: 0.1937 - precision_49: 1.0000 - recall_49: 1.0000e-04 - auc_49: 0.5077\n",
      "[CV] END ......................beta1=0.6, beta2=0.9, lr=0.05; total time=  28.5s\n",
      "Epoch 1/40\n",
      "625/625 [==============================] - 4s 5ms/step - loss: 336.0804 - categorical_accuracy: 0.3071 - precision_50: 0.3382 - recall_50: 0.2417 - auc_50: 0.6084 - val_loss: 2.3302 - val_categorical_accuracy: 0.1966 - val_precision_50: 0.1966 - val_recall_50: 0.1966 - val_auc_50: 0.5059\n",
      "Epoch 2/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3051 - categorical_accuracy: 0.2052 - precision_50: 0.2009 - recall_50: 0.1100 - auc_50: 0.5019 - val_loss: 2.8908 - val_categorical_accuracy: 0.1957 - val_precision_50: 0.1957 - val_recall_50: 0.1957 - val_auc_50: 0.5008\n",
      "Epoch 3/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3380 - categorical_accuracy: 0.2058 - precision_50: 0.2042 - recall_50: 0.1176 - auc_50: 0.5045 - val_loss: 1.7295 - val_categorical_accuracy: 0.1957 - val_precision_50: 0.0000e+00 - val_recall_50: 0.0000e+00 - val_auc_50: 0.5052\n",
      "Epoch 4/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3526 - categorical_accuracy: 0.2030 - precision_50: 0.2015 - recall_50: 0.1153 - auc_50: 0.5047 - val_loss: 2.9204 - val_categorical_accuracy: 0.1957 - val_precision_50: 0.1957 - val_recall_50: 0.1957 - val_auc_50: 0.4949\n",
      "Epoch 5/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3760 - categorical_accuracy: 0.2013 - precision_50: 0.2059 - recall_50: 0.1236 - auc_50: 0.5047 - val_loss: 2.3760 - val_categorical_accuracy: 0.1957 - val_precision_50: 0.1957 - val_recall_50: 0.1957 - val_auc_50: 0.5052\n",
      "Epoch 6/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3970 - categorical_accuracy: 0.1958 - precision_50: 0.1914 - recall_50: 0.1176 - auc_50: 0.4996 - val_loss: 2.0830 - val_categorical_accuracy: 0.1919 - val_precision_50: 0.0000e+00 - val_recall_50: 0.0000e+00 - val_auc_50: 0.4907\n",
      "Epoch 7/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3326 - categorical_accuracy: 0.1984 - precision_50: 0.1974 - recall_50: 0.1136 - auc_50: 0.4992 - val_loss: 2.3893 - val_categorical_accuracy: 0.2100 - val_precision_50: 0.2100 - val_recall_50: 0.2100 - val_auc_50: 0.5088\n",
      "Epoch 8/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3591 - categorical_accuracy: 0.1945 - precision_50: 0.1906 - recall_50: 0.1078 - auc_50: 0.4929 - val_loss: 2.0378 - val_categorical_accuracy: 0.1919 - val_precision_50: 0.0000e+00 - val_recall_50: 0.0000e+00 - val_auc_50: 0.4964\n",
      "Epoch 9/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3345 - categorical_accuracy: 0.1920 - precision_50: 0.1982 - recall_50: 0.1189 - auc_50: 0.4989 - val_loss: 2.3754 - val_categorical_accuracy: 0.1966 - val_precision_50: 0.0000e+00 - val_recall_50: 0.0000e+00 - val_auc_50: 0.4999\n",
      "Epoch 10/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3890 - categorical_accuracy: 0.1968 - precision_50: 0.1961 - recall_50: 0.1148 - auc_50: 0.4986 - val_loss: 2.5684 - val_categorical_accuracy: 0.1919 - val_precision_50: 0.1919 - val_recall_50: 0.1919 - val_auc_50: 0.4892\n",
      "Epoch 11/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3507 - categorical_accuracy: 0.1953 - precision_50: 0.1987 - recall_50: 0.0972 - auc_50: 0.4950 - val_loss: 1.7919 - val_categorical_accuracy: 0.2057 - val_precision_50: 0.0000e+00 - val_recall_50: 0.0000e+00 - val_auc_50: 0.5104\n",
      "Epoch 12/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3079 - categorical_accuracy: 0.2030 - precision_50: 0.1983 - recall_50: 0.1076 - auc_50: 0.5032 - val_loss: 2.0638 - val_categorical_accuracy: 0.1957 - val_precision_50: 0.1957 - val_recall_50: 0.1957 - val_auc_50: 0.5040\n",
      "Epoch 13/40\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 2.3323 - categorical_accuracy: 0.2019 - precision_50: 0.2029 - recall_50: 0.1162 - auc_50: 0.5021 - val_loss: 1.9965 - val_categorical_accuracy: 0.1919 - val_precision_50: 0.0000e+00 - val_recall_50: 0.0000e+00 - val_auc_50: 0.4895\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 2.5635 - categorical_accuracy: 0.1996 - precision_50: 0.0000e+00 - recall_50: 0.0000e+00 - auc_50: 0.5107\n",
      "625/625 [==============================] - 2s 2ms/step - loss: 1.6581 - categorical_accuracy: 0.1933 - precision_50: 1.0000 - recall_50: 5.0000e-05 - auc_50: 0.5091\n",
      "[CV] END ......................beta1=0.6, beta2=0.9, lr=0.05; total time=  26.8s\n",
      "Epoch 1/40\n",
      "782/782 [==============================] - 4s 3ms/step - loss: 4.1064 - categorical_accuracy: 0.8873 - precision_51: 0.8874 - recall_51: 0.8873 - auc_51: 0.9398 - val_loss: 0.3833 - val_categorical_accuracy: 0.9703 - val_precision_51: 0.9709 - val_recall_51: 0.9703 - val_auc_51: 0.9877\n",
      "Epoch 2/40\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.3996 - categorical_accuracy: 0.9613 - precision_51: 0.9617 - recall_51: 0.9612 - auc_51: 0.9868 - val_loss: 0.2382 - val_categorical_accuracy: 0.9698 - val_precision_51: 0.9705 - val_recall_51: 0.9693 - val_auc_51: 0.9911\n",
      "Epoch 3/40\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1884 - categorical_accuracy: 0.9727 - precision_51: 0.9735 - recall_51: 0.9719 - auc_51: 0.9928 - val_loss: 0.2799 - val_categorical_accuracy: 0.9702 - val_precision_51: 0.9715 - val_recall_51: 0.9694 - val_auc_51: 0.9912\n",
      "Epoch 4/40\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1618 - categorical_accuracy: 0.9780 - precision_51: 0.9784 - recall_51: 0.9776 - auc_51: 0.9942 - val_loss: 0.1129 - val_categorical_accuracy: 0.9791 - val_precision_51: 0.9805 - val_recall_51: 0.9782 - val_auc_51: 0.9962\n",
      "Epoch 5/40\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1351 - categorical_accuracy: 0.9815 - precision_51: 0.9821 - recall_51: 0.9813 - auc_51: 0.9956 - val_loss: 0.1526 - val_categorical_accuracy: 0.9750 - val_precision_51: 0.9757 - val_recall_51: 0.9748 - val_auc_51: 0.9947\n",
      "Epoch 6/40\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1439 - categorical_accuracy: 0.9828 - precision_51: 0.9834 - recall_51: 0.9822 - auc_51: 0.9957 - val_loss: 0.1450 - val_categorical_accuracy: 0.9807 - val_precision_51: 0.9814 - val_recall_51: 0.9794 - val_auc_51: 0.9944\n",
      "Epoch 7/40\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1023 - categorical_accuracy: 0.9841 - precision_51: 0.9843 - recall_51: 0.9837 - auc_51: 0.9965 - val_loss: 0.0877 - val_categorical_accuracy: 0.9845 - val_precision_51: 0.9858 - val_recall_51: 0.9834 - val_auc_51: 0.9969\n",
      "Epoch 8/40\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1041 - categorical_accuracy: 0.9864 - precision_51: 0.9870 - recall_51: 0.9861 - auc_51: 0.9970 - val_loss: 0.1868 - val_categorical_accuracy: 0.9828 - val_precision_51: 0.9834 - val_recall_51: 0.9825 - val_auc_51: 0.9950\n",
      "Epoch 9/40\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0937 - categorical_accuracy: 0.9866 - precision_51: 0.9869 - recall_51: 0.9863 - auc_51: 0.9967 - val_loss: 0.1478 - val_categorical_accuracy: 0.9777 - val_precision_51: 0.9785 - val_recall_51: 0.9769 - val_auc_51: 0.9956\n",
      "Epoch 10/40\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0980 - categorical_accuracy: 0.9886 - precision_51: 0.9889 - recall_51: 0.9884 - auc_51: 0.9970 - val_loss: 0.1740 - val_categorical_accuracy: 0.9859 - val_precision_51: 0.9859 - val_recall_51: 0.9859 - val_auc_51: 0.9950\n",
      "Epoch 11/40\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0960 - categorical_accuracy: 0.9905 - precision_51: 0.9905 - recall_51: 0.9904 - auc_51: 0.9975 - val_loss: 0.2195 - val_categorical_accuracy: 0.9825 - val_precision_51: 0.9827 - val_recall_51: 0.9823 - val_auc_51: 0.9952\n",
      "Epoch 12/40\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0929 - categorical_accuracy: 0.9899 - precision_51: 0.9899 - recall_51: 0.9898 - auc_51: 0.9969 - val_loss: 0.1541 - val_categorical_accuracy: 0.9866 - val_precision_51: 0.9870 - val_recall_51: 0.9866 - val_auc_51: 0.9966\n",
      "Epoch 13/40\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0740 - categorical_accuracy: 0.9932 - precision_51: 0.9933 - recall_51: 0.9932 - auc_51: 0.9981 - val_loss: 0.1806 - val_categorical_accuracy: 0.9886 - val_precision_51: 0.9887 - val_recall_51: 0.9886 - val_auc_51: 0.9956\n",
      "Epoch 14/40\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0757 - categorical_accuracy: 0.9935 - precision_51: 0.9937 - recall_51: 0.9933 - auc_51: 0.9980 - val_loss: 0.2513 - val_categorical_accuracy: 0.9884 - val_precision_51: 0.9886 - val_recall_51: 0.9882 - val_auc_51: 0.9961\n",
      "Epoch 15/40\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0707 - categorical_accuracy: 0.9932 - precision_51: 0.9932 - recall_51: 0.9932 - auc_51: 0.9981 - val_loss: 0.2434 - val_categorical_accuracy: 0.9884 - val_precision_51: 0.9886 - val_recall_51: 0.9884 - val_auc_51: 0.9951\n",
      "Epoch 16/40\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0729 - categorical_accuracy: 0.9936 - precision_51: 0.9939 - recall_51: 0.9935 - auc_51: 0.9980 - val_loss: 0.2593 - val_categorical_accuracy: 0.9886 - val_precision_51: 0.9886 - val_recall_51: 0.9886 - val_auc_51: 0.9948\n",
      "Epoch 17/40\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.0704 - categorical_accuracy: 0.9930 - precision_51: 0.9931 - recall_51: 0.9930 - auc_51: 0.9979 - val_loss: 0.3201 - val_categorical_accuracy: 0.9830 - val_precision_51: 0.9832 - val_recall_51: 0.9830 - val_auc_51: 0.9928\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomizedSearchCV(estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x000001493DB2E6D0>,\n",
       "                   param_distributions={'beta1': (0.4, 0.5, 0.6, 0.7, 0.8),\n",
       "                                        'beta2': (0.6, 0.7, 0.8, 0.9, 0.999),\n",
       "                                        'lr': (0.0005, 0.001, 0.005, 0.01, 0.05,\n",
       "                                               0.1)},\n",
       "                   random_state=42, return_train_score=True, verbose=2)"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "rnd_search_dnn.fit(X_data_train0to4,y_data_train0to4, epochs = 40, validation_data=(X_data_valid0to4,y_data_valid0to4), callbacks = [earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ]
}