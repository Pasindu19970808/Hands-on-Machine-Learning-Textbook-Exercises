{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Abstract\nAn Exploratory Data Analysis is done to find the most impactful variables in predicting Insurance Premium charges. Then Linear Regression using a linear model and Polynomial Regression using a polynomial model (order = 2) are carried out to observe how much the model improves from a simple linear regression model to a slightly complex polynomial model. A significant improvement is observed. \n\nSubsequently, Ridge Regression is used to investigate higher order polynomials with a regularization parameter to prevent overfitting and reduce the model's sensitivity to the weight of coefficients. It is observed that a Polynomial model of the 5th order paired with a regularization parameter of 100 results in the best R2 value.","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/insurance/insurance.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking for missing values\ndfna = df.isna()\nfor column in dfna.columns.values.tolist():\n    print(column)\n    print(dfna[column].value_counts(dropna = False))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**From the above step we know that the data contains no missing values**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# **Now we *create correlation heat maps, regression plots* and bar plots to assess which variables have a significant impact on premium cost**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"First we investigate relationship between sex and price","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting a boxplot to assess if gender has an impact on insurance cost\nsns.boxplot(x = 'sex',y = 'charges', data = df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**As seen in the above boxplot, while there is a slightly higher charge paid by male patients, both genders have the same median charge and have largely overlapping range between the upper and lower quartiles. This gives us a preliminary indication that sex is not going to have a significant effect on the premium charge**\n\nTo prove whether there is a significant difference between male and female premium charges, we use the Analysis of Variance to carry out the F-test on the data.\n\nAnalysis of variance involves finding the mean of 2 or more categorical variables, finding the variation between the means and finding its ratio with the variance of each group. It is coupled with a p-value with a significance value of 0.05.\n\n**Our Null Hypothesis is that there is no significant difference between the categorical variables and their coressponding charges. If the p-value is lower than the siginificance value, we say that our starting Null Hypothesis is wrong and there is infact a significant difference between the categorical variables and their coressponding charges.**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats\ndf_ftestsex = df[['sex','charges']].groupby(df['sex'])\ndf_ftestsex.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ftest_val,p_val = stats.f_oneway(df_ftestsex.get_group('female')['charges'],df_ftestsex.get_group('male')['charges'])\nprint('F-Value : {} , pvalue : {}'.format(ftest_val,p_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"**According to the above result we see although the p value is lower than the significance level of 0.05 which shows that there is a  difference between the charges incurred on males and females, the low F-test score shows that the correlation between the sex and premium charged is low. Regardless, a p value of 0.036, although lower than 0.05, is not low enough to show a *significant difference* between the charges on males and females**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"****Now we shall investigate the relationship between region and premium charged.****","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x = 'region', y = 'charges', data = df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ftestregion = df[['region','charges']].groupby('region')\ndf_ftestregion.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ftest_valregion, p_valregion = stats.f_oneway(df_ftestregion.get_group('southeast')['charges'],df_ftestregion.get_group('southwest')['charges'],\\\n                                             df_ftestregion.get_group('northeast')['charges'],df_ftestregion.get_group('northwest')['charges'])\nprint('F-Value : {} , pvalue : {}'.format(ftest_valregion,p_valregion))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here too, the F-value is quite low, although the P-value is lower than the significance value. This shows that the impact region has on the premium charge is weak. Nevertheless it is worth investigating if having both Sex and Region as variables has a significant impact on the linear regression model. **However, the expected observation is that the coefficients assigned to the categorical variables will carry a lower weight.**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Now we investigate the how premium charge varies depending whether an individual smokes or not**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x = 'smoker', y = 'charges', data = df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We immediately observe from the above boxplot that there is a significant difference between the premium charges incurred on smokers as compared to non-smokers. We expect the F-test to support our observation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_ftestsmoker = df[['smoker','charges']].groupby('smoker')\ndf_ftestsmoker.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ftest_valsmoker, p_valsmoker = stats.f_oneway(df_ftestsmoker.get_group('yes')['charges'],df_ftestsmoker.get_group('no')['charges'])\nprint('F-Value : {} , pvalue : {}'.format(ftest_valsmoker,p_valsmoker))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The above F-Test is strong proof of our preliminary observation. A very high F-Value coupled with an almost 0 P-value shows that there is a significant statistical difference between the charges incurred on smokers and non-smokers. Furthermore there is a clear correlation between smoking and the premimum charged. *Hence, we should include this variable in our model***","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Now we concentrate on the numerical variables. To do that we calculate the correlation between all numerical variables.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#correlation matrix\ndfcorr = df.corr()\n#correlation with premium charges\ndfcorrcharges = dfcorr[['charges']]\ndfcorrcharges","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The correlation coefficients are quite weak for all numerical variables. To investigate further we look at the Pearson Correlation Coefficient. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Age vs Charges","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corrcoeff_age,pval_age = stats.pearsonr(df['age'],df['charges'])\nprint(\"The Pearson Correlation Coefficient is\", corrcoeff_age, \" with a P-value of P =\", pval_age)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The linear relation between Age and Charge incurred is quite weak, but the P-Value is almost 0, hence the correlation is statistically significant. We can draw a regression plot to see if this observation is justified. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(x = 'age', y = 'charges', data = df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As expected the relationship is positive yet weak","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# BMI vs Charges","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corrcoeff_bmi,pval_bmi = stats.pearsonr(df['bmi'],df['charges'])\nprint(\"The Pearson Correlation Coefficient is\", corrcoeff_bmi, \" with a P-value of P =\", pval_bmi)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A weak linear relationship, however the correlation is statistically significant","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(x = 'bmi', y = 'charges', data = df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Children vs Charges","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"corrcoeff_child,pval_child = stats.pearsonr(df['children'],df['charges'])\nprint(\"The Pearson Correlation Coefficient is\", corrcoeff_child, \" with a P-value of P =\", pval_child) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The linear relationship is extremely weak with a value of 0.068. The P-value only suggests moderate certainty. Hence this variable can be ignored in model development.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.regplot(x = 'children', y = 'charges', data = df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above Exploratory Data Analysis, the following variables are chosen to be included in the model for initial investigation:\n1. Smoker\n2. Age\n3. Sex\n4. BMI","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"As Smoker and Sex are categorical variables we use ****one-hot encoding**** to make them into numerical variables","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#smokerdf is the one-hot encoded dataframe\nsmokerdf = pd.get_dummies(df['smoker']) \nsmokerdf.rename(columns = {'no':'non-smoker','yes':'smokes'}, inplace = True)\nsmokerdf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we are dropping the smoker column to replace it with the one-hot encoded dataframe \"smokerdf\"\ndf = pd.concat([df,smokerdf],axis = 1)\ndf.drop(['smoker'],axis = 1, inplace = True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sexdf = pd.get_dummies(df['sex'])\nsexdf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([df,sexdf],axis = 1)\ndf.drop(['sex'],axis = 1, inplace = True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#independent variable dataframe \nx = df[['age','bmi','non-smoker','smokes','female','male']]\ny = df[['charges']]\nx.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"To get a understanding as to whether each numerical variable has a linear or non linear relationship with the premimum charged, residual plots are used. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Age vs. Charge","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#residual plot for a linear relationship\nsns.residplot(x[['age']],y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The above residual plot suggests that there is linear relationship between Age and Premium Charge as the data is randomly distributed for all values of Age.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# BMI vs Charge","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsns.residplot(x[['bmi']],y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"The above residual shows that the error increases with bmi suggesting a non linear relationship. Nevertheless, we shall continue on with Linear regression now. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Scaling the x variables\nOnly Age and BMI have been scaled, as the other variables are either 1s or 0s due to the one-hot encoding which was previously done","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"SCALE = StandardScaler()\nxforscaling = x[['age','bmi']]\nSCALE.fit(xforscaling)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The mean values of Age and BMI are {} and {} respectively'.format(SCALE.mean_[0],SCALE.mean_[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaledxdata = pd.DataFrame(SCALE.transform(xforscaling))\nscaledxdata.rename(columns = {0:'Age_scaled',1:'BMI_scaled'},inplace = True)\nscaledxdata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xtemp = x.drop(['age','bmi'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xscaleddata = pd.concat([scaledxdata,xtemp],axis = 1)\nxscaleddata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**This step is to split the data into a training set and test set**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(xscaleddata,y,random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Linear regression using a linear model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"linreg = LinearRegression()\nlinreg.fit(x_train,y_train)\nprint(linreg.coef_,linreg.intercept_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ypredict_test = linreg.predict(x_test)\nypredict_test[0:10,:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# R2 Value of the Linear Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(r2_score(y_test,ypredict_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Linear regression using a polynomial features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"poly = PolynomialFeatures(2, include_bias = False)\nxpoly_train = poly.fit_transform(x_train)\nxpoly_train = pd.DataFrame(xpoly_train)\nxpoly_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linregpoly = LinearRegression()\nlinregpoly.fit(xpoly_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ypolypredict = linregpoly.predict(poly.fit_transform(x_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# R2 Value of the Polynomial Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print(r2_score(y_test,ypolypredict))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Residual Curves for both Linear Regression and Polynomial Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#residual curve for linear regression\nypredict_test = pd.DataFrame(ypredict_test)\nypredict_test.rename(columns = {0:'charges'}, inplace = True)\nypredict_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_testresid = y_test.reset_index(drop = True)\ny_testresid.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"linreg_resid = y_testresid - ypredict_test\nlinreg_resid.reset_index(inplace = True)\nlinreg_resid.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x = 'index', y = 'charges', data = linreg_resid).set_title('Residual Plot')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#residual curve for polynomial regression\nypolypredict_resid = pd.DataFrame(ypolypredict)\nypolypredict_resid.rename(columns = {0:'charges'},inplace = True)\nypolypredict_resid.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nlinregpoly_resid = y_testresid - ypolypredict_resid\nlinregpoly_resid.reset_index(inplace = True)\nlinregpoly_resid.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(x = 'index', y = 'charges', data = linregpoly_resid).set_title('Residual Plot_Polynomial')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Testing higher orders polynomials along with Ridge regression and Cross Validation using GridSearchCV\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"When testing higher order polynomials, it tends to result in overfitting resulting in high variance. To avoid this overfitting, we want to introduce some bias into out model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import Ridge","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xscaleddata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"polyorders = [2,3,4,5,6,7]\nparameters = [{'alpha':[0.001,0.005,0.01,0.05,0.1,0.5,1,5,10,100,1000]}]\nresults = {}\nfor order in polyorders:\n    #getting the polynomical object \n    poly =  PolynomialFeatures(degree = order, include_bias = False)\n    #transforming scaled x data to polynomial format\n    xpolydata = poly.fit_transform(xscaleddata)\n    #ridge model to introduce the regularization term\n    ridgemodel = Ridge()\n    gridsearchcv = GridSearchCV(ridgemodel,parameters,cv = 10)\n    gridsearchcv.fit(xpolydata,y)\n    #putting the results into the 'results' dictionary\n    results[order] = [gridsearchcv.best_params_,gridsearchcv.best_score_]\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above results, we see that polynomial orders of 3 and 5 paired with regularization parameter of 5 and 100 respecively result in high R2 values. However these R2 values are lesser than the R2 value of 0.8808592958824164 that we obtained using a polynomial order of 2, ***without cross validation***. Hence I want to apply the same parameters of alpha = 5,100 to polynomial order = 3,5 to the same test/train split to observe the resulting R2 values.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Investigating Polynomial order 3 with Regularization Parameter 5","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#this is the same test/train split used with Polynomial order of 2 previously\nx_train,x_test,y_train,y_test = train_test_split(xscaleddata,y,random_state = 0)\npoly3 = PolynomialFeatures(degree = 3)\nx_trainpoly3 = poly3.fit_transform(x_train)\nx_testpoly3 = poly3.fit_transform(x_test)\nridgepoly3 = Ridge(alpha = 5)\nridgepoly3.fit(x_trainpoly3,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predicting the values from the Ridge Regression model for x_testpoly3\ny_testpoly3predict = ridgepoly3.predict(x_testpoly3)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculating the R2 value on the test set using Ridge Regression model (Polynomial order = 3, Regularization Parameter = 5)\nr2_score(y_test,y_testpoly3predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"From the above result, the R2 value is almost the same as the one we obtained using a Polynomial Order of 2 with no regularization. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Investigating Polynomial order 5 with Regularization Parameter 100\n\n\n\n\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#5th order polynomial\npoly5 = PolynomialFeatures(degree = 5)\nx_trainpoly5 = poly5.fit_transform(x_train)\nx_testpoly5 = poly5.fit_transform(x_test)\nridgepoly5 = Ridge(alpha = 100)\nridgepoly5.fit(x_trainpoly5,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predicting the values from the Ridge Regression model for x_testpoly5\ny_testpoly5predict = ridgepoly5.predict(x_testpoly5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculating the R2 value on the test set using Ridge Regression model (Polynomial order = 5, Regularization Parameter = 100)\nr2_score(y_test,y_testpoly5predict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Final Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"A polynomial order of 5 coupled with a Regularization parameter of 100 gives a slightly better R2 value of 0.886 which is approximately 0.89. This is slightly better than using a Polynomial order of 2 with no regularization parameter. Thus our final model will use a 5th order polynomial with a regularization parameter of 100. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#final model\nridgepoly5","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}